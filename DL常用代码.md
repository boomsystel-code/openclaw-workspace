# æ·±åº¦å­¦ä¹ å¸¸ç”¨ä»£ç 

*ç²¾é€‰çš„æ·±åº¦å­¦ä¹ æ ¸å¿ƒä»£ç ç‰‡æ®µ*

---

## 1. PyTorchåŸºç¡€

### 1.1 å¼ é‡æ“ä½œ

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# åˆ›å»ºå¼ é‡
x = torch.randn(3, 4)  # æ­£æ€åˆ†å¸ƒ
x = torch.zeros(3, 4)  # é›¶å¼ é‡
x = torch.ones(3, 4)   # å…¨ä¸€å¼ é‡
x = torch.arange(0, 10, 2)  # åºåˆ—

# å¼ é‡æ“ä½œ
x = x.view(-1, 1)  # é‡å¡‘
x = x.unsqueeze(0)  # å¢åŠ ç»´åº¦
x = x.squeeze()    # ç§»é™¤ç»´åº¦
x = x.clone()       # å¤åˆ¶
```

### 1.2 æ¨¡å‹å®šä¹‰

```python
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super().__init__()
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.extend([nn.Linear(prev_dim, hidden_dim), nn.ReLU()])
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)
```

### 1.3 å·ç§¯ç½‘ç»œ

```python
class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = self.dropout(F.relu(self.fc1(x)))
        return self.fc2(x)
```

---

## 2. è®­ç»ƒå¾ªç¯

```python
import torch.optim as optim

# ä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
# æˆ–
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss()
# æˆ–
criterion = nn.MSELoss()

# è®­ç»ƒ
model.train()
for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

---

## 3. å­¦ä¹ ç‡è°ƒåº¦

```python
# ä½™å¼¦é€€ç«
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=num_epochs
)

# é˜¶æ¢¯è¡°å‡
scheduler = optim.lr_scheduler.StepLR(
    optimizer, step_size=30, gamma=0.1
)

# é¢„çƒ­+ä½™å¼¦
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2
)

# 1Cycleç­–ç•¥
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer, max_lr=0.1, epochs=num_epochs, 
    steps_per_epoch=len(train_loader)
)
```

---

## 4. æ­£åˆ™åŒ–

```python
# Dropout
nn.Dropout(p=0.5)

# æ‰¹å½’ä¸€åŒ–
nn.BatchNorm2d(num_features)

# æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™ï¼‰
optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)

# æ ‡ç­¾å¹³æ»‘
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## 5. æ³¨æ„åŠ›æœºåˆ¶

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        output = torch.matmul(attn_weights, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, d_model)
        
        return self.w_o(output)
```

---

## 6. Transformeræ¨¡å—

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        ff_output = self.feed_forward(x)
        return self.norm2(x + self.dropout(ff_output))
```

---

## 7. æ··åˆç²¾åº¦è®­ç»ƒ

```python
scaler = torch.cuda.amp.GradScaler()

for inputs, labels in train_loader:
    optimizer.zero_grad()
    
    with torch.cuda.amp.autocast():
        outputs = model(inputs)
        loss = criterion(outputs, labels)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

## 8. åˆ†å¸ƒå¼è®­ç»ƒ

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–
dist.init_process_group(backend='nccl')
local_rank = int(os.environ['LOCAL_RANK'])
torch.cuda.set_device(local_rank)

# æ¨¡å‹
model = model.cuda(local_rank)
model = DDP(model, device_ids=[local_rank])

# æ•°æ®
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)
```

---

## 9. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
torch.save(model, 'model.pth')

# ä¿å­˜çŠ¶æ€å­—å…¸
torch.save(model.state_dict(), 'model_state.pth')

# åŠ è½½
model.load_state_dict(torch.load('model_state.pth'))
model.eval()

# æ¨ç†
with torch.no_grad():
    outputs = model(input)
```

---

## 10. æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
from torch.utils.checkpoint import checkpoint

class CheckpointedModel(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)
    
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            if i < len(self.layers) - 1:
                x = checkpoint(layer, x)
            else:
                x = layer(x)
        return x
```

---

*æ·±åº¦å­¦ä¹ å¸¸ç”¨ä»£ç æ•´ç†å®Œæˆï¼* ğŸ’»ğŸš€
