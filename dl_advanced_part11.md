# ğŸš€ æ·±åº¦å­¦ä¹ é«˜çº§æŠ€æœ¯ Part 11

*æ‰©å±•çŸ¥è¯†ä¸å®è·µ*

---

## 131. é«˜çº§NLPæŠ€æœ¯

### 131.1 å¤§æ¨¡å‹è®­ç»ƒæŠ€å·§

```python
class LLM TrainingPipeline:
    """å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæµæ°´çº¿"""
    
    def __init__(self, model, tokenizer, config):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = self._create_scheduler()
        
        # æ¢¯åº¦ç¼©æ”¾
        self.scaler = torch.cuda.amp.GradScaler()
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        def lr_lambda(step):
            if step < self.config.warmup_steps:
                return float(step) / float(max(1, self.config.warmup_steps))
            progress = float(step - self.config.warmup_steps) / float(
                max(1, self.config.train_steps - self.config.warmup_steps)
            )
            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))
        
        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
    
    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        
        for step, batch in enumerate(dataloader):
            input_ids = batch['input_ids'].cuda()
            attention_mask = batch['attention_mask'].cuda()
            labels = batch['labels'].cuda()
            
            # æ··åˆç²¾åº¦å‰å‘
            with torch.cuda.amp.autocast():
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss / self.config.gradient_accumulation_steps
            
            # åå‘
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (step + 1) % self.config.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config.max_grad_norm
                )
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.optimizer.zero_grad()
                
                # å­¦ä¹ ç‡è°ƒåº¦
                self.scheduler.step()
            
            total_loss += loss.item()
        
        return total_loss / len(dataloader)

class FlashAttention:
    """Flash Attentionå®ç°"""
    
    def __init__(self, embed_dim, num_heads, dropout=0.0):
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # Q, K, VæŠ•å½±
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, query, key, value, attn_mask=None):
        batch_size, seq_len, _ = query.shape
        
        # æŠ•å½±å¹¶åˆ†å¤´
        Q = self.q_proj(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Flash Attention
        attn_output = self._flash_attention(Q, K, V, attn_mask)
        
        # åˆå¹¶å¤´
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.embed_dim
        )
        
        return self.out_proj(attn_output)
    
    def _flash_attention(self, Q, K, V, mask):
        """Flash Attentionæ ¸å¿ƒå®ç°"""
        # è®¡ç®—softmax
        scale = 1.0 / math.sqrt(self.head_dim)
        attn = torch.matmul(Q, K.transpose(-2, -1)) * scale
        
        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)
        
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        # ä¸Vç›¸ä¹˜
        output = torch.matmul(attn, V)
        
        return output
```

### 131.2 åˆ†è¯ä¸è¯è¡¨

```python
class BytePairEncoding:
    """å­—èŠ‚å¯¹ç¼–ç """
    
    def __init__(self, vocab_size=10000, min_frequency=2):
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.word_freq = {}
        self.merges = {}
        self.vocab = {}
    
    def train(self, text):
        """è®­ç»ƒBPE"""
        # ç»Ÿè®¡è¯é¢‘
        for word in text.split():
            self.word_freq[word] = self.word_freq.get(word, 0) + 1
        
        # åˆå§‹åŒ–è¯è¡¨
        self.vocab = {chr(i + 256): i for i in range(self.vocab_size)}
        
        # è¿­ä»£åˆå¹¶
        for i in range(self.vocab_size - 256):
            # æ‰¾åˆ°æœ€é¢‘ç¹çš„bigram
            best_pair = self._find_most_frequent_bigram()
            
            if not best_pair or self.word_freq[best_pair] < self.min_frequency:
                break
            
            # åˆå¹¶
            self._merge_pair(best_pair)
            self.merges[best_pair] = len(self.vocab)
    
    def _find_most_frequent_bigram(self):
        """æ‰¾åˆ°æœ€é¢‘ç¹çš„bigram"""
        bigram_freq = {}
        
        for word, freq in self.word_freq.items():
            for i in range(len(word) - 1):
                bigram = (word[i], word[i + 1])
                bigram_freq[bigram] = bigram_freq.get(bigram, 0) + freq
        
        if not bigram_freq:
            return None
        
        return max(bigram_freq, key=bigram_freq.get)
    
    def _merge_pair(self, pair):
        """åˆå¹¶pair"""
        new_symbol = ''.join(pair)
        
        # æ›´æ–°è¯é¢‘
        new_word_freq = {}
        for word, freq in self.word_freq.items():
            new_word = word.replace(''.join(pair), new_symbol)
            new_word_freq[new_word] = new_word_freq.get(new_word, 0) + freq
        
        self.word_freq = new_word_freq
    
    def encode(self, text):
        """ç¼–ç """
        tokens = list(text)
        
        while len(tokens) > 1:
            # æ‰¾åˆ°å¯åˆå¹¶çš„pair
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            
            best_idx = None
            best_pair = None
            for i, pair in enumerate(pairs):
                if pair in self.merges:
                    if best_idx is None or self.merges[pair] < self.merges[best_pair]:
                        best_idx = i
                        best_pair = pair
            
            if best_idx is None:
                break
            
            # åˆå¹¶
            new_symbol = chr(len(self.vocab) + 256)
            self.vocab[new_symbol] = len(self.vocab)
            
            tokens[best_idx] = new_symbol
            del tokens[best_idx + 1]
        
        return tokens
```

### 131.3 é•¿ä¸Šä¸‹æ–‡å¤„ç†

```python
class LongContextTransformer:
    """é•¿ä¸Šä¸‹æ–‡Transformer"""
    
    def __init__(self, base_model, max_context=32768):
        self.base_model = base_model
        self.max_context = max_context
        
        # ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
        self.local_window = 1024
        self.global_attention = [0]  # CLS tokenæ€»æ˜¯å…¨å±€
    
    def forward(self, input_ids, attention_mask=None):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len = input_ids.shape
        
        if seq_len <= self.local_window:
            # çŸ­åºåˆ—ä½¿ç”¨å®Œæ•´æ³¨æ„åŠ›
            return self.base_model(input_ids, attention_mask)
        
        # æ„å»ºç¨€ç–æ³¨æ„åŠ›æ©ç 
        sparse_mask = self._build_sparse_mask(seq_len)
        
        # åˆ†å—å¤„ç†
        chunks = self._split_into_chunks(input_ids, self.local_window)
        outputs = []
        
        for i, chunk in enumerate(chunks):
            # å…¨å±€token
            global_token = input_ids[:, i * self.local_window:i * self.local_window + 1]
            
            # å±€éƒ¨æ³¨æ„åŠ›
            local_output = self.base_model(chunk, None)
            
            # ä¸å…¨å±€äº¤äº’
            for j, token_idx in enumerate(self.global_attention):
                if token_idx < len(chunk):
                    global_repr = local_output[:, token_idx:token_idx + 1]
            
            outputs.append(local_output)
        
        # æ‹¼æ¥
        return torch.cat(outputs, dim=1)
    
    def _build_sparse_mask(self, seq_len):
        """æ„å»ºç¨€ç–æ³¨æ„åŠ›æ©ç """
        mask = torch.zeros(seq_len, seq_len)
        
        # å±€éƒ¨çª—å£
        for i in range(seq_len):
            start = max(0, i - self.local_window)
            end = min(seq_len, i + self.local_window + 1)
            mask[i, start:end] = 1
        
        # å…¨å±€token
        for global_idx in self.global_attention:
            mask[:, global_idx] = 1
            mask[global_idx, :] = 1
        
        return mask
    
    def _split_into_chunks(self, tensor, chunk_size):
        """åˆ†å—"""
        return tensor.split(chunk_size, dim=1)
```

---

## 132. è§†è§‰å¤§æ¨¡å‹

### 132.1 SAMåˆ†å‰²

```python
class SegmentAnythingModel:
    """Segment Anything Model"""
    
    def __init__(self, encoder_dim=1280, num_masks=4):
        # å›¾åƒç¼–ç å™¨
        self.image_encoder = ResNet50Backbone() if num_masks < 10 else ViT_B()
        
        # æç¤ºç¼–ç å™¨
        self.prompt_encoder = PromptEncoder()
        
        # æ©ç è§£ç å™¨
        self.mask_decoder = MaskDecoder(
            transformer_dim=encoder_dim,
            num_masks=num_masks
        )
    
    def forward(self, image, prompts):
        """å‰å‘ä¼ æ’­"""
        # å›¾åƒç‰¹å¾
        image_embeddings = self.image_encoder(image)
        
        # æç¤ºç¼–ç 
        sparse_embeddings, dense_embeddings = self.prompt_encoder(prompts)
        
        # æ©ç é¢„æµ‹
        masks, iou_pred = self.mask_decoder(
            image_embeddings,
            sparse_embeddings,
            dense_embeddings
        )
        
        return masks, iou_pred

class MaskDecoder:
    """æ©ç è§£ç å™¨"""
    
    def __init__(self, transformer_dim=256, num_masks=4):
        self.transformer_dim = transformer_dim
        self.num_masks = num_masks
        
        # Transformer
        self.transformer = MaskTransformer(transformer_dim)
        
        # è¾“å‡ºå±‚
        self.output_tokens = nn.Embedding(4, transformer_dim)
        self.output_hypernet = nn.Linear(transformer_dim, 256)
        self.mask_features = nn.Conv2d(transformer_dim, 256, kernel_size=1)
    
    def forward(self, image_embeddings, sparse_embeddings, dense_embeddings):
        """å‰å‘ä¼ æ’­"""
        # Transformerå¤„ç†
        tokens = self.output_tokens.weight.unsqueeze(0).expand(
            image_embeddings.size(0), -1, -1
        )
        
        # èåˆ
        sos_tokens = torch.cat([tokens, sparse_embeddings], dim=1)
        
        # Transformer
        hs = self.transformer(
            sos_tokens,
            image_embeddings,
            dense_embeddings
        )
        
        # è¾“å‡ºæ©ç token
        mask_tokens = hs[:, :4]
        
        # æ©ç ç‰¹å¾
        mask_features = self.mask_features(image_embeddings)
        
        # æ¯ä¸ªtokené¢„æµ‹ä¸€ä¸ªæ©ç 
        masks = []
        for i in range(self.num_masks):
            mask_features_i = mask_features * self.output_hypernet(mask_tokens[:, i]).unsqueeze(-1).unsqueeze(-1)
            mask = F.conv2d(mask_features_i, self.mask_features.weight, bias=None)
            masks.append(mask)
        
        masks = torch.stack(masks, dim=1)
        
        return masks
```

### 132.2 è§†è§‰ç”Ÿæˆ

```python
class ImageGenerationModel:
    """å›¾åƒç”Ÿæˆæ¨¡å‹"""
    
    def __init__(self, config):
        self.config = config
        
        # U-Netå»å™ªç½‘ç»œ
        self.unet = UNetModel(
            in_channels=4,
            model_channels=128,
            out_channels=4,
            num_res_blocks=2,
            attention_resolutions=[8, 16],
            channel_mult=[1, 2, 4, 8],
            num_heads=8
        )
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = CLIPTextEncoder()
        
        # VAE
        self.vae = AutoencoderKL(
            in_channels=3,
            latent_channels=4,
            out_channels=3
        )
    
    def train_step(self, images, prompts):
        """è®­ç»ƒæ­¥éª¤"""
        # ç¼–ç å›¾åƒ
        latents = self.vae.encode(images).latent_dist.sample()
        latents = latents * 0.18215
        
        # é‡‡æ ·å™ªå£°
        noise = torch.randn_like(latents)
        timesteps = torch.randint(0, 1000, (latents.size(0),))
        
        # æ·»åŠ å™ªå£°
        noisy_latents = self._add_noise(latents, noise, timesteps)
        
        # ç¼–ç æ–‡æœ¬
        text_embeddings = self.text_encoder(prompts)
        
        # é¢„æµ‹å™ªå£°
        noise_pred = self.unet(
            noisy_latents, timesteps,
            encoder_hidden_states=text_embeddings
        ).sample
        
        # æŸå¤±
        loss = F.mse_loss(noise_pred, noise)
        return loss
    
    def generate(self, prompts, num_images=1, guidance_scale=7.5):
        """ç”Ÿæˆå›¾åƒ"""
        # ç¼–ç æ–‡æœ¬
        text_embeddings = self.text_encoder(prompts)
        
        # æ— åˆ†ç±»å™¨å¼•å¯¼
        if guidance_scale > 1.0:
            uncond_embeddings = self.text_encoder([''] * len(prompts))
            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
        
        # ç”Ÿæˆ
        latents = self._generate_latents(text_embeddings)
        
        # è§£ç 
        images = self.vae.decode(latents / 0.18215)
        
        return images
    
    def _generate_latents(self, text_embeddings):
        """ç”Ÿæˆæ½œåœ¨å˜é‡"""
        latents = torch.randn(
            text_embeddings.size(0) // 2,
            4, 64, 64
        ).cuda()
        
        scheduler = DDIMScheduler()
        scheduler.set_timesteps(50)
        
        for t in scheduler.timesteps:
            # é¢„æµ‹å™ªå£°
            noise_pred = self.unet(
                latents, t,
                encoder_hidden_states=text_embeddings
            ).sample
            
            # å¼•å¯¼
            if text_embeddings.size(0) > latents.size(0):
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
            
            # é‡‡æ ·
            latents = scheduler.step(noise_pred, t, latents).prev_sample
        
        return latents
```

### 132.3 è§†é¢‘ç†è§£

```python
class VideoUnderstandingModel:
    """è§†é¢‘ç†è§£æ¨¡å‹"""
    
    def __init__(self, config):
        # ç©ºé—´ç¼–ç å™¨
        self.spatial_encoder = VideoViT(
            num_frames=config.num_frames,
            patch_size=2,
            embed_dim=768
        )
        
        # æ—¶é—´ç¼–ç å™¨
        self.temporal_encoder = TimeTransformer(
            embed_dim=768,
            num_layers=6
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(768, config.num_classes)
    
    def forward(self, video):
        """è§†é¢‘å‰å‘"""
        batch_size, num_frames, channels, height, width = video.shape
        
        # ç©ºé—´ç‰¹å¾æå–
        spatial_features = []
        for t in range(num_frames):
            frame = video[:, t]
            features = self.spatial_encoder(frame)
            spatial_features.append(features)
        
        # å †å æ—¶é—´ç‰¹å¾
        spatial_features = torch.stack(spatial_features, dim=1)  # [B, T, C]
        
        # æ—¶é—´å»ºæ¨¡
        temporal_features = self.temporal_encoder(spatial_features)
        
        # åˆ†ç±»
        output = self.classifier(temporal_features[:, -1])  # æœ€åä¸€å¸§
        
        return output

class TimeTransformer:
    """æ—¶é—´Transformer"""
    
    def __init__(self, embed_dim, num_layers=6, num_heads=8):
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)
            for _ in range(num_layers)
        ])
        
        self.temporal_embedding = nn.Parameter(torch.zeros(1, 1, embed_dim))
    
    def forward(self, x):
        # æ·»åŠ æ—¶é—´ä½ç½®ç¼–ç 
        x = x + self.temporal_embedding
        
        # Transformer
        for layer in self.layers:
            x = layer(x)
        
        return x
```

---

## 133. è¯­éŸ³AI

### 133.1 Whisper

```python
class WhisperModel:
    """Whisperè¯­éŸ³è¯†åˆ«æ¨¡å‹"""
    
    def __init__(self, config):
        # ç¼–ç å™¨
        self.encoder = AudioEncoder(
            n_mels=config.n_mels,
            n_ctx=config.n_ctx,
            n_state=config.n_state,
            n_head=config.n_head,
            n_layer=config.n_layer
        )
        
        # è§£ç å™¨
        self.decoder = WhisperDecoder(
            n_vocab=config.n_vocab,
            n_ctx=config.n_ctx,
            n_state=config.n_state,
            n_head=config.n_head,
            n_layer=config.n_layer
        )
    
    def transcribe(self, audio):
        """è½¬å½•"""
        # æå–æ¢…å°”é¢‘è°±
        mel = self._extract_mel(audio)
        
        # ç¼–ç 
        audio_features = self.encoder(mel)
        
        # è§£ç 
        tokens = self.decoder.generate(audio_features)
        
        return self._decode_tokens(tokens)
    
    def _extract_mel(self, audio, n_mels=80, n_fft=400, hop_length=160):
        """æå–æ¢…å°”é¢‘è°±"""
        # çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
        stft = torch.stft(audio, n_fft, hop_length, window=torch.hann_window(n_fft), return_complex=True)
        magnitude = stft.abs()
        
        # æ¢…å°”æ»¤æ³¢å™¨
        mel_filter = self._create_mel_filter(n_mels, n_fft)
        mel = torch.matmul(magnitude, mel_filter)
        
        # å¯¹æ•°å‹ç¼©
        mel = torch.log(mel + 1e-9)
        
        return mel
```

### 133.2 è¯­éŸ³åˆæˆ

```python
class TextToSpeech:
    """æ–‡æœ¬è½¬è¯­éŸ³"""
    
    def __init__(self, config):
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = TextEncoder(
            n_vocab=config.n_vocab,
            n_ctx=config.n_ctx,
            n_state=config.n_state,
            n_head=config.n_head,
            n_layer=config.n_layer
        )
        
        # éŸ³é¢‘è§£ç å™¨
        self.audio_decoder = AudioDecoder(
            n_mels=config.n_mels,
            n_ctx=config.n_ctx,
            n_state=config.n_state,
            n_head=config.n_head,
            n_layer=config.n_layer
        )
    
    def synthesize(self, text):
        """åˆæˆè¯­éŸ³"""
        # ç¼–ç æ–‡æœ¬
        text_tokens = self._tokenize(text)
        text_features = self.text_encoder(text_tokens)
        
        # ç”ŸæˆéŸ³é¢‘
        mel = self.audio_decoder.generate(text_features)
        
        # è½¬æ¢ä¸ºæ³¢å½¢
        waveform = self._mel_to_waveform(mel)
        
        return waveform
    
    def _mel_to_waveform(self, mel, n_fft=1024, hop_length=256):
        """æ¢…å°”é¢‘è°±è½¬æ³¢å½¢"""
        # Griffin-Lim
        waveform = self._griffin_lim(mel, n_fft, hop_length)
        
        return waveform
    
    def _griffin_lim(self, mel_spectrogram, n_fft, hop_length):
        """Griffin-Limç®—æ³•"""
        # åˆå§‹åŒ–ç›¸ä½
        signal = torch.randn(mel_spectrogram.size(0), n_fft // 2 + 1, mel_spectrogram.size(1))
        
        for _ in range(50):
            # STFT
            stft = torch.stft(signal, n_fft, hop_length, window=torch.hann_window(n_fft), return_complex=True)
            
            # æ›´æ–°ç›¸ä½
            phase = stft.angle()
            reconstruction = mel_spectrogram * torch.exp(1j * phase)
            
            # ISTFT
            signal = torch.istft(reconstruction, n_fft, hop_length, window=torch.hann_window(n_fft))
        
        return signal
```

### 133.3 è¯­éŸ³è½¬æ¢

```python
class VoiceConversion:
    """è¯­éŸ³è½¬æ¢"""
    
    def __init__(self, config):
        # å†…å®¹ç¼–ç å™¨
        self.content_encoder = ContentEncoder()
        
        # è¯´è¯è€…ç¼–ç å™¨
        self.speaker_encoder = SpeakerEncoder()
        
        # è§£ç å™¨
        self.decoder = Decoder()
    
    def convert(self, source_audio, target_speaker):
        """è½¬æ¢è¯­éŸ³"""
        # æå–å†…å®¹
        content = self.content_encoder(source_audio)
        
        # æå–è¯´è¯è€…ç‰¹å¾
        speaker = self.speaker_encoder(target_speaker)
        
        # èåˆå¹¶è§£ç 
        converted = self.decoder(content, speaker)
        
        return converted
```

---

## 134. æ¨èç³»ç»Ÿæ·±åº¦

### 134.1 æ·±åº¦ååŒè¿‡æ»¤

```python
class DeepCollaborativeFiltering:
    """æ·±åº¦ååŒè¿‡æ»¤"""
    
    def __init__(self, num_users, num_items, embedding_dim=64, hidden_dims=[128, 64, 32]):
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # MLPå±‚
        layers = []
        input_dim = embedding_dim * 2
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            input_dim = hidden_dim
        
        self.mlp = nn.Sequential(*layers)
        self.output_layer = nn.Linear(hidden_dims[-1], 1)
    
    def forward(self, user_ids, item_ids):
        # åµŒå…¥
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # æ‹¼æ¥
        x = torch.cat([user_emb, item_emb], dim=-1)
        
        # MLP
        x = self.mlp(x)
        
        # è¾“å‡º
        output = self.output_layer(x)
        
        return output.squeeze()
```

### 134.2 å›¾æ¨è

```python
class GraphRecommender:
    """å›¾æ¨èç³»ç»Ÿ"""
    
    def __init__(self, num_users, num_items, embedding_dim=64):
        # ç”¨æˆ·åµŒå…¥
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        
        # ç‰©å“åµŒå…¥
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # å›¾ç¥ç»ç½‘ç»œ
        self.gnn = LightGCN(
            embedding_dim=embedding_dim,
            num_layers=3
        )
    
    def forward(self, user_item_graph, user_ids, item_ids):
        """å‰å‘ä¼ æ’­"""
        # å›¾åµŒå…¥
        user_emb, item_emb = self.gnn(user_item_graph)
        
        # é‡‡æ ·æ­£è´Ÿæ ·æœ¬
        pos_scores = (user_emb[user_ids] * item_emb[item_ids]).sum(dim=-1)
        neg_scores = (user_emb[user_ids] * item_emb[self.neg_items]).sum(dim=-1)
        
        return pos_scores, neg_scores

class LightGCN:
    """LightGCN"""
    
    def __init__(self, embedding_dim, num_layers=3):
        self.embedding_dim = embedding_dim
        self.num_layers = num_layers
        
        self.alpha = 1.0 / (num_layers + 1)
    
    def forward(self, graph, user_emb, item_emb):
        """å‰å‘ä¼ æ’­"""
        # åˆå§‹åµŒå…¥
        all_emb = torch.cat([user_emb, item_emb], dim=0)
        emb_list = [all_emb]
        
        # å¤šå±‚å›¾å·ç§¯
        for _ in range(self.num_layers):
            all_emb = self._propagate(graph, all_emb)
            emb_list.append(all_emb)
        
        # åŠ æƒæ±‚å’Œ
        final_emb = torch.zeros_like(all_emb)
        for emb in emb_list:
            final_emb += emb * self.alpha
        
        # åˆ†ç¦»
        user_final = final_emb[:len(user_emb)]
        item_final = final_emb[len(user_emb):]
        
        return user_final, item_final
    
    def _propagate(self, graph, embeddings):
        """å›¾ä¼ æ’­"""
        # å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
        norm_adj = self._normalize_adjacency(graph)
        
        # ä¼ æ’­
        return torch.sparse.mm(norm_adj, embeddings)
```

### 134.3 å¤šä»»åŠ¡æ¨è

```python
class MultiTaskRecommender:
    """å¤šä»»åŠ¡æ¨è"""
    
    def __init__(self, shared_bottom, task_towers):
        self.shared_bottom = shared_bottom
        self.task_towers = nn.ModuleDict(task_towers)
    
    def forward(self, user_features, item_features):
        """å‰å‘ä¼ æ’­"""
        # å…±äº«åº•å±‚
        shared_repr = self.shared_bottom(user_features, item_features)
        
        # å¤šä»»åŠ¡è¾“å‡º
        outputs = {}
        for task_name, tower in self.task_towers.items():
            outputs[task_name] = tower(shared_repr)
        
        return outputs
    
    def loss(self, predictions, labels):
        """å¤šä»»åŠ¡æŸå¤±"""
        total_loss = 0
        
        for task_name, pred in predictions.items():
            label = labels[task_name]
            
            if task_name == 'click':
                loss = F.binary_cross_entropy_with_logits(pred, label)
            elif task_name == 'conversion':
                loss = F.binary_cross_entropy_with_logits(pred, label)
            elif task_name == 'dwell_time':
                loss = F.mse_loss(pred, label)
            
            total_loss += loss
        
        return total_loss
```

---

## 135. å¼‚å¸¸æ£€æµ‹

### 135.1 å•ç±»åˆ†ç±»

```python
class OneClassSVM:
    """å•ç±»SVM"""
    
    def __init__(self, kernel='rbf', nu=0.1, gamma='scale'):
        self.kernel = kernel
        self.nu = nu
        self.gamma = gamma
        self.model = None
    
    def fit(self, X):
        """è®­ç»ƒ"""
        from sklearn.svm import OneClassSVM
        
        self.model = OneClassSVM(
            kernel=self.kernel,
            nu=self.nu,
            gamma=self.gamma
        ).fit(X)
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹"""
        return self.model.predict(X)
    
    def score_samples(self, X):
        """å¼‚å¸¸åˆ†æ•°"""
        return self.model.decision_function(X)

class DeepSVDD:
    """æ·±åº¦SVDD"""
    
    def __init__(self, input_dim, hidden_dims=[128, 64], radius=0.0):
        self.encoder = self._build_encoder(input_dim, hidden_dims)
        self.radius = radius
        self.center = None
    
    def _build_encoder(self, input_dim, hidden_dims):
        """æ„å»ºç¼–ç å™¨"""
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU()
            ])
            prev_dim = hidden_dim
        
        return nn.Sequential(*layers)
    
    def fit(self, X, epochs=100):
        """è®­ç»ƒ"""
        self.encoder.train()
        
        optimizer = torch.optim.Adam(self.encoder.parameters())
        
        for epoch in range(epochs):
            for x in X:
                # ç¼–ç 
                z = self.encoder(x)
                
                # è®¡ç®—åˆ°ä¸­å¿ƒçš„è·ç¦»
                if self.center is None:
                    self.center = z.mean(dim=0)
                
                loss = ((z - self.center) ** 2).sum(dim=1).mean()
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹"""
        self.encoder.eval()
        
        with torch.no_grad():
            z = self.encoder(X)
            distances = ((z - self.center) ** 2).sum(dim=1)
        
        # è¿”å›å¼‚å¸¸åˆ†æ•°
        return -distances  # è·ç¦»è¶Šå¤§ï¼Œå¼‚å¸¸åˆ†æ•°è¶Šé«˜
```

### 135.2 è‡ªç¼–ç å™¨å¼‚å¸¸æ£€æµ‹

```python
class AutoEncoderAnomalyDetection:
    """è‡ªç¼–ç å™¨å¼‚å¸¸æ£€æµ‹"""
    
    def __init__(self, input_dim, latent_dim=32):
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, latent_dim)
        )
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim)
        )
    
    def fit(self, X, epochs=100):
        """è®­ç»ƒ"""
        optimizer = torch.optim.Adam(self.parameters())
        
        for epoch in range(epochs):
            # ç¼–ç -è§£ç 
            z = self.encoder(X)
            reconstructed = self.decoder(z)
            
            # é‡å»ºæŸå¤±
            loss = F.mse_loss(reconstructed, X)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹å¼‚å¸¸"""
        with torch.no_grad():
            z = self.encoder(X)
            reconstructed = self.decoder(z)
            
            # é‡å»ºè¯¯å·®
            reconstruction_error = F.mse_loss(reconstructed, X, reduction='none').sum(dim=1)
        
        return reconstruction_error
    
    def detect(self, X, threshold=None):
        """æ£€æµ‹å¼‚å¸¸"""
        errors = self.predict(X)
        
        if threshold is None:
            threshold = errors.mean() + 3 * errors.std()
        
        return errors > threshold, errors
```

### 135.3 æ—¶åºå¼‚å¸¸æ£€æµ‹

```python
class TimeSeriesAnomalyDetection:
    """æ—¶åºå¼‚å¸¸æ£€æµ‹"""
    
    def __init__(self, input_dim, hidden_dim=64):
        # ç¼–ç å™¨
        self.encoder = nn.LSTM(
            input_dim, hidden_dim, num_layers=2,
            batch_first=True, dropout=0.2
        )
        
        # è§£ç å™¨
        self.decoder = nn.LSTM(
            hidden_dim, input_dim, num_layers=2,
            batch_first=True, dropout=0.2
        )
    
    def fit(self, X, epochs=100):
        """è®­ç»ƒ"""
        optimizer = torch.optim.Adam(self.parameters())
        
        for epoch in range(epochs):
            # ç¼–ç 
            _, (hidden, _) = self.encoder(X)
            
            # è§£ç 
            reconstructed, _ = self.decoder(hidden.repeat(1, X.size(1), 1))
            
            # æŸå¤±
            loss = F.mse_loss(reconstructed, X)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        return self
    
    def detect(self, X, threshold=None):
        """æ£€æµ‹"""
        with torch.no_grad():
            _, (hidden, _) = self.encoder(X)
            reconstructed, _ = self.decoder(hidden.repeat(1, X.size(1), 1))
            
            # é‡å»ºè¯¯å·®
            errors = F.mse_loss(reconstructed, X, reduction='none').mean(dim=2)
        
        if threshold is None:
            threshold = errors.mean() + 3 * errors.std()
        
        return errors > threshold, errors
```

---

## 136. AutoMLé«˜çº§

### 136.1 ç¥ç»æ¶æ„æœç´¢

```python
class NASController:
    """NASæ§åˆ¶å™¨"""
    
    def __init__(self, search_space, hidden_size=100):
        self.controller = nn.LSTM(
            hidden_size, hidden_size, num_layers=2
        )
        
        self.embeddings = nn.Embedding(len(search_space), hidden_size)
        
        # æ¯ä¸ªå†³ç­–çš„è§£ç å™¨
        self.decoders = nn.ModuleDict()
        for key, options in search_space.items():
            self.decoders[key] = nn.Linear(hidden_size, len(options))
    
    def sample(self):
        """é‡‡æ ·æ¶æ„"""
        architecture = {}
        
        for key in search_space.keys():
            # åµŒå…¥
            embed = self.embeddings.weight.mean(dim=0, keepdim=True)
            
            # è§£ç 
            logits = self.decoders[key](embed)
            probs = F.softmax(logits, dim=-1)
            
            # é‡‡æ ·
            choice = torch.multinomial(probs, 1).item()
            architecture[key] = search_space[key][choice]
        
        return architecture

class DARTSOptimizer:
    """DARTSä¼˜åŒ–å™¨"""
    
    def __init__(self, model, unrolled=False):
        self.model = model
        self.optimizer = torch.optim.SGD(
            self.model.parameters(),
            lr=0.01,
            momentum=0.9,
            weight_decay=0.0003
        )
        self.unrolled = unrolled
    
    def step(self, X_valid, y_valid, X_train, y_train):
        """ä¸€æ­¥ä¼˜åŒ–"""
        if self.unrolled:
            # å±•å¼€ä¼˜åŒ–
            self._unrolled_step(X_valid, y_valid, X_train, y_train)
        else:
            # è¿‘ä¼¼ä¼˜åŒ–
            self._approx_step(X_valid, y_valid)
    
    def _approx_step(self, X_valid, y_valid):
        """è¿‘ä¼¼ä¼˜åŒ–"""
        # è®¡ç®—éªŒè¯é›†ä¸Šçš„æ¢¯åº¦
        self.optimizer.zero_grad()
        loss = self.model(X_valid, y_valid)
        loss.backward()
        
        # ä½¿ç”¨éªŒè¯æ¢¯åº¦æ›´æ–°æ¶æ„å‚æ•°
        self.model.update_alphas()
        
        # ä½¿ç”¨è®­ç»ƒé›†æ›´æ–°æƒé‡
        self.optimizer.step()
```

### 136.2 è¶…å‚æ•°ä¼˜åŒ–

```python
class BayesianOptimization:
    """è´å¶æ–¯ä¼˜åŒ–"""
    
    def __init__(self, objective, search_space, acquisition='EI'):
        self.objective = objective
        self.search_space = search_space
        self.acquisition = acquisition
        
        # é«˜æ–¯è¿‡ç¨‹
        self.gp = GaussianProcessRegressor()
        
        # é‡‡é›†å‡½æ•°
        if acquisition == 'EI':
            self.acquisition_func = expected_improvement
        elif acquisition == 'UCB':
            self.acquisition_func = upper_confidence_bound
    
    def optimize(self, n_iterations=100):
        """ä¼˜åŒ–"""
        # åˆå§‹æ ·æœ¬
        X_samples = self._initial_samples(10)
        y_samples = [self.objective(x) for x in X_samples]
        
        for _ in range(n_iterations):
            # æ‹ŸåˆGP
            self.gp.fit(X_samples, y_samples)
            
            # è·å–ä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹
            next_x = self._optimize_acquisition()
            
            # è¯„ä¼°
            next_y = self.objective(next_x)
            
            # æ›´æ–°
            X_samples.append(next_x)
            y_samples.append(next_y)
        
        return min(y_samples), X_samples[np.argmin(y_samples)]
    
    def _initial_samples(self, n):
        """åˆå§‹æ ·æœ¬"""
        samples = []
        for _ in range(n):
            sample = {}
            for key, space in self.search_space.items():
                if isinstance(space, tuple):  # è¿ç»­
                    sample[key] = np.random.uniform(space[0], space[1])
                elif isinstance(space, list):  # ç¦»æ•£
                    sample[key] = np.random.choice(space)
            samples.append(sample)
        return samples
    
    def _optimize_acquisition(self):
        """ä¼˜åŒ–é‡‡é›†å‡½æ•°"""
        # åœ¨é‡‡é›†å‡½æ•°ä¸Šä¼˜åŒ–
        return np.random.choice(self._initial_samples(1))[0]
```

### 136.3 å…ƒå­¦ä¹ AutoML

```python
class MetaLearningAutoML:
    """å…ƒå­¦ä¹ AutoML"""
    
    def __init__(self, meta_features_dim=100, task_embedding_dim=32):
        # å…ƒç‰¹å¾ç¼–ç å™¨
        self.meta_encoder = nn.Sequential(
            nn.Linear(meta_features_dim, 64),
            nn.ReLU(),
            nn.Linear(64, task_embedding_dim)
        )
        
        # é¢„æµ‹å™¨
        self.predictor = nn.Linear(task_embedding_dim,