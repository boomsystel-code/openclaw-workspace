# åˆ†å¸ƒå¼è®­ç»ƒå®æˆ˜

*ç²¾é€‰çš„åˆ†å¸ƒå¼è®­ç»ƒå®æˆ˜ä»£ç *

---

## 1. Data Parallelï¼ˆæ•°æ®å¹¶è¡Œï¼‰

```python
import torch
import torch.nn as nn

# æ¨¡å‹
model = nn.Sequential(
    nn.Linear(4096, 8192),
    nn.ReLU(),
    nn.Linear(8192, 4096)
)

# å¤šGPUè®­ç»ƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ä½¿ç”¨DataParallel
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)

# è®­ç»ƒ
for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## 2. DistributedDataParallelï¼ˆåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼‰

### 2.1 åŸºç¡€è®¾ç½®

```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# åˆå§‹åŒ–
dist.init_process_group(backend='nccl', init_method='env://')

# è·å–æœ¬åœ°rank
local_rank = int(os.environ['LOCAL_RANK'])
torch.cuda.set_device(local_rank)

# æ¨¡å‹
model = MyModel().cuda(local_rank)
model = DDP(model, device_ids=[local_rank], output_device=local_rank)

# æ•°æ®é›†
train_dataset = MyDataset()
train_sampler = DistributedSampler(train_dataset, shuffle=True)
train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)

# è®­ç»ƒå¾ªç¯
model.train()
for epoch in range(num_epochs):
    train_sampler.set_epoch(epoch)  # æ¯ä¸ªepochæ‰“ä¹±æ•°æ®
    for inputs, labels in train_loader:
        inputs = inputs.cuda(local_rank, non_blocking=True)
        labels = labels.cuda(local_rank, non_blocking=True)
        
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if local_rank == 0:
        print(f"Epoch {epoch} completed")
```

### 2.2 å¯åŠ¨å‘½ä»¤

```bash
# å•èŠ‚ç‚¹å¤šGPU
torchrun --nproc_per_node=8 train.py

# å¤šèŠ‚ç‚¹
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=xxx --master_port=1234 train.py
```

---

## 3. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        inputs = inputs.cuda()
        labels = labels.cuda()
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦å‰å‘
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        
        # æ··åˆç²¾åº¦åå‘
        scaler.scale(loss).backward()
        
        # æ¢¯åº¦è£å‰ª
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # ä¼˜åŒ–å™¨æ­¥è¿›
        scaler.step(optimizer)
        scaler.update()
```

---

## 4. Gradient Checkpointing

```python
from torch.utils.checkpoint import checkpoint

class CheckpointedModel(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)
    
    def forward(self, x):
        for i, layer in enumerate(self.layers):
            if i < len(self.layers) - 1:
                # æ£€æŸ¥ç‚¹ä¸­é—´å±‚ï¼ŒèŠ‚çœæ˜¾å­˜
                x = checkpoint(layer, x)
            else:
                x = layer(x)
        return x

# ä½¿ç”¨
model = CheckpointedModel([layer1, layer2, layer3, layer4, layer5])
```

---

## 5. ZeROä¼˜åŒ–

### 5.1 DeepSpeed ZeRO

```python
import deepspeed

# DeepSpeedé…ç½®
ds_config = {
    "train_batch_size": 64,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 0.001,
            "weight_decay": 0.01
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.001,
            "warmup_num_steps": 1000
        }
    },
    "zero_optimization": {
        "stage": 3,  # ZeRO Stage 3
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True
        },
        "contiguous_gradients": True,
        "overlap_comm": True
    },
    "fp16": {
        "enabled": True,
        "auto_cast": True,
        "loss_scale": 0,
        "initial_scale_power": 16
    },
    "activation_checkpointing": {
        "enabled": True,
        "checkpoints_per_rank": 10
    }
}

# åˆå§‹åŒ–DeepSpeed
model, optimizer, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config=ds_config,
)

# è®­ç»ƒ
for epoch in range(num_epochs):
    for batch in train_loader:
        loss = model(batch)
        model.backward(loss)
        model.step()
```

### 5.2 ZeROé…ç½®è¯´æ˜

| Stage | ä¼˜åŒ–å†…å®¹ | æ˜¾å­˜å‡å°‘ |
|-------|---------|---------|
| Stage 1 | Optimizer state partitioning | ~4x |
| Stage 2 | Gradient partitioning | ~8x |
| Stage 3 | Parameter partitioning | ~16x |

---

## 6. FSDPï¼ˆFully Sharded Data Parallelï¼‰

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy

# æ¨¡å‹
model = MyModel()

# FSDPåŒ…è£…
fsdp_model = FSDP(
    model,
    device_id=torch.cuda.current_device(),
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # Stage 3
    # sharding_strategy=ShardingStrategy.SHARD_GRAD_OP,  # Stage 2
    # sharding_strategy=ShardingStrategy.NO_SHARD,  # Stage 1
    mixed_precision=MixedPrecision(
        param_dtype=torch.float16,
        reduce_dtype=torch.float32,
        buffer_dtype=torch.float16
    ),
    auto_wrap_policy=default_auto_wrap_policy,
    backward_prefetch=BackwardPrefetch.PRE_FORWARD,
    forward_prefetch=True,
)

# ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(fsdp_model.parameters(), lr=0.001)

# è®­ç»ƒ
for epoch in range(num_epochs):
    for batch in train_loader:
        outputs = fsdp_model(batch)
        loss = criterion(outputs, batch['labels'])
        
        optimizer.zero_grad()
        fsdp_model.backward(loss)
        fsdp_model.step()
```

---

## 7. Pipeline Parallelismï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰

```python
import torch.nn as nn

# åˆ†å‰²æ¨¡å‹åˆ°å¤šä¸ªGPU
class PipelineParallelModel(nn.Module):
    def __init__(self, dev0, dev1):
        super().__init__()
        self.dev0 = dev0
        self.dev1 = dev1
        
        self.stage1 = nn.Sequential(
            nn.Linear(4096, 8192),
            nn.ReLU()
        ).to(dev0)
        
        self.stage2 = nn.Sequential(
            nn.Linear(8192, 4096),
            nn.ReLU(),
            nn.Linear(4096, 1000)
        ).to(dev1)
    
    def forward(self, x):
        x = x.to(self.dev0)
        x = self.stage1(x)
        x = x.to(self.dev1)
        x = self.stage2(x)
        return x

# è®­ç»ƒ
model = PipelineParallelModel(torch.device('cuda:0'), torch.device('cuda:1'))

# æµæ°´çº¿è°ƒåº¦
microbatch_size = 32
for batch in train_loader:
    batch = batch.to('cuda:0')
    
    # åˆ†å‰²æˆmicrobatch
    microbatches = batch.split(microbatch_size)
    
    # å‰å‘
    outputs = []
    for micro in microbatches:
        output = model(micro)
        outputs.append(output)
    
    # åå‘
    for output, micro in reversed(list(zip(outputs, microbatches))):
        output.backward(micro)
```

---

## 8. Tensor Parallelismï¼ˆå¼ é‡å¹¶è¡Œï¼‰

```python
import torch
import torch.nn as nn

class TensorParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, tp_size=2):
        super().__init__()
        self.tp_size = tp_size
        
        # åˆ†å‰²æƒé‡
        self.weight = nn.Parameter(torch.empty(out_features // tp_size, in_features))
        self.bias = nn.Parameter(torch.empty(out_features // tp_size))
        
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
    
    def forward(self, x):
        # Allgatherè¾“å…¥
        world_size = dist.get_world_size()
        x_list = [torch.empty_like(x) for _ in range(world_size)]
        dist.all_gather(x_list, x)
        x = torch.cat(x_list, dim=-1)
        
        # æœ¬åœ°è®¡ç®—
        output = F.linear(x, self.weight, self.bias)
        
        # Scatterè¾“å‡º
        output_chunks = output.chunk(self.tp_size, dim=-1)
        output = output_chunks[dist.get_rank()]
        
        return output
```

---

## 9. æ¢¯åº¦ç´¯ç§¯

```python
def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):
    model.train()
    total_loss = 0
    
    accumulation_counter = 0
    
    for batch in dataloader:
        inputs, labels = batch['input'], batch['label']
        
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # ç´¯ç§¯æ¢¯åº¦
        loss = loss / accumulation_steps
        loss.backward()
        
        accumulation_counter += 1
        
        # æ¯Nä¸ªbatchæ›´æ–°ä¸€æ¬¡
        if accumulation_counter % accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * accumulation_steps
    
    return total_loss / len(dataloader)
```

---

## 10. æ¢¯åº¦å‹ç¼©

```python
def topk_gradient_compression(gradients, compression_factor=0.01):
    """Top-kæ¢¯åº¦å‹ç¼©"""
    k = int(gradients.numel() * compression_factor)
    k = max(k, 1)
    
    # æ‰¾å‡ºæœ€å¤§çš„kä¸ªå€¼
    values, indices = torch.topk(gradients.abs().flatten(), k)
    
    # åˆ›å»ºæ©ç 
    mask = torch.zeros_like(gradients)
    mask.view(-1)[indices] = 1
    
    # å‹ç¼©
    compressed = gradients * mask
    
    return compressed, mask

def decompress_gradient(compressed, mask):
    """è§£å‹ç¼©"""
    if mask is not None:
        return compressed / mask
    return compressed
```

---

## 11. åˆ†å¸ƒå¼é€šä¿¡

```python
import torch.distributed as dist

# All-reduceï¼ˆæ¢¯åº¦æ±‡æ€»ï¼‰
dist.all_reduce(tensor, op=dist.ReduceOp.SUM)

# All-gatherï¼ˆæ”¶é›†æ‰€æœ‰æ•°æ®ï¼‰
tensor_list = [torch.empty_like(tensor) for _ in range(world_size)]
dist.all_gather(tensor_list, tensor)

# Broadcastï¼ˆå¹¿æ’­ï¼‰
dist.broadcast(tensor, src=0)

# Reduceï¼ˆæ±‡æ€»ï¼‰
dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)

# Send/Recv
if rank == 0:
    dist.send(tensor, dst=1)
else:
    dist.recv(tensor, src=0)
```

---

## 12. å®Œæ•´è®­ç»ƒè„šæœ¬æ¨¡æ¿

```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def main():
    # åˆå§‹åŒ–
    dist.init_process_group(backend='nccl', init_method='env://')
    local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(local_rank)
    
    # è®¾ç½®éšæœºç§å­
    seed = 42
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # æ¨¡å‹
    model = MyModel().cuda(local_rank)
    model = DDP(model, device_ids=[local_rank])
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # æ•°æ®
    train_dataset = MyDataset()
    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)
    
    # æ··åˆç²¾åº¦
    scaler = torch.cuda.amp.GradScaler()
    
    # è®­ç»ƒ
    for epoch in range(num_epochs):
        train_sampler.set_epoch(epoch)
        
        for batch in train_loader:
            inputs = batch['input'].cuda(local_rank, non_blocking=True)
            labels = batch['label'].cuda(local_rank, non_blocking=True)
            
            optimizer.zero_grad()
            
            with torch.cuda.amp.autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        
        if local_rank == 0:
            print(f"Epoch {epoch} completed")
    
    # ä¿å­˜æ¨¡å‹
    if local_rank == 0:
        torch.save(model.state_dict(), 'checkpoint.pt')

if __name__ == '__main__':
    main()
```

---

## 13. å¯åŠ¨å‘½ä»¤æ±‡æ€»

```bash
# å•èŠ‚ç‚¹å¤šGPU
torchrun --nproc_per_node=8 train.py

# DeepSpeed
deepspeed --num_gpus=8 train.py

# FSDP
torchrun --nproc_per_node=8 --use_fsdp train.py

# Slurmç¤ºä¾‹
sbatch --nodes=4 --gpus=8 --ntasks=32 --cpus-per-task=4 train.sh
```

---

*åˆ†å¸ƒå¼è®­ç»ƒå®æˆ˜æ•´ç†å®Œæˆï¼* ğŸš€ğŸ’»
