# AI综合卷

## 第一部分：数学基础

### 第1章：线性代数
1.1 向量空间
1.2 矩阵运算
1.3 特征分解
1.4 SVD分解
1.5 矩阵分解

### 第2章：概率论
2.1 概率公理
2.2 条件概率
2.3 贝叶斯推断
2.4 期望方差
2.5 分布

### 第3章：统计推断
3.1 点估计
3.2 区间估计
3.3 假设检验
3.4 最大似然
3.5 贝叶斯估计

### 第4章：优化理论
4.1 梯度下降
4.2 随机梯度
4.3 动量方法
4.4 自适应优化
4.5 凸优化

## 第二部分：Python编程

### 第5章：基础语法
5.1 数据类型
5.2 运算符
5.3 控制流
5.4 函数

### 第6章：数据结构
6.1 列表
6.2 字典
6.3 集合
6.4 元组
6.5 堆栈队列

### 第6章：面向对象
6.1 类定义
6.2 继承
6.3 多态
6.4 封装
6.5 抽象类

### 第7章：函数式编程
7.1 lambda
7.2 map/filter
7.3 reduce
7.4 生成器
7.5 装饰器

## 第三部分：NumPy与Pandas

### 第8章：NumPy
8.1 数组创建
8.2 索引切片
8.3 广播
8.4 矩阵运算
8.5 随机数

### 第9章：Pandas
9.1 Series
9.2 DataFrame
9.3 数据选择
9.4 数据清洗
9.5 聚合

### 第10章：数据可视化
10.1 Matplotlib
10.2 Seaborn
10.3 Plotly
10.4 可视化原则

## 第四部分：机器学习

### 第11章：监督学习
11.1 线性回归
11.2 逻辑回归
11.3 决策树
11.4 SVM
11.5 KNN

### 第12章：无监督学习
12.1 K-means
12.2 层次聚类
12.3 DBSCAN
12.4 PCA
12.5 异常检测

### 第13章：集成学习
13.1 Bagging
13.2 Boosting
13.3 Random Forest
13.4 XGBoost
13.5 LightGBM

### 第14章：模型评估
14.1 交叉验证
14.2 评估指标
14.3 超参调优
14.4 偏差方差
14.5 过拟合

## 第五部分：特征工程

### 第15章：特征选择
15.1 过滤法
15.2 包裹法
15.3 嵌入法
15.4 特征重要性

### 第16章：特征提取
16.1 PCA
16.2 LDA
16.3 t-SNE
16.4 UMAP
16.5 自编码器

### 第17章：特征编码
17.1 独热编码
17.2 标签编码
17.3 目标编码
17.4 频率编码
17.5 嵌入

### 第18章：特征缩放
18.1 标准化
18.2 归一化
18.3 鲁棒缩放
18.4 分位数
18.5 稀疏编码

## 第六部分：深度学习基础

### 第19章：神经网络
19.1 感知机
19.2 MLP
19.3 前向传播
19.4 反向传播
19.5 梯度计算

### 第20章：激活函数
20.1 Sigmoid
20.2 Tanh
20.3 ReLU
20.4 LeakyReLU
20.5 GELU

### 第21章：损失函数
21.1 MSE
21.2 CrossEntropy
21.3 BCE
21.4 自定义损失
21.5 标签平滑

### 第22章：优化器
22.1 SGD
22.2 Momentum
22.3 AdaGrad
22.4 Adam
22.5 AdamW

## 第七部分：卷积神经网络

### 第23章：卷积操作
23.1 卷积核
23.2 步长填充
23.3 感受野
23.4 通道数
23.5 转置卷积

### 第24章：池化操作
24.1 最大池化
24.2 平均池化
24.3 全局池化
24.4 自适应池化
24.5 随机池化

### 第25章：BatchNorm
25.1 原理
25.2 计算
25.3 使用
25.4 变体
25.5 注意事项

### 第26章：经典架构
26.1 LeNet
26.2 AlexNet
26.3 VGG
26.4 ResNet
26.5 EfficientNet

## 第八部分：循环神经网络

### 第27章：RNN
27.1 循环结构
27.2 梯度计算
27.3 梯度消失
27.4 梯度爆炸
27.5 梯度裁剪

### 第28章：LSTM
28.1 门控机制
28.2 细胞状态
28.3 遗忘门
28.4 输入门
28.5 输出门

### 第29章：GRU
29.1 更新门
29.2 重置门
29.3 候选隐藏
29.4 隐藏状态
29.5 简化版

### 第30章：序列模型
30.1 双向RNN
30.2 深层RNN
30.3 序列到序列
30.4 注意力机制
30.5 束搜索

## 第九部分：注意力机制与Transformer

### 第31章：注意力
31.1 注意力计算
31.2 软硬注意力
31.3 注意力权重
31.4 加权求和
31.5 注意力可视化

### 第32章：自注意力
32.1 QKV
32.2 缩放点积
32.3 多头注意力
32.4 位置编码
32.5 掩码

### 第33章：Transformer
33.1 编码器
33.2 解码器
33.3 前馈网络
33.4 残差连接
33.5 层归一化

### 第34章：位置编码
34.1 绝对位置
34.2 相对位置
34.3 RoPE
34.4 ALiBi
34.5 学习位置

## 第十部分：预训练模型

### 第35章：BERT
35.1 架构
35.2 MLM预训练
35.3 NSP预训练
35.4 微调
35.5 变体

### 第36章：GPT
36.1 语言模型
36.2 单向结构
36.3 预训练
36.4 微调
36.5 提示学习

### 第37章：T5
37.1 Seq2Seq
37.2 文本到文本
37.3 统一框架
37.4 多任务
37.5 编码器解码器

### 第38章：开源LLM
38.1 LLaMA
38.2 Mistral
38.3 Falcon
38.4 Qwen
38.5 ChatGLM

## 第十一部分：生成模型

### 第39章：GAN
39.1 生成器
39.2 判别器
39.3 对抗训练
39.4 损失函数
39.5 模式崩溃

### 第40章：DCGAN
40.1 卷积GAN
40.2 架构设计
40.3 训练技巧
40.4 条件GAN
40.5 应用

### 第41章：StyleGAN
41.1 风格控制
41.2 映射网络
41.3 AdaIN
41.4 渐进增长
41.5 StyleGAN2/3

### 第42章：扩散模型
42.1 DDPM
42.2 前向过程
42.3 反向过程
42.4 采样
42.5 条件生成
