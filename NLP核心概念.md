# NLPæ ¸å¿ƒæ¦‚å¿µ

*ç²¾é€‰çš„è‡ªç„¶è¯­è¨€å¤„ç†æ ¸å¿ƒæ¦‚å¿µå’Œä»£ç *

---

## 1. æ–‡æœ¬é¢„å¤„ç†

### 1.1 Tokenization

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# åˆ†è¯
text = "Hello, world!"
tokens = tokenizer.tokenize(text)
# ['Hello', ',', 'world', '!']

# ç¼–ç 
encoding = tokenizer(text, return_tensors='pt')
# {'input_ids': tensor([[ 101, 7592, 1010, 2088, 1029,  102]]),
#  'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]),
#  'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}

# æ‰¹é‡ç¼–ç 
texts = ["Hello world", "How are you?"]
encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
```

### 1.2 ç‰¹æ®ŠToken

- `[CLS]`ï¼šåˆ†ç±»tokenï¼Œä½äºå¥é¦–
- `[SEP]`ï¼šåˆ†éš”tokenï¼Œå¥å­ç»“æŸ
- `[PAD]`ï¼šå¡«å……tokenï¼Œè¡¥é½é•¿åº¦
- `[UNK]`ï¼šæœªçŸ¥è¯

---

## 2. è¯­è¨€æ¨¡å‹

### 2.1 BERT

```python
from transformers import BertModel, BertTokenizer

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

inputs = tokenizer("Hello, world!", return_tensors='pt')
outputs = model(**inputs)

# last_hidden_state: [batch, seq_len, hidden_dim]
# pooler_output: [batch, hidden_dim]
pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token
```

### 2.2 GPT

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

input_text = "Once upon a time"
inputs = tokenizer(input_text, return_tensors='pt')

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(**inputs, max_length=100, temperature=0.7)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### 2.3 RoBERTa

```python
from transformers import RobertaModel, RobertaTokenizer

model = RobertaModel.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
```

---

## 3. æ–‡æœ¬åˆ†ç±»

### 3.1 BERTåˆ†ç±»å™¨

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

class BertClassifier(nn.Module):
    def __init__(self, num_classes, dropout=0.1):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(768, num_classes)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS]
        pooled = self.dropout(pooled)
        return self.classifier(pooled)

# è®­ç»ƒ
model = BertClassifier(num_classes=2)
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

for epoch in range(num_epochs):
    for input_ids, attention_mask, labels in train_loader:
        outputs = model(input_ids, attention_mask)
        loss = nn.CrossEntropyLoss()(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 3.2 æƒ…æ„Ÿåˆ†æ

```python
from transformers import pipeline

# ä½¿ç”¨é¢„è®­ç»ƒæƒ…æ„Ÿåˆ†æ
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
# [{'label': 'POSITIVE', 'score': 0.9998}]

# ä¸­æ–‡æƒ…æ„Ÿåˆ†æ
classifier = pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-chinanews-chinese")
result = classifier("è¿™ä¸ªäº§å“å¾ˆå¥½ç”¨")
```

---

## 4. å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

```python
from transformers import pipeline

# å®ä½“è¯†åˆ«
ner = pipeline("ner", aggregation_strategy="simple")
result = ner("John lives in New York")
# [{'entity_group': 'PER', 'word': 'John', 'score': 0.99},
#  {'entity_group': 'LOC', 'word': 'New York', 'score': 0.98}]

# ä½¿ç”¨BERTè¿›è¡ŒNER
from transformers import BertForTokenClassification, BertTokenizer

model = BertForTokenClassification.from_pretrained('dslim/bert-base-NER')
tokenizer = BertTokenizer.from_pretrained('dslim/bert-base-NER')

inputs = tokenizer("John lives in New York", return_tensors='pt', truncation=True)
outputs = model(**inputs).logits
predictions = torch.argmax(outputs, dim=2)
```

---

## 5. é—®ç­”ç³»ç»Ÿ

### 5.1 æŠ½å–å¼é—®ç­”

```python
from transformers import pipeline

# é—®ç­”
qa = pipeline("question-answering")
result = qa(question="What is the capital of France?", 
            context="Paris is the capital of France.")
# {'answer': 'Paris', 'score': 0.99, 'start': 0, 'end': 5}
```

### 5.2 BERTé—®ç­”

```python
from transformers import BertForQuestionAnswering, BertTokenizer

model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = "What is the capital of France?"
context = "Paris is the capital of France."

inputs = tokenizer(question, context, return_tensors='pt')
outputs = model(**inputs)

start_scores = outputs.start_logits
end_scores = outputs.end_logits

start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)
answer = tokenizer.decode(inputs['input_ids'][0][start_index:end_index+1])
```

---

## 6. æ–‡æœ¬ç”Ÿæˆ

### 6.1 GPTç”Ÿæˆ

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors='pt')

# è´ªå©ªæœç´¢
outputs = model.generate(inputs['input_ids'], max_length=100)
print(tokenizer.decode(outputs[0]))

# æŸæœç´¢
outputs = model.generate(inputs['input_ids'], max_length=100, num_beams=5)
print(tokenizer.decode(outputs[0]))

# Nucleusé‡‡æ ·
outputs = model.generate(inputs['input_ids'], max_length=100, 
                        do_sample=True, top_k=50, top_p=0.95)
print(tokenizer.decode(outputs[0]))
```

### 6.2 æ–‡æœ¬æ‘˜è¦

```python
from transformers import pipeline

# æ‘˜è¦
summarizer = pipeline("summarization")
result = summarizer(article, max_length=130, min_length=30)
```

---

## 7. æœºå™¨ç¿»è¯‘

```python
from transformers import pipeline

# ç¿»è¯‘
translator = pipeline("translation_en_to_fr")
result = translator("Hello, how are you?")
# [{'translation_text': 'Bonjour, comment allez-vous?'}]

# ä½¿ç”¨MarianMT
from transformers import MarianMTModel, MarianTokenizer

model_name = "Helsinki-NLP/opus-mt-en-zh"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

inputs = tokenizer("Hello, world!", return_tensors="pt")
translated = model.generate(**inputs)
result = tokenizer.decode(translated[0], skip_special_tokens=True)
```

---

## 8. æ–‡æœ¬ç›¸ä¼¼åº¦

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ç¼–ç å¥å­
model = SentenceTransformer('all-MiniLM-L6-v2')
sentences = ["I love cats", "I like animals", "The weather is nice"]
embeddings = model.encode(sentences)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = cosine_similarity([embeddings[0]], [embeddings[1]])
print(f"Similarity: {similarity[0][0]}")
```

---

## 9. æç¤ºå·¥ç¨‹

### 9.1 Few-shotæç¤º

```python
prompt = """
Classify the sentiment of these reviews:

Review: "This product is amazing!"
Sentiment: Positive

Review: "This product is terrible."
Sentiment: Negative

Review: "It's okay, not great."
Sentiment:
"""
```

### 9.2 Chain-of-Thoughtæç¤º

```python
prompt = """
Solve this problem step by step:

If I have 5 apples and I buy 3 more apples, then I eat 2 apples, 
how many apples do I have?

Let's think step by step:
1. Starting with 5 apples
2. Buying 3 more: 5 + 3 = 8 apples
3. Eating 2: 8 - 2 = 6 apples

Answer: 6 apples
```

---

## 10. å¾®è°ƒæŠ€æœ¯

### 10.1 LoRAå¾®è°ƒ

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["query", "value"],
    lora_dropout=0.05,
    bias="none",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: 884736 || all params: 124615808 || trainable%: 0.71
```

### 10.2 é‡åŒ–å¾®è°ƒ

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quantization_config,
)
```

---

*NLPæ ¸å¿ƒæ¦‚å¿µæ•´ç†å®Œæˆï¼* ğŸ“šğŸ’¬
