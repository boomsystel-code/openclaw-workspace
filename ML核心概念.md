# æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ

*ç²¾é€‰çš„æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µå’Œç®—æ³•*

---

## 1. ç›‘ç£å­¦ä¹ 

### 1.1 çº¿æ€§æ¨¡å‹

**çº¿æ€§å›å½’**ï¼š
$$y = wx + b$$

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

**é€»è¾‘å›å½’**ï¼ˆäºŒåˆ†ç±»ï¼‰ï¼š
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
proba = model.predict_proba(X_test)
```

### 1.2 å†³ç­–æ ‘

```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth=5)
model.fit(X_train, y_train)
```

### 1.3 é›†æˆå­¦ä¹ 

**éšæœºæ£®æ—**ï¼š
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
```

**æ¢¯åº¦æå‡ï¼ˆXGBoostï¼‰**ï¼š
```python
import xgboost as xgb
model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1)
model.fit(X_train, y_train)
```

### 1.4 æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

```python
from sklearn.svm import SVC
model = SVC(kernel='rbf', C=1.0)
model.fit(X_train, y_train)
```

---

## 2. æ— ç›‘ç£å­¦ä¹ 

### 2.1 èšç±»

**K-means**ï¼š
```python
from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)
model.fit(X)
labels = model.labels_
```

**DBSCAN**ï¼ˆå¯†åº¦èšç±»ï¼‰ï¼š
```python
from sklearn.cluster import DBSCAN
model = DBSCAN(eps=0.5, min_samples=5)
labels = model.fit_predict(X)
```

### 2.2 é™ç»´

**PCA**ï¼š
```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
X_reduced = pca.fit_transform(X)
```

**t-SNE**ï¼ˆéçº¿æ€§é™ç»´ï¼‰ï¼š
```python
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2)
X_2d = tsne.fit_transform(X)
```

---

## 3. æ¨¡å‹è¯„ä¼°

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# äº¤å‰éªŒè¯
scores = cross_val_score(model, X, y, cv=5)

# è¯„ä¼°æŒ‡æ ‡
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
```

---

## 4. ç‰¹å¾å·¥ç¨‹

### 4.1 ç‰¹å¾ç¼©æ”¾

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# å½’ä¸€åŒ–
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)
```

### 4.2 ç¼–ç 

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# æ ‡ç­¾ç¼–ç 
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# ç‹¬çƒ­ç¼–ç 
from sklearn.compose import ColumnTransformer
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])])
X_encoded = ct.fit_transform(X)
```

---

## 5. äº¤å‰éªŒè¯

```python
from sklearn.model_selection import KFold, StratifiedKFold

# KæŠ˜äº¤å‰éªŒè¯
kfold = KFold(n_splits=5, shuffle=True)

# åˆ†å±‚KæŠ˜
skfold = StratifiedKFold(n_splits=5, shuffle=True)
```

---

## 6. è¶…å‚è°ƒä¼˜

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# ç½‘æ ¼æœç´¢
param_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

---

## 7. åå·®-æ–¹å·®æƒè¡¡

- **é«˜åå·®**ï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼šæ¨¡å‹å¤ªç®€å•
  - è¡¨ç°ï¼šè®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®éƒ½å¾ˆé«˜
  - è§£å†³ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦ã€å¢åŠ ç‰¹å¾

- **é«˜æ–¹å·®**ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼šæ¨¡å‹å¤ªå¤æ‚
  - è¡¨ç°ï¼šè®­ç»ƒè¯¯å·®å¾ˆä½ï¼Œæµ‹è¯•è¯¯å·®å¾ˆé«˜
  - è§£å†³ï¼šå¢åŠ æ•°æ®ã€æ­£åˆ™åŒ–ã€dropout

---

*æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µæ•´ç†å®Œæˆï¼* ğŸ“š
