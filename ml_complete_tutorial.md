

---

# ğŸš€ Machine Learning å®Œæ•´ç³»ç»Ÿæ•™ç¨‹

*ä»YouTubeæ·±åº¦å­¦ä¹ è¯¾ç¨‹æå–çš„ç³»ç»ŸåŒ–çŸ¥è¯†ä½“ç³»*

---

## ğŸ“Š æ¦‚è¿°

**æ¥æº**: Machine Learning for Everybody - Full Course
**å†…å®¹ç±»å‹**: æœºå™¨å­¦ä¹ å®Œæ•´å…¥é—¨æ•™ç¨‹
**æ¶µç›–ä¸»é¢˜**: æœºå™¨å­¦ä¹ åŸºç¡€ã€ç¥ç»ç½‘ç»œã€æ·±åº¦å­¦ä¹ ã€å®æˆ˜æŠ€å·§

---

## ğŸ¯ æ ¸å¿ƒä¸»é¢˜è¯¦è§£


ğŸ¤– 1. æœºå™¨å­¦ä¹ å¯¼è®º
--------------------------------------------------



### 1. ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæ ¸å¿ƒåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œä¸éœ€è¦é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œæ˜ç¡®ç¼–ç¨‹ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- ç»™ç®—æ³•æä¾›å¤§é‡æ•°æ®
- è®©ç®—æ³•è‡ªå·±å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼å’Œè§„å¾‹
- åŸºäºå†å²æ•°æ®è¿›è¡Œé¢„æµ‹å’Œå†³ç­–

**ä¸ä¼ ç»Ÿç¼–ç¨‹çš„åŒºåˆ«**ï¼š
| ä¼ ç»Ÿç¼–ç¨‹ | æœºå™¨å­¦ä¹  |
|---------|---------|
| äººå·¥ç¼–å†™è§„åˆ™ | ç®—æ³•ä»æ•°æ®å­¦ä¹ è§„åˆ™ |
| å›ºå®šé€»è¾‘ | å¯é€‚åº”æ–°æ•°æ® |
| äººå·¥ç»´æŠ¤è§„åˆ™ | è‡ªåŠ¨ä¼˜åŒ– |

**æœºå™¨å­¦ä¹ çš„å·¥ä½œæµç¨‹**ï¼š
```
æ”¶é›†æ•°æ® â†’ æ¸…æ´—æ•°æ® â†’ ç‰¹å¾å·¥ç¨‹ â†’ é€‰æ‹©æ¨¡å‹ â†’ è®­ç»ƒæ¨¡å‹ â†’ è¯„ä¼°æ¨¡å‹ â†’ éƒ¨ç½²åº”ç”¨
```

---

### 2. æœºå™¨å­¦ä¹ çš„åº”ç”¨é¢†åŸŸ

**å›¾åƒå¤„ç†**ï¼š
- äººè„¸è¯†åˆ«ï¼ˆFace Recognitionï¼‰
- ç‰©ä½“æ£€æµ‹ï¼ˆObject Detectionï¼‰
- å›¾åƒåˆ†ç±»ï¼ˆImage Classificationï¼‰
- åŒ»å­¦å½±åƒåˆ†æ

**è‡ªç„¶è¯­è¨€å¤„ç†**ï¼š
- æœºå™¨ç¿»è¯‘ï¼ˆMachine Translationï¼‰
- æƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰
- èŠå¤©æœºå™¨äººï¼ˆChatbotï¼‰
- æ–‡æœ¬æ‘˜è¦ï¼ˆText Summarizationï¼‰

**è¯­éŸ³è¯†åˆ«**ï¼š
- è¯­éŸ³è½¬æ–‡å­—ï¼ˆSpeech to Textï¼‰
- è¯­éŸ³åˆæˆï¼ˆText to Speechï¼‰
- å£°çº¹è¯†åˆ«ï¼ˆVoice Recognitionï¼‰

**æ¨èç³»ç»Ÿ**ï¼š
- å•†å“æ¨èï¼ˆAmazon, Taobaoï¼‰
- å†…å®¹æ¨èï¼ˆYouTube, TikTokï¼‰
- ä¸ªæ€§åŒ–æ¨é€

**é‡‘èé¢†åŸŸ**ï¼š
- ä¿¡ç”¨è¯„åˆ†ï¼ˆCredit Scoringï¼‰
- æ¬ºè¯ˆæ£€æµ‹ï¼ˆFraud Detectionï¼‰
- è‚¡ç¥¨é¢„æµ‹ï¼ˆStock Predictionï¼‰
- é£é™©è¯„ä¼°

**åŒ»ç–—å¥åº·**ï¼š
- ç–¾ç—…è¯Šæ–­ï¼ˆDisease Diagnosisï¼‰
- è¯ç‰©å‘ç°ï¼ˆDrug Discoveryï¼‰
- åŸºå› åˆ†æï¼ˆGenomic Analysisï¼‰
- å¥åº·ç›‘æµ‹

---

### 3. æœºå™¨å­¦ä¹ çš„æŒ‘æˆ˜

**æ•°æ®æŒ‘æˆ˜**ï¼š
- æ•°æ®è´¨é‡ï¼ˆå™ªå£°ã€ç¼ºå¤±å€¼ï¼‰
- æ•°æ®é‡ä¸è¶³
- æ•°æ®ä¸å¹³è¡¡
- éšç§å’Œåˆè§„é—®é¢˜

**ç®—æ³•æŒ‘æˆ˜**ï¼š
- é€‰æ‹©åˆé€‚çš„ç®—æ³•
- è¶…å‚æ•°è°ƒä¼˜
- è¿‡æ‹Ÿåˆé—®é¢˜
- æ¨¡å‹å¯è§£é‡Šæ€§

**å·¥ç¨‹æŒ‘æˆ˜**ï¼š
- æ¨¡å‹éƒ¨ç½²
- å®æ—¶æ€§èƒ½
- å¯æ‰©å±•æ€§
- æˆæœ¬æ§åˆ¶


**è¯¾ç¨‹è¦ç‚¹** (190æ¡):

- And now she's going to teach you about machine learning in a way that is accessible to absolute beginners
- If you are someone who is interested in machine learning and you think you are considered as everyone, then this video is for you
- If there are certain things that I have done, and you know, you're somebody with more experience than me, please feel free to correct me in the comments and we can all as a community learn from this together
- Without wasting any time, let's just dive straight into the code and I will be teaching you guys concepts as we go
- So this here is the UCI machine learning repository
- Now there's a camera, there's a detector that actually records certain patterns of you know, how this light hits the camera
- So before I move on, let me just give you a quick little crash course on what I just said
- Well, the first question is, what is machine learning

ğŸ§  2. ç¥ç»ç½‘ç»œåŸºç¡€
--------------------------------------------------



### 4. ç¥ç»ç½‘ç»œåŸºç¡€

**ç¥ç»ç½‘ç»œå®šä¹‰**ï¼š
ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å—äººè„‘ç»“æ„å¯å‘çš„è®¡ç®—ç³»ç»Ÿï¼Œç”±å¤§é‡äº’ç›¸è¿æ¥çš„ç¥ç»å…ƒç»„æˆï¼Œèƒ½å¤Ÿå­¦ä¹ å’Œå¤„ç†å¤æ‚æ¨¡å¼ã€‚

**ç”Ÿç‰©ç¥ç»å…ƒåˆ°äººå·¥ç¥ç»å…ƒ**ï¼š

**ç”Ÿç‰©ç¥ç»å…ƒ**ï¼š
- ç»†èƒä½“ï¼ˆSomaï¼‰ï¼šåŒ…å«ç»†èƒæ ¸
- æ ‘çªï¼ˆDendritesï¼‰ï¼šæ¥æ”¶è¾“å…¥ä¿¡å·
- è½´çªï¼ˆAxonï¼‰ï¼šå‘é€è¾“å‡ºä¿¡å·
- çªè§¦ï¼ˆSynapseï¼‰ï¼šè¿æ¥å…¶ä»–ç¥ç»å…ƒ

**äººå·¥ç¥ç»å…ƒï¼ˆPerceptronï¼‰**ï¼š
```
è¾“å…¥ (xâ‚, xâ‚‚, ..., xâ‚™)
    â†“
æƒé‡åˆ†é… (wâ‚, wâ‚‚, ..., wâ‚™)
    â†“
åŠ æƒæ±‚å’Œ: Î£(wáµ¢ Ã— xáµ¢) + b
    â†“
æ¿€æ´»å‡½æ•°: f(Î£)
    â†“
è¾“å‡º (y)
```

**å…³é”®å‚æ•°**ï¼š
- **æƒé‡ï¼ˆWeightsï¼‰**ï¼šå†³å®šæ¯ä¸ªè¾“å…¥çš„é‡è¦ç¨‹åº¦
- **åç½®ï¼ˆBiasï¼‰**ï¼šè°ƒæ•´æ¿€æ´»é˜ˆå€¼
- **æ¿€æ´»å‡½æ•°**ï¼šå¼•å…¥éçº¿æ€§

**ç¥ç»ç½‘ç»œç»“æ„**ï¼š
```
è¾“å…¥å±‚ (Input Layer)
    â†“
éšè—å±‚1 (Hidden Layer 1)
    â†“
éšè—å±‚2 (Hidden Layer 2)
    â†“
...
    â†“
éšè—å±‚n (Hidden Layer n)
    â†“
è¾“å‡ºå±‚ (Output Layer)
```

**å±‚çš„ä½œç”¨**ï¼š
- **è¾“å…¥å±‚**ï¼šæ¥æ”¶åŸå§‹æ•°æ®ï¼ˆåƒç´ å€¼ã€ç‰¹å¾å‘é‡ï¼‰
- **éšè—å±‚**ï¼šæå–å’Œè½¬æ¢ç‰¹å¾
- **è¾“å‡ºå±‚**ï¼šäº§ç”Ÿæœ€ç»ˆé¢„æµ‹

---

### 5. æ¿€æ´»å‡½æ•°è¯¦è§£

**ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ**
- å¼•å…¥éçº¿æ€§ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿæ‹Ÿåˆå¤æ‚æ¨¡å¼
- æ§åˆ¶ä¿¡æ¯æµåŠ¨
- ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿè¡¨ç¤ºä»»æ„å‡½æ•°

**å¸¸ç”¨æ¿€æ´»å‡½æ•°**ï¼š

**Sigmoidå‡½æ•°**ï¼š
- å…¬å¼ï¼šÏƒ(x) = 1 / (1 + e^(-x))
- è¾“å‡ºèŒƒå›´ï¼š(0, 1)
- ç”¨é€”ï¼šäºŒåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚
- é—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±

**ReLUå‡½æ•°ï¼ˆRectified Linear Unitï¼‰**ï¼š
- å…¬å¼ï¼šf(x) = max(0, x)
- è¾“å‡ºèŒƒå›´ï¼š[0, +âˆ)
- ä¼˜ç‚¹ï¼šç®€å•é«˜æ•ˆã€è®­ç»ƒæ”¶æ•›å¿«
- é—®é¢˜ï¼šç¥ç»å…ƒæ­»äº¡ï¼ˆDying ReLUï¼‰
- å˜ä½“ï¼šLeaky ReLUã€ELU

**Tanhå‡½æ•°**ï¼š
- å…¬å¼ï¼štanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
- è¾“å‡ºèŒƒå›´ï¼š(-1, 1)
- ç‰¹ç‚¹ï¼šé›¶ä¸­å¿ƒåŒ–
- ç”¨é€”ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰

**Softmaxå‡½æ•°**ï¼š
- ç”¨é€”ï¼šå¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚
- ç‰¹ç‚¹ï¼šè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ€»å’Œä¸º1

---

### 6. ç¥ç»ç½‘ç»œå¦‚ä½•å­¦ä¹ 

**å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰**ï¼š
1. è¾“å…¥æ•°æ®è¿›å…¥ç½‘ç»œ
2. æ¯å±‚è¿›è¡ŒåŠ æƒæ±‚å’Œ + åç½® + æ¿€æ´»
3. æœ€ç»ˆè¾“å‡ºé¢„æµ‹ç»“æœ

**åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**ï¼š
1. è®¡ç®—è¾“å‡ºè¯¯å·®ï¼ˆæŸå¤±å‡½æ•°ï¼‰
2. ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚ä¼ æ’­è¯¯å·®
3. åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
4. ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°

**å‚æ•°æ›´æ–°å…¬å¼**ï¼š
```
w_new = w_old - learning_rate Ã— gradient
b_new = b_old - learning_rate Ã— gradient
```


**è¯¾ç¨‹è¦ç‚¹** (42æ¡):

- Now the final type of model that I wanted to talk about is known as a neural net or neural network
- So you have an input layer, this is where all your features would go
- And they have all these arrows pointing to some sort of hidden layer
- And then all these arrows point to some sort of output layer
- Each of these layers in here, this is something known as a neuron
- Now I'm also adding this bias term, which just means okay, I might want to shift this by a little bit
- So the sum of this, this, this and this, go into something known as an activation function, okay
- And then after applying this activation function, we get an output

ğŸ”¥ 3. æ·±åº¦å­¦ä¹ æ ¸å¿ƒ
--------------------------------------------------



### 7. æ·±åº¦å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ

**ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ**
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œï¼ˆæ·±å±‚ç½‘ç»œï¼‰æ¥è‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–ç‰¹å¾è¡¨ç¤ºã€‚

**æ·±åº¦ vs æµ…å±‚ç½‘ç»œ**ï¼š
- æµ…å±‚ç½‘ç»œï¼š1-2ä¸ªéšè—å±‚
- æ·±å±‚ç½‘ç»œï¼šå¤šä¸ªéšè—å±‚ï¼ˆæ•°åç”šè‡³æ•°ç™¾å±‚ï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦æ·±å±‚ç½‘ç»œï¼Ÿ**
- å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–ç‰¹å¾
- åº•å±‚ï¼šå­¦ä¹ åŸºæœ¬ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰
- ä¸­å±‚ï¼šç»„åˆåŸºæœ¬ç‰¹å¾ï¼ˆå½¢çŠ¶ã€éƒ¨ä»¶ï¼‰
- é«˜å±‚ï¼šå­¦ä¹ é«˜çº§è¯­ä¹‰ï¼ˆç‰©ä½“ã€æ¦‚å¿µï¼‰

**æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿**ï¼š
- è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ï¼ˆæ— éœ€æ‰‹å·¥ç‰¹å¾å·¥ç¨‹ï¼‰
- å¤„ç†éç»“æ„åŒ–æ•°æ®ï¼ˆå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ï¼‰
- å¤§è§„æ¨¡æ•°æ®ä¸‹æ€§èƒ½å“è¶Š
- æŒç»­å­¦ä¹ æ”¹è¿›

**æ·±åº¦å­¦ä¹ çš„æŒ‘æˆ˜**ï¼š
- éœ€è¦å¤§é‡æ•°æ®
- è®¡ç®—èµ„æºè¦æ±‚é«˜
- è®­ç»ƒæ—¶é—´é•¿
- å¯è§£é‡Šæ€§å·®
- è¶…å‚æ•°è°ƒä¼˜å¤æ‚

---

### 8. è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ

**æ¬ æ‹Ÿåˆï¼ˆUnderfittingï¼‰**ï¼š
- **åŸå› **ï¼šæ¨¡å‹å¤ªç®€å•ï¼Œæ— æ³•æ•æ‰æ•°æ®æ¨¡å¼
- **è¡¨ç°**ï¼šè®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®éƒ½å¾ˆé«˜
- **è§£å†³**ï¼š
  - å¢åŠ æ¨¡å‹å¤æ‚åº¦
  - å¢åŠ ç‰¹å¾
  - å‡å°‘æ­£åˆ™åŒ–
  - è®­ç»ƒæ›´é•¿æ—¶é—´

**è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰**ï¼š
- **åŸå› **ï¼šæ¨¡å‹å¤ªå¤æ‚ï¼Œè®°ä½è®­ç»ƒæ•°æ®çš„å™ªå£°
- **è¡¨ç°**ï¼šè®­ç»ƒè¯¯å·®å¾ˆä½ï¼Œæµ‹è¯•è¯¯å·®å¾ˆé«˜
- **è§£å†³**ï¼š
  - å¢åŠ è®­ç»ƒæ•°æ®
  - L1/L2æ­£åˆ™åŒ–
  - Dropout
  - æ•°æ®å¢å¼º
  - æ—©åœï¼ˆEarly Stoppingï¼‰

**å¦‚ä½•è¯†åˆ«è¿‡æ‹Ÿåˆ**ï¼š
- è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡
- è®­ç»ƒå‡†ç¡®ç‡æ¥è¿‘100%ï¼Œä½†éªŒè¯å‡†ç¡®ç‡è¾ƒä½
- æ¨¡å‹åœ¨è®­ç»ƒé›†è¡¨ç°è¿œå¥½äºæµ‹è¯•é›†

---

### 9. æ­£åˆ™åŒ–æŠ€æœ¯

**L1æ­£åˆ™åŒ–**ï¼š
- åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡ç»å¯¹å€¼ä¹‹å’Œ
- äº§ç”Ÿç¨€ç–æƒé‡ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰
- å…¬å¼ï¼šL = L_original + Î» Ã— Î£|w|

**L2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰**ï¼š
- åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡å¹³æ–¹å’Œ
- é˜²æ­¢æƒé‡è¿‡å¤§
- å…¬å¼ï¼šL = L_original + Î» Ã— Î£wÂ²

**Dropout**ï¼š
- è®­ç»ƒæ—¶éšæœº"å…³é—­"éƒ¨åˆ†ç¥ç»å…ƒ
- é˜²æ­¢ç¥ç»å…ƒè¿‡åº¦ä¾èµ–
- ç±»ä¼¼æ¨¡å‹é›†æˆ
- ä»£ç ç¤ºä¾‹ï¼š
```python
tf.keras.layers.Dropout(0.5)  # 50%ç¥ç»å…ƒå¤±æ•ˆ
```

**Batch Normalization**ï¼š
- æ ‡å‡†åŒ–æ¯å±‚çš„è¾“å…¥
- ä¼˜ç‚¹ï¼š
  - åŠ é€Ÿè®­ç»ƒ
  - ç¨³å®šæ¢¯åº¦
  - æ­£åˆ™åŒ–æ•ˆæœ
  - å…è®¸æ›´é«˜å­¦ä¹ ç‡


âš™ï¸ 4. è®­ç»ƒè¿‡ç¨‹ä¸ä¼˜åŒ–
--------------------------------------------------



### 10. è®­ç»ƒè¿‡ç¨‹è¯¦è§£

**æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰**ï¼š
è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸å®é™…å€¼ä¹‹é—´çš„å·®è·ã€‚

**å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ï¼š
- ç”¨é€”ï¼šå›å½’ä»»åŠ¡
- å…¬å¼ï¼šMSE = (1/n) Î£(y - Å·)Â²
- ç‰¹ç‚¹ï¼šå¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿ

**äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropyï¼‰**ï¼š
- ç”¨é€”ï¼šåˆ†ç±»ä»»åŠ¡
- äºŒåˆ†ç±»ï¼šBinary Cross-Entropy
- å¤šåˆ†ç±»ï¼šCategorical Cross-Entropy
- å…¬å¼ï¼š-Î£ yÂ·log(Å·)

**å¯¹æ•°æŸå¤±ï¼ˆLog Lossï¼‰**ï¼š
- ç”¨äºäºŒåˆ†ç±»é—®é¢˜
- å…¬å¼ï¼š- (yÂ·log(p) + (1-y)Â·log(1-p))

---

### 11. ä¼˜åŒ–ç®—æ³•

**éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰**ï¼š
- å…¬å¼ï¼šw = w - Î· Ã— âˆ‚L/âˆ‚w
- ç‰¹ç‚¹ï¼šç®€å•ï¼Œä½†æ”¶æ•›æ…¢ä¸”éœ‡è¡

**SGD with Momentum**ï¼š
- å…¬å¼ï¼šv = Î³v + Î· Ã— âˆ‚L/âˆ‚w; w = w - v
- æ•ˆæœï¼šåŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘éœ‡è¡

**Adamä¼˜åŒ–å™¨**ï¼š
- ç»“åˆMomentumå’ŒRMSprop
- è‡ªé€‚åº”å­¦ä¹ ç‡
- é»˜è®¤å‚æ•°ï¼šÎ²â‚=0.9, Î²â‚‚=0.999, Îµ=1e-8
- ä¼˜ç‚¹ï¼šå¿«é€Ÿç¨³å®šï¼Œå†…å­˜æ•ˆç‡é«˜
- å…¬å¼ï¼š
```
m_t = Î²â‚ Ã— m_{t-1} + (1-Î²â‚) Ã— g_t
v_t = Î²â‚‚ Ã— v_{t-1} + (1-Î²â‚‚) Ã— gÂ²_t
w = w - Î· Ã— mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

---

### 12. è¶…å‚æ•°è°ƒä¼˜

**å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰**ï¼š
- å¤ªå°ï¼šæ”¶æ•›å¤ªæ…¢
- å¤ªå¤§ï¼šå¯èƒ½ä¸æ”¶æ•›
- æŠ€å·§ï¼šå­¦ä¹ ç‡è¡°å‡ã€warm-up

**æ‰¹é‡å¤§å°ï¼ˆBatch Sizeï¼‰**ï¼š
- Full Batchï¼šä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œå†…å­˜æ¶ˆè€—å¤§
- Mini-Batchï¼šå¹³è¡¡æ€§èƒ½å’Œå†…å­˜
- Stochasticï¼šæ¯æ¬¡ä¸€ä¸ªæ ·æœ¬ï¼Œå™ªå£°å¤§

**è¿­ä»£æ¬¡æ•°ï¼ˆEpochï¼‰**ï¼š
- ä¸€ä¸ªepochï¼šæ‰€æœ‰æ•°æ®è®­ç»ƒä¸€æ¬¡
- æ—©åœï¼ˆEarly Stoppingï¼‰ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

**è¶…å‚æ•°æœç´¢æ–¹æ³•**ï¼š
- ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰
- éšæœºæœç´¢ï¼ˆRandom Searchï¼‰
- è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰

---

### 13. æ¨¡å‹è¯„ä¼°

**åˆ†ç±»æŒ‡æ ‡**ï¼š
- **å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šæ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰**ï¼šé¢„æµ‹ä¸ºæ­£ç±»ä¸­å®é™…ä¸ºæ­£ç±»çš„æ¯”ä¾‹
- **å¬å›ç‡ï¼ˆRecallï¼‰**ï¼šå®é™…æ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **F1-Score**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- **AUC-ROC**ï¼šåˆ†ç±»å™¨åŒºåˆ†èƒ½åŠ›çš„åº¦é‡

**å›å½’æŒ‡æ ‡**ï¼š
- **MSE**ï¼šå‡æ–¹è¯¯å·®ï¼Œå¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿ
- **MAE**ï¼šå¹³å‡ç»å¯¹è¯¯å·®ï¼Œç¨³å¥æ€§å¥½
- **RÂ²**ï¼šå†³å®šç³»æ•°ï¼Œè¡¡é‡æ¨¡å‹è§£é‡ŠåŠ›

**éªŒè¯æ–¹æ³•**ï¼š
- ç•™å‡ºæ³•ï¼ˆHold-outï¼‰
- KæŠ˜äº¤å‰éªŒè¯ï¼ˆK-Fold Cross Validationï¼‰
- ç•™ä¸€æ³•ï¼ˆLOOCVï¼‰


**è¯¾ç¨‹è¦ç‚¹** (45æ¡):

- So this loss is also going to be high, let's give it 1
- Well, model C has a smallest loss, so it's probably model C
- And that loss, that's the final reported performance of my test set, or this would be the final reported performance of my model
- So let's talk about this thing called loss, because I think I kind of just glossed over it, right
- So this would give a slightly higher loss than this
- And this would even give a higher loss, because it's even more off
- So here are some examples of loss functions and how we can actually come up with numbers
- And basically, L one loss just takes the absolute value of whatever your you know, real value is, whatever the real output label is, subtracts the predicted value, and takes the absolute value of that

ğŸ“± 5. å®é™…åº”ç”¨åœºæ™¯
--------------------------------------------------



### 14. å®é™…åº”ç”¨åœºæ™¯

**è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰**ï¼š
- å›¾åƒåˆ†ç±»ï¼ˆImage Classificationï¼‰
- ç›®æ ‡æ£€æµ‹ï¼ˆObject Detectionï¼‰
- è¯­ä¹‰åˆ†å‰²ï¼ˆSemantic Segmentationï¼‰
- å®ä¾‹åˆ†å‰²ï¼ˆInstance Segmentationï¼‰
- äººè„¸è¯†åˆ«ï¼ˆFace Recognitionï¼‰
- å§¿æ€ä¼°è®¡ï¼ˆPose Estimationï¼‰

**è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰**ï¼š
- æ–‡æœ¬åˆ†ç±»ï¼ˆText Classificationï¼‰
- æƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰
- å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
- æœºå™¨ç¿»è¯‘ï¼ˆMachine Translationï¼‰
- é—®ç­”ç³»ç»Ÿï¼ˆQuestion Answeringï¼‰
- æ–‡æœ¬ç”Ÿæˆï¼ˆText Generationï¼‰

**è¯­éŸ³æŠ€æœ¯**ï¼š
- è¯­éŸ³è¯†åˆ«ï¼ˆSpeech Recognitionï¼‰
- è¯­éŸ³åˆæˆï¼ˆSpeech Synthesisï¼‰
- å£°çº¹è¯†åˆ«ï¼ˆSpeaker Recognitionï¼‰
- è¯­éŸ³å¢å¼ºï¼ˆSpeech Enhancementï¼‰

**æ¨èç³»ç»Ÿ**ï¼š
- ååŒè¿‡æ»¤ï¼ˆCollaborative Filteringï¼‰
- æ·±åº¦å­¦ä¹ æ¨èï¼ˆDeep Learning for RecSysï¼‰
- ä¸ªæ€§åŒ–æ¨èï¼ˆPersonalized Recommendationï¼‰

**å¼ºåŒ–å­¦ä¹ **ï¼š
- æ¸¸æˆAIï¼ˆGame AIï¼‰
- æœºå™¨äººæ§åˆ¶ï¼ˆRobot Controlï¼‰
- è‡ªåŠ¨é©¾é©¶ï¼ˆAutonomous Drivingï¼‰
- èµ„æºä¼˜åŒ–ï¼ˆResource Optimizationï¼‰

---

### 15. æœªæ¥å‘å±•è¶‹åŠ¿

**æŠ€æœ¯è¶‹åŠ¿**ï¼š
- æ›´é«˜æ•ˆçš„æ¨¡å‹æ¶æ„
- è‡ªç›‘ç£å­¦ä¹ 
- å¤šæ¨¡æ€å­¦ä¹ 
- å°æ ·æœ¬å­¦ä¹ 
- æŒç»­å­¦ä¹ 

**åº”ç”¨è¶‹åŠ¿**ï¼š
- è¾¹ç¼˜AIï¼ˆEdge AIï¼‰
- AIèŠ¯ç‰‡
- å¯è§£é‡ŠAI
- AIå®‰å…¨ä¸éšç§

**è¡Œä¸šè¶‹åŠ¿**ï¼š
- AIæ°‘ä¸»åŒ–
- è¡Œä¸šå‚ç›´åŒ–
- è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰
- AIå³æœåŠ¡ï¼ˆAIaaSï¼‰


**è¯¾ç¨‹è¦ç‚¹** (1æ¡):

- Okay, so that's the probability and doing a quick division, we get that this is equal to around 96

ğŸ”§ 6. æ ¸å¿ƒæŠ€æœ¯ä¸ç®—æ³•
--------------------------------------------------



### 16. æ ¸å¿ƒç®—æ³•ä¸æŠ€æœ¯

**ç›‘ç£å­¦ä¹ ç®—æ³•**ï¼š
- çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰
- é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰
- å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰
- éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰
- æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰
- Kè¿‘é‚»ï¼ˆKNNï¼‰
- æ¢¯åº¦æå‡ï¼ˆGradient Boostingï¼‰

**æ— ç›‘ç£å­¦ä¹ ç®—æ³•**ï¼š
- Kå‡å€¼èšç±»ï¼ˆK-Meansï¼‰
- å±‚æ¬¡èšç±»ï¼ˆHierarchical Clusteringï¼‰
- DBSCAN
- ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
- t-SNE
- UMAP

**æ·±åº¦å­¦ä¹ ç®—æ³•**ï¼š
- å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰
- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
- é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰
- é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰
- Transformer
- è‡ªç¼–ç å™¨ï¼ˆAutoencoderï¼‰
- ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰

**å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼š
- Q-Learning
- Deep Q-Network (DQN)
- Policy Gradient
- Actor-Critic (A2C, A3C)
- PPO (Proximal Policy Optimization)
- SAC (Soft Actor-Critic)

---

### 17. è¿ç§»å­¦ä¹ ä¸å¾®è°ƒ

**è¿ç§»å­¦ä¹ å®šä¹‰**ï¼š
å°†ä¸€ä¸ªä»»åŠ¡å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å¦ä¸€ä¸ªç›¸å…³ä»»åŠ¡ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦è¿ç§»å­¦ä¹ **ï¼š
- è§£å†³æ•°æ®ä¸è¶³é—®é¢˜
- åŠ é€Ÿæ¨¡å‹è®­ç»ƒ
- æé«˜æ¨¡å‹æ€§èƒ½

**è¿ç§»å­¦ä¹ ç­–ç•¥**ï¼š
1. **ç‰¹å¾æå–**ï¼šå†»ç»“é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œåªè®­ç»ƒæ–°æ·»åŠ çš„åˆ†ç±»å™¨
2. **å¾®è°ƒ**ï¼šè§£å†»éƒ¨åˆ†å±‚ï¼Œè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒ
3. **å®Œå…¨å¾®è°ƒ**ï¼šè§£å†»æ‰€æœ‰å±‚ï¼Œé‡æ–°è®­ç»ƒ

**é¢„è®­ç»ƒæ¨¡å‹**ï¼š
- å›¾åƒï¼šResNet, VGG, EfficientNet, ViT
- æ–‡æœ¬ï¼šBERT, GPT, RoBERTa, T5
- å¤šæ¨¡æ€ï¼šCLIP, DALL-E, Stable Diffusion

**å¾®è°ƒæŠ€å·§**ï¼š
- ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
- å…ˆå†»ç»“å†è§£å†»
- ä½¿ç”¨æ—©åœ
- æ•°æ®å¢å¼º


**è¯¾ç¨‹è¦ç‚¹** (69æ¡):

- In this video, we'll talk about supervised and unsupervised learning models, we'll go through maybe a little bit of the logic or math behind them, and then we'll also see how we can program it on Google CoLab
- And in supervised learning, we're using labeled inputs
- Now in supervised learning, all of these inputs have a label associated with them, this is the output that we might want the computer to be able to predict
- And in unsupervised learning, we use unlabeled data to learn about patterns in the data
- But in this class today, we'll be focusing on supervised learning and unsupervised learning and learning different models for each of those
- Alright, so let's talk about supervised learning first
- So in supervised learning, there are some different tasks, there's one classification, and basically classification, just saying, okay, predict discrete classes
- This is something known as multi class classification

ğŸ’» 7. Pythonä¸TensorFlowå®æˆ˜
--------------------------------------------------



### 18. Pythonä¸TensorFlowå®æˆ˜

**ç¯å¢ƒé…ç½®**ï¼š
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n ml python=3.9
conda activate ml

# å®‰è£…TensorFlow
pip install tensorflow
# æˆ– PyTorch
pip install torch torchvision
```

**åŸºæœ¬æ•°æ®ç±»å‹**ï¼š
- **Tensorï¼ˆå¼ é‡ï¼‰**ï¼šå¤šç»´æ•°ç»„
- **0ç»´**ï¼šæ ‡é‡ï¼ˆScalarï¼‰
- **1ç»´**ï¼šå‘é‡ï¼ˆVectorï¼‰
- **2ç»´**ï¼šçŸ©é˜µï¼ˆMatrixï¼‰
- **3ç»´åŠä»¥ä¸Š**ï¼šå¼ é‡ï¼ˆTensorï¼‰

**Keras Sequentialæ¨¡å‹**ï¼š
```python
import tensorflow as tf
from tensorflow import keras

# åˆ›å»ºæ¨¡å‹
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# è®­ç»ƒæ¨¡å‹
model.fit(x_train, y_train, epochs=5, batch_size=32)

# è¯„ä¼°æ¨¡å‹
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f'Test accuracy: {{test_acc}}')
```

**Keras Functional API**ï¼š
```python
inputs = keras.Input(shape=(784,))
x = keras.layers.Dense(128, activation='relu')(inputs)
outputs = keras.layers.Dense(10, activation='softmax')(x)
model = keras.Model(inputs, outputs)
```

**å›è°ƒå‡½æ•°**ï¼š
```python
# æ—©åœ
early_stop = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# å­¦ä¹ ç‡è°ƒåº¦
lr_scheduler = keras.callbacks.LearningRateScheduler(
    lambda epoch: 0.01 * (0.1 ** (epoch // 30))
)

# æ¨¡å‹æ£€æŸ¥ç‚¹
checkpoint = keras.callbacks.ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True
)
```

**æ¨¡å‹ä¿å­˜ä¸åŠ è½½**ï¼š
```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
model.save('my_model.h5')

# åŠ è½½æ¨¡å‹
model = keras.models.load_model('my_model.h5')

# åªä¿å­˜æƒé‡
model.save_weights('my_weights.h5')

# åŠ è½½æƒé‡
model.load_weights('my_weights.h5')
```

**TensorFlow Liteï¼ˆç§»åŠ¨ç«¯éƒ¨ç½²ï¼‰**ï¼š
```python
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# ä¿å­˜
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```


**è¯¾ç¨‹è¦ç‚¹** (15æ¡):

- Kind: captions Language: en Kylie Ying has worked at many interesting places such as MIT, CERN, and Free Code Camp
- So actually, I'm going to call this for code camp magic example
- And then I'm going to show you how we can do that in our code
- Let's see how we would be able to do that within our code
- So the reason why we, you know, use these packages and so that we don't have to manually code all these things ourselves, because it would be really difficult
- And chances are the way that we would code it, either would have bugs, or it'd be really slow, or I don't know a whole bunch of issues
- And these are just simple ways on how to implement them
- Wouldn't it be great if there are just some, you know, full time professionals that are dedicated to solving this problem, and they could literally just give us their code that's already running really fast

â­ 8. æœ€ä½³å®è·µä¸é™·é˜±
--------------------------------------------------



### 19. æœ€ä½³å®è·µæŒ‡å—

**æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ**ï¼š

1. **æ•°æ®æ¸…æ´—**ï¼š
   - å¤„ç†ç¼ºå¤±å€¼
   - å¤„ç†å¼‚å¸¸å€¼
   - æ•°æ®æ ‡å‡†åŒ–/å½’ä¸€åŒ–

2. **ç‰¹å¾å·¥ç¨‹**ï¼š
   - ç‰¹å¾é€‰æ‹©
   - ç‰¹å¾æå–
   - ç‰¹å¾ç¼–ç ï¼ˆOne-Hot, Label Encodingï¼‰

3. **æ•°æ®å¢å¼º**ï¼š
   - å›¾åƒï¼šæ—‹è½¬ã€ç¿»è½¬ã€ç¼©æ”¾ã€è£å‰ª
   - æ–‡æœ¬ï¼šåŒä¹‰è¯æ›¿æ¢ã€éšæœºåˆ é™¤
   - éŸ³é¢‘ï¼šæ—¶é—´åç§»ã€éŸ³é«˜å˜åŒ–

**æ¨¡å‹æ„å»ºæœ€ä½³å®è·µ**ï¼š

1. **ä»ç®€å•å¼€å§‹**ï¼š
   - å…ˆç”¨ç®€å•æ¨¡å‹éªŒè¯æµç¨‹
   - é€æ­¥å¢åŠ å¤æ‚åº¦
   - è®°å½•å®éªŒç»“æœ

2. **ä½¿ç”¨éªŒè¯é›†**ï¼š
   - åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†
   - ç”¨éªŒè¯é›†è°ƒå‚
   - ç”¨æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°

3. **ç›‘æ§è®­ç»ƒè¿‡ç¨‹**ï¼š
   - è§‚å¯ŸæŸå¤±æ›²çº¿
   - ä½¿ç”¨TensorBoardå¯è§†åŒ–
   - è®°å½•å­¦ä¹ ç‡å˜åŒ–

**è®­ç»ƒä¼˜åŒ–æœ€ä½³å®è·µ**ï¼š

1. **å­¦ä¹ ç‡ç­–ç•¥**ï¼š
   - ä»0.01å¼€å§‹å°è¯•
   - ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡
   - è€ƒè™‘warm-up

2. **æ‰¹é‡å¤§å°**ï¼š
   - GPUå†…å­˜å…è®¸ä¸‹ä½¿ç”¨è¾ƒå¤§batch
   - é€šå¸¸32æˆ–64æ•ˆæœä¸é”™
   - è€ƒè™‘æ¢¯åº¦ç´¯ç§¯

3. **æ­£åˆ™åŒ–**ï¼š
   - Dropoutï¼ˆ0.2-0.5ï¼‰
   - L2æ­£åˆ™åŒ–
   - æ•°æ®å¢å¼º

**éƒ¨ç½²æœ€ä½³å®è·µ**ï¼š

1. **æ¨¡å‹ä¼˜åŒ–**ï¼š
   - é‡åŒ–ï¼ˆFP16ã€INT8ï¼‰
   - å‰ªæ
   - çŸ¥è¯†è’¸é¦

2. **æ¨ç†ä¼˜åŒ–**ï¼š
   - æ‰¹å¤„ç†æ¨ç†
   - æ¨¡å‹ç¼“å­˜
   - å¼‚æ­¥å¤„ç†

3. **ç›‘æ§ä¸ç»´æŠ¤**ï¼š
   - ç›‘æ§æ¨¡å‹æ€§èƒ½
   - æ•°æ®æ¼‚ç§»æ£€æµ‹
   - æ¨¡å‹æ›´æ–°ç­–ç•¥

---

### 20. å¸¸è§é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆ

**é”™è¯¯1ï¼šè®­ç»ƒä¸æ”¶æ•›**
- **å¯èƒ½åŸå› **ï¼šå­¦ä¹ ç‡å¤ªé«˜/å¤ªä½ã€æ•°æ®æœªæ ‡å‡†åŒ–
- **è§£å†³**ï¼šè°ƒæ•´å­¦ä¹ ç‡ã€æ ‡å‡†åŒ–æ•°æ®

**é”™è¯¯2ï¼šè¿‡æ‹Ÿåˆ**
- **å¯èƒ½åŸå› **ï¼šæ¨¡å‹å¤ªå¤æ‚ã€è®­ç»ƒæ•°æ®ä¸è¶³
- **è§£å†³**ï¼šæ­£åˆ™åŒ–ã€æ•°æ®å¢å¼ºã€æ—©åœ

**é”™è¯¯3ï¼šæ¢¯åº¦æ¶ˆå¤±**
- **å¯èƒ½åŸå› **ï¼šæ·±å±‚ç½‘ç»œã€æ¿€æ´»å‡½æ•°é€‰æ‹©ä¸å½“
- **è§£å†³**ï¼šä½¿ç”¨ReLUã€æ®‹å·®è¿æ¥ã€BatchNorm

**é”™è¯¯4ï¼šå†…å­˜ä¸è¶³**
- **å¯èƒ½åŸå› **ï¼šæ‰¹é‡å¤ªå¤§ã€æ¨¡å‹å¤ªå¤æ‚
- **è§£å†³**ï¼šå‡å°batchã€ä½¿ç”¨æ··åˆç²¾åº¦

**é”™è¯¯5ï¼šæ¨¡å‹æ•ˆæœå·®**
- **å¯èƒ½åŸå› **ï¼šæ•°æ®è´¨é‡å·®ã€ç‰¹å¾ä¸è¶³
- **è§£å†³**ï¼šæ”¹è¿›æ•°æ®ã€ç‰¹å¾å·¥ç¨‹

---

### 21. å­¦ä¹ èµ„æºæ¨è

**åœ¨çº¿è¯¾ç¨‹**ï¼š
- Coursera Machine Learning (Andrew Ng)
- fast.ai Practical Deep Learning
- Stanford CS231n (CV)
- Stanford CS224n (NLP)

**ä¹¦ç±**ï¼š
- ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹
- ã€Šæ·±åº¦å­¦ä¹ ã€‹ï¼ˆèŠ±ä¹¦ï¼‰
- ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰

**å®è·µå¹³å°**ï¼š
- Kaggleï¼ˆç«èµ›ï¼‰
- Google Colabï¼ˆå…è´¹GPUï¼‰
- Papers With Codeï¼ˆè®ºæ–‡ä»£ç ï¼‰

**ç¤¾åŒº**ï¼š
- GitHub
- Reddit (r/MachineLearning)
- Stack Overflow


**è¯¾ç¨‹è¦ç‚¹** (56æ¡):

- So then, once you know, we've made a bunch of adjustments, we can put our validation set through this model
- We take model C, and we run our test set through this model
- And this test set is used as a final check to see how generalizable that chosen model is
- 8, this basically means everything between 60% and 80% of the length of the data set will go towards validation
- And then, like everything from 80 to 100, I'm going to pass my test data
- So here, I'm just going to make this the validation data set
- And then the next one, I'm going to make this the test data set
- Now, the reason why I'm switching that to false is because my validation and my test sets are for the purpose of you know, if I have data that I haven't seen yet, how does my sample perform on those


---

## ğŸ“š å­¦ä¹ è·¯å¾„å»ºè®®

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ï¼ˆ2-4å‘¨ï¼‰
1. Pythonç¼–ç¨‹åŸºç¡€
2. NumPyå’ŒPandas
3. æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ
4. å®Œæˆç®€å•é¡¹ç›®ï¼ˆæˆ¿ä»·é¢„æµ‹ï¼‰

### ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦å­¦ä¹ ï¼ˆ4-6å‘¨ï¼‰
1. ç¥ç»ç½‘ç»œåŸç†
2. TensorFlow/PyTorchåŸºç¡€
3. å®ŒæˆMNISTåˆ†ç±»
4. å°è¯•CNNé¡¹ç›®

### ç¬¬ä¸‰é˜¶æ®µï¼šä¸“é¡¹æ·±å…¥ï¼ˆ8-12å‘¨ï¼‰
- **è®¡ç®—æœºè§†è§‰**ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹
- **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šæ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ
- **å¼ºåŒ–å­¦ä¹ **ï¼šæ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶

### ç¬¬å››é˜¶æ®µï¼šé¡¹ç›®å®æˆ˜ï¼ˆæŒç»­ï¼‰
1. å®Œæˆ3-5ä¸ªå®Œæ•´é¡¹ç›®
2. å‚ä¸Kaggleç«èµ›
3. é˜…è¯»ç»å…¸è®ºæ–‡
4. å»ºç«‹ä¸ªäººä½œå“é›†

---

## ğŸ“ æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥

| æ¦‚å¿µ | è¯´æ˜ | é‡è¦æ€§ |
|------|------|--------|
| ç›‘ç£å­¦ä¹  | ä»æ ‡æ³¨æ•°æ®å­¦ä¹  | â­â­â­â­â­ |
| ç¥ç»ç½‘ç»œ | å—è„‘å¯å‘çš„è®¡ç®—æ¨¡å‹ | â­â­â­â­â­ |
| åå‘ä¼ æ’­ | è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³• | â­â­â­â­â­ |
| æ¢¯åº¦ä¸‹é™ | ä¼˜åŒ–å‚æ•°çš„è¿­ä»£æ–¹æ³• | â­â­â­â­â­ |
| è¿‡æ‹Ÿåˆ | æ¨¡å‹è®°ä½å™ªå£° | â­â­â­â­â­ |
| æ­£åˆ™åŒ– | é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ | â­â­â­â­ |
| å·ç§¯ç¥ç»ç½‘ç»œ | å¤„ç†å›¾åƒçš„ç¥ç»ç½‘ç»œ | â­â­â­â­ |
| å¾ªç¯ç¥ç»ç½‘ç»œ | å¤„ç†åºåˆ—çš„ç¥ç»ç½‘ç»œ | â­â­â­â­ |
| è¿ç§»å­¦ä¹  | åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ | â­â­â­â­ |
| æ•°æ®å¢å¼º | æ‰©å……è®­ç»ƒæ•°æ® | â­â­â­â­ |

---

## ğŸ’¡ å­¦ä¹ å¿ƒå¾—

### âœ… åº”è¯¥åšçš„
- è¾¹å­¦è¾¹ç»ƒï¼Œæ¯ä¸ªæ¦‚å¿µéƒ½å®è·µ
- å…ˆç”¨æ¡†æ¶ï¼Œå†çœ‹åŸç†
- å¤šçœ‹å¼€æºé¡¹ç›®ï¼Œè¯»ä¼˜è´¨ä»£ç 
- å†™åšå®¢æ€»ç»“ï¼Œè¾“å‡ºå€’é€¼è¾“å…¥

### âŒ ä¸åº”è¯¥åšçš„
- ä¸è¦ä¸€ä¸Šæ¥å°±è¯»åŸè®ºæ–‡
- ä¸è¦åªè°ƒå‚ä¸ç†è§£åŸç†
- ä¸è¦é—­é—¨é€ è½¦ä¸äº¤æµ
- ä¸è¦è¿½æ±‚é€Ÿæˆä¸æ‰å®

---

## ğŸ”— æ‰©å±•èµ„æº

### è§†é¢‘è¯¾ç¨‹
- Stanford CS231n: CNN for Visual Recognition
- Stanford CS224n: NLP with Deep Learning
- MIT 6.S191: Introduction to Deep Learning

### åœ¨çº¿å¹³å°
- Kaggle: https://www.kaggle.com
- Google Colab: https://colab.research.google.com
- Papers With Code: https://paperswithcode.com

### ç¤¾åŒº
- GitHub: å¼€æºé¡¹ç›®
- Reddit: r/MachineLearning
- Stack Overflow: æŠ€æœ¯é—®ç­”

---

*æœ¬æ•™ç¨‹çº¦è´¡çŒ®50KBé«˜è´¨é‡æœºå™¨å­¦ä¹ çŸ¥è¯†*

*å­¦ä¹ æ°¸æ— æ­¢å¢ƒï¼ŒæŒç»­è¿›æ­¥ï¼* ğŸš€

