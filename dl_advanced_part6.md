# ğŸš€ æ·±åº¦å­¦ä¹ é«˜çº§æŠ€æœ¯ Part 6

*æœ€å‰æ²¿çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸åº”ç”¨*

---

## 100. å¤§è¯­è¨€æ¨¡å‹æ¶æ„æ·±å…¥

### 100.1 Transformeræ¶æ„è¯¦è§£

**æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒå…¬å¼**ï¼š

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜åŠ¿**ï¼š
- å¹¶è¡Œè®¡ç®—ä¸åŒå­ç©ºé—´çš„ç‰¹å¾
- æ•æ‰ä¸åŒç±»å‹çš„å…³ç³»
- å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›

### 100.2 Transformerå˜ä½“

**Longformerï¼ˆé•¿ä¸Šä¸‹æ–‡ï¼‰**ï¼š
- æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼š$O(n \times w)$
- å…¨å±€æ³¨æ„åŠ›ï¼šé€‰æ‹©æ€§ä½ç½®
- ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼

```python
class LongformerAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention_window = config.attention_window
        self.attention_probs_dropout = nn.Dropout(config.attention_probs_dropout)
    
    def forward(self, hidden_states, attention_mask=None):
        # æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
        seq_len = hidden_states.size(1)
        
        # è®¡ç®—çª—å£æ³¨æ„åŠ›
        output = []
        for i in range(seq_len):
            start = max(0, i - self.attention_window)
            end = min(seq_len, i + self.attention_window + 1)
            
            window = hidden_states[:, start:end]
            attn_weights = self._compute_attention(hidden_states[:, i:i+1], window)
            output.append(attn_weights @ window)
        
        return torch.cat(output, dim=1)
```

**BigBirdï¼ˆç¨€ç–æ³¨æ„åŠ›ï¼‰**ï¼š
- éšæœºæ³¨æ„åŠ›ï¼š$O(n)$
- çª—å£æ³¨æ„åŠ›ï¼š$O(n \times w)$
- å…¨å±€tokenï¼š$O(n)$

```python
class BigBirdAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.random_ratio = config.random_ratio
        self.blocksize = config.blocksize
    
    def forward(self, query, key, value):
        # 1. éšæœºæ³¨æ„åŠ›
        random_attn = self._random_attention(query, key)
        
        # 2. çª—å£æ³¨æ„åŠ›ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
        window_attn = self._sliding_attention(query, key)
        
        # 3. å…¨å±€tokenæ³¨æ„åŠ›
        global_attn = self._global_attention(query, key, value)
        
        # èåˆ
        attn = random_attn + window_attn + global_attn
        attn = attn / 3.0
        
        return attn @ value
```

### 100.3 ä½ç½®ç¼–ç 

**RoPEï¼ˆRotary Position Embeddingï¼‰**ï¼š

```python
class RotaryEmbedding(nn.Module):
    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        
        # è®¡ç®—é¢‘ç‡
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
    
    def forward(self, seq_len, device):
        # ç”Ÿæˆä½ç½®
        positions = torch.arange(seq_len, device=device).float()
        
        # è®¡ç®—è§’åº¦
        angles = positions.unsqueeze(1) * self.inv_freq.unsqueeze(0)
        
        # å¤æ•°è¡¨ç¤º
        return torch.polar(torch.ones_like(angles), angles)

def rotate_half(x):
    """æ—‹è½¬ä¸€åŠ"""
    x1, x2 = x.chunk(2, dim=-1)
    return torch.cat([-x2, x1], dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin):
    """åº”ç”¨RoPE"""
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

**ALiBiï¼ˆAttention with Linear Biasesï¼‰**ï¼š

```python
class ALiBiAttention(nn.Module):
    def __init__(self, num_heads, slope_init=0.5):
        super().__init__()
        self.num_heads = num_heads
        
        # å¯å­¦ä¹ çš„æ–œç‡
        self.slopes = nn.Parameter(torch.tensor([slope_init * (2 ** (-i)) 
                                                for i in range(num_heads)]))
    
    def forward(self, query, key, value):
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_scores = torch.matmul(query, key.transpose(-2, -1))
        
        # ALiBiåç½®
        seq_len = query.size(2)
        positions = torch.arange(seq_len, device=query.device).float()
        
        # åˆ›å»ºåç½®çŸ©é˜µ
        bias = positions.unsqueeze(0) - positions.unsqueeze(1)
        bias = bias.abs() * -self.slopes.view(1, self.num_heads, 1, 1)
        
        # åº”ç”¨åç½®
        attn_scores = attn_scores + bias
        
        # Softmax
        attn_probs = F.softmax(attn_scores, dim=-1)
        
        return torch.matmul(attn_probs, value)
```

### 100.4 KV Cacheä¼˜åŒ–

```python
class KVCache:
    def __init__(self, batch_size, num_heads, head_dim, max_seq_len):
        self.key_cache = torch.zeros(
            batch_size, num_heads, max_seq_len, head_dim
        )
        self.value_cache = torch.zeros(
            batch_size, num_heads, max_seq_len, head_dim
        )
        self.seen_tokens = 0
    
    def update(self, key_states, value_states):
        """æ›´æ–°cache"""
        self.key_cache[:, :, self.seen_tokens:self.seen_tokens+key_states.size(2)] = key_states
        self.value_cache[:, :, self.seen_tokens:self.seen_tokens+value_states.size(2)] = value_states
        self.seen_tokens += key_states.size(2)
    
    def get(self, seq_len):
        """è·å–cache"""
        return self.key_cache[:, :, :seq_len], self.value_cache[:, :, :seq_len]

class CacheAwareAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.cache = None
    
    def forward(self, query, key, value, use_cache=False):
        if use_cache and self.cache is not None:
            # æ›´æ–°cache
            self.cache.update(key, value)
            key, value = self.cache.get(self.cache.seen_tokens)
        
        # æ ‡å‡†æ³¨æ„åŠ›
        attn_scores = torch.matmul(query, key.transpose(-2, -1))
        attn_scores = attn_scores / (query.size(-1) ** 0.5)
        attn_probs = F.softmax(attn_scores, dim=-1)
        
        return torch.matmul(attn_probs, value)
```

---

## 101. æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰

### 101.1 MoEæ¶æ„

```python
class MixtralExpert(nn.Module):
    """Mixtralä¸“å®¶æ¨¡å—"""
    
    def __init__(self, hidden_size, intermediate_size):
        super().__init__()
        self.w1 = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.w2 = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.w3 = nn.Linear(hidden_size, intermediate_size, bias=False)
    
    def forward(self, x):
        # SwiGLUæ¿€æ´»
        hidden = self.w1(x)
        gate = F.silu(hidden)
        hidden = self.w3(x) * gate
        return self.w2(hidden)

class MoELayer(nn.Module):
    """MoEå±‚"""
    
    def __init__(self, num_experts, top_k, hidden_size, intermediate_size):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        
        # ä¸“å®¶è·¯ç”±å™¨
        self.router = nn.Linear(hidden_size, num_experts)
        
        # ä¸“å®¶æ± 
        self.experts = nn.ModuleList([
            MixtralExpert(hidden_size, intermediate_size)
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        # è®¡ç®—è·¯ç”±logits
        router_logits = self.router(x)
        router_probs = F.softmax(router_logits, dim=-1)
        
        # é€‰æ‹©top-kä¸“å®¶
        top_k_weights, top_k_indices = torch.topk(
            router_probs, self.top_k, dim=-1
        )
        
        # å½’ä¸€åŒ–æƒé‡
        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)
        
        # ä¸“å®¶è®¡ç®—
        final_hidden = torch.zeros_like(x)
        
        for expert_idx in range(self.num_experts):
            expert_mask = (top_k_indices == expert_idx).any(dim=-1)
            if expert_mask.any():
                expert_weight = top_k_weights[expert_mask].sum(dim=-1, keepdim=True)
                expert_output = self.experts[expert_idx](x[expert_mask])
                final_hidden[expert_mask] += expert_output * expert_weight.unsqueeze(-1)
        
        return final_hidden

class SwitchTransformer(nn.Module):
    """Switch Transformer"""
    
    def __init__(self, num_experts, hidden_size, intermediate_size):
        super().__init__()
        self.router = nn.Linear(hidden_size, num_experts)
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, intermediate_size),
                nn.ReLU(),
                nn.Linear(intermediate_size, hidden_size)
            )
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        # è·¯ç”±
        router_logits = self.router(x)
        routing_weights = F.softmax(router_logits, dim=-1)
        
        # è½¯åˆ‡æ¢ï¼šåŠ æƒæ‰€æœ‰ä¸“å®¶
        final_output = torch.zeros_like(x)
        
        for expert_idx, expert in enumerate(self.experts):
            expert_output = expert(x)
            final_output += expert_output * routing_weights[:, expert_idx:expert_idx+1]
        
        return final_output
```

### 101.2 è´Ÿè½½å‡è¡¡æŸå¤±

```python
class LoadBalancedLoss:
    def __init__(self, num_experts, expert_capacity_factor=1.0):
        self.num_experts = num_experts
        self.expert_capacity_factor = expert_capacity_factor
    
    def compute_loss(self, routing_probs, expert_indices):
        # 1. ä¸“å®¶é€‰æ‹©é¢‘ç‡
        expert_selection_freq = torch.bincount(
            expert_indices.view(-1), 
            minlength=self.num_expects
        ).float()
        
        # 2. ç†æƒ³å‡åŒ€åˆ†å¸ƒ
        ideal_freq = torch.ones_like(expert_selection_freq) / self.num_experts
        
        # 3. è´Ÿè½½å‡è¡¡æŸå¤±
        load_balance_loss = F.kl_div(
            routing_probs.mean(dim=0).log(),
            ideal_freq,
            reduction='batchmean'
        )
        
        # 4. å¸®åŠ©è¾…åŠ©æŸå¤±ï¼šè·¯ç”±æ›´å‡åŒ€
        aux_loss = self.num_experts * torch.sum(
            routing_probs * (routing_probs.mean(dim=0) - ideal_freq)
        )
        
        return load_balance_loss + aux_loss
```

### 101.3 GLaMæ¶æ„

```python
class GLaM(nn.Module):
    """GLaM: Generalist Language Model"""
    
    def __init__(self, num_layers, hidden_size, num_heads, 
                 num_experts, top_k):
        super().__init__()
        self.layers = nn.ModuleList([
            GLaMTransformerLayer(hidden_size, num_heads, num_experts, top_k)
            for _ in range(num_layers)
        ])
        
        self.attention_norm = nn.LayerNorm(hidden_size)
        self.ffn_norm = nn.LayerNorm(hidden_size)
    
    def forward(self, x, attention_mask=None):
        # æ®‹å·®è¿æ¥
        residual = x
        
        # è‡ªæ³¨æ„åŠ›
        x = self.attention_norm(x)
        x = x + self._causal_attention(x, attention_mask)
        
        # MoE FFN
        x = self.ffn_norm(x)
        x = x + self._moe_feed_forward(x)
        
        return x
```

---

## 102. æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ·±å…¥

### 102.1 å‘é‡æ£€ç´¢

```python
class VectorDatabase:
    """å‘é‡æ•°æ®åº“"""
    
    def __init__(self, embedding_dim=768):
        self.embedding_dim = embedding_dim
        self.embeddings = []
        self.documents = []
        self.metadata = []
    
    def add(self, documents, embeddings, metadata=None):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents.extend(documents)
        self.embeddings.extend(embeddings)
        self.metadata.extend(metadata or [{}] * len(documents))
    
    def search(self, query_embedding, top_k=10):
        """å‘é‡æ£€ç´¢"""
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = [
            cosine_similarity(query_embedding, emb) 
            for emb in self.embeddings
        ]
        
        # Top-k
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        return [
            {
                'document': self.documents[idx],
                'embedding': self.embeddings[idx],
                'metadata': self.metadata[idx],
                'score': similarities[idx]
            }
            for idx in top_indices
        ]

class FAISSVectorStore:
    """FAISSå‘é‡å­˜å‚¨"""
    
    def __init__(self, embedding_dim=768, metric='cosine'):
        self.embedding_dim = embedding_dim
        self.metric = metric
        
        if metric == 'cosine':
            self.index = faiss.IndexFlatIP(embedding_dim)
        else:
            self.index = faiss.IndexFlatL2(embedding_dim)
        
        self.documents = []
    
    def add(self, documents, embeddings):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents.extend(documents)
        
        # å½’ä¸€åŒ–ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦éœ€è¦ï¼‰
        if self.metric == 'cosine':
            faiss.normalize_L2(embeddings)
        
        self.index.add(embeddings.astype('float32'))
    
    def search(self, query_embedding, top_k=10):
        """æ£€ç´¢"""
        # å½’ä¸€åŒ–æŸ¥è¯¢
        if self.metric == 'cosine':
            faiss.normalize_L2(query_embedding)
        
        scores, indices = self.index.search(
            query_embedding.astype('float32'), top_k
        )
        
        return [
            {
                'document': self.documents[idx],
                'score': score
            }
            for idx, score in zip(indices[0], scores[0])
        ]
```

### 102.2 æ£€ç´¢ç­–ç•¥

```python
class HybridRetrieval:
    """æ··åˆæ£€ç´¢"""
    
    def __init__(self, dense_index, sparse_index):
        self.dense_retriever = dense_index
        self.sparse_retriever = sparse_index
    
    def search(self, query, top_k=10, alpha=0.5):
        """æ··åˆæ£€ç´¢ï¼šç¨ å¯† + ç¨€ç–"""
        # ç¨ å¯†æ£€ç´¢
        dense_results = self.dense_retriever.search(query, top_k * 2)
        
        # ç¨€ç–æ£€ç´¢
        sparse_results = self.sparse_retriever.search(query, top_k * 2)
        
        # èåˆåˆ†æ•°
        fused_results = self._fusion(
            dense_results, 
            sparse_results, 
            top_k, 
            alpha
        )
        
        return fused_results
    
    def _fusion(self, dense_results, sparse_results, top_k, alpha):
        """åˆ†æ•°èåˆ"""
        # å½’ä¸€åŒ–
        max_dense = max(r['score'] for r in dense_results) if dense_results else 1
        max_sparse = max(r['score'] for r in sparse_results) if sparse_results else 1
        
        # RRFèåˆ
        fused = {}
        for rank, result in enumerate(dense_results):
            doc_id = result['document']['id']
            rrf_score = 1.0 / (rank + 60)
            fused[doc_id] = {
                'document': result['document'],
                'score': alpha * (result['score'] / max_dense) + 
                        (1 - alpha) * rrf_score
            }
        
        for rank, result in enumerate(sparse_results):
            doc_id = result['document']['id']
            if doc_id in fused:
                fused[doc_id]['score'] += (1 - alpha) * rrf_score
            else:
                rrf_score = 1.0 / (rank + 60)
                fused[doc_id] = {
                    'document': result['document'],
                    'score': (1 - alpha) * rrf_score
                }
        
        # æ’åº
        sorted_results = sorted(fused.values(), 
                               key=lambda x: x['score'], 
                               reverse=True)
        
        return sorted_results[:top_k]

class Reranker:
    """é‡æ’åºæ¨¡å‹"""
    
    def __init__(self, model_name="cross-encoder/ms-marco-MiniLM"):
        from sentence_transformers import CrossEncoder
        
        self.model = CrossEncoder(model_name)
    
    def rerank(self, query, candidates, top_k=5):
        """é‡æ’åº"""
        # æ„å»ºè¾“å…¥å¯¹
        pairs = [(query, cand['document']['text']) for cand in candidates]
        
        # è®¡ç®—åˆ†æ•°
        scores = self.model.predict(pairs)
        
        # æ’åº
        for cand, score in zip(candidates, scores):
            cand['rerank_score'] = score
        
        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
        
        return candidates[:top_k]
```

### 102.3 Agentic RAG

```python
class AgenticRAG:
    """æ™ºèƒ½ä½“é©±åŠ¨çš„RAG"""
    
    def __init__(self, llm, retriever, tools):
        self.llm = llm
        self.retriever = retriever
        self.tools = tools
        self.memory = []
    
    def query(self, question):
        """æ™ºèƒ½æŸ¥è¯¢"""
        # 1. åˆ†æé—®é¢˜
        analysis = self._analyze_question(question)
        
        # 2. å†³å®šæ£€ç´¢ç­–ç•¥
        if analysis['needs_knowledge']:
            # æ‰§è¡Œæ£€ç´¢
            retrieved_docs = self._retrieve(question)
            
            # é‡æ’åº
            reranked_docs = self._rerank(question, retrieved_docs)
            
            # ç”Ÿæˆä¸Šä¸‹æ–‡
            context = self._build_context(reranked_docs)
        else:
            context = ""
        
        # 3. è°ƒç”¨å·¥å…·ï¼ˆå¦‚éœ€è¦è®¡ç®—ï¼‰
        if analysis['needs_calculation']:
            result = self._call_tool(analysis['tool'], analysis['params'])
            context += f"\n\n[å·¥å…·ç»“æœ]: {result}"
        
        # 4. ç”Ÿæˆå›ç­”
        answer = self._generate_answer(question, context)
        
        # 5. è®°å¿†
        self.memory.append({
            'question': question,
            'answer': answer,
            'context': context
        })
        
        return answer
    
    def _analyze_question(self, question):
        """åˆ†æé—®é¢˜ç±»å‹"""
        system_prompt = """
        åˆ†æç”¨æˆ·é—®é¢˜çš„ç±»å‹ï¼š
        1. æ˜¯å¦éœ€è¦å¤–éƒ¨çŸ¥è¯†ï¼Ÿ
        2. æ˜¯å¦éœ€è¦è®¡ç®—æˆ–å·¥å…·è°ƒç”¨ï¼Ÿ
        3. éœ€è¦ä»€ä¹ˆç±»å‹çš„å·¥å…·ï¼Ÿ
        
        è¿”å›JSONæ ¼å¼ã€‚
        """
        
        response = self.llm.generate(
            system_prompt + question,
            format='json'
        )
        
        return json.loads(response)
    
    def _retrieve(self, query):
        """æ£€ç´¢"""
        return self.retriever.search(query, top_k=10)
    
    def _rerank(self, query, documents):
        """é‡æ’åº"""
        reranker = Reranker()
        return reranker.rerank(query, documents, top_k=5)
    
    def _build_context(self, documents):
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context = "\n\n".join([
            f"æ–‡æ¡£{i+1}: {doc['document']['text']}"
            for i, doc in enumerate(documents)
        ])
        return context
```

### 102.4 RAGè¯„ä¼°

```python
class RAGEvaluator:
    """RAGè¯„ä¼°"""
    
    def __init__(self):
        self.metrics = {
            'faithfulness': FaithfulnessMetric(),
            'answer_relevance': AnswerRelevanceMetric(),
            'context_precision': ContextPrecisionMetric(),
            'context_recall': ContextRecallMetric()
        }
    
    def evaluate(self, question, answer, contexts, ground_truth=None):
        """è¯„ä¼°RAGç³»ç»Ÿ"""
        results = {}
        
        for metric_name, metric in self.metrics.items():
            results[metric_name] = metric.compute(
                question, answer, contexts, ground_truth
            )
        
        # ç»¼åˆåˆ†æ•°
        results['overall'] = np.mean(list(results.values()))
        
        return results

class FaithfulnessMetric:
    """å¿ å®åº¦ï¼šå›ç­”æ˜¯å¦å¿ å®äºæ£€ç´¢çš„ä¸Šä¸‹æ–‡"""
    
    def compute(self, question, answer, contexts, ground_truth=None):
        # æå–ç­”æ¡ˆä¸­çš„å£°æ˜
        claims = self._extract_claims(answer)
        
        # æ£€æŸ¥æ¯ä¸ªå£°æ˜æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
        supported_claims = []
        for claim in claims:
            if self._claim_supported(claim, contexts):
                supported_claims.append(claim)
        
        return len(supported_claims) / len(claims) if claims else 0
```

---

## 103. å¼ºåŒ–å­¦ä¹ é«˜çº§æŠ€å·§

### 103.1 åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ 

```python
class ApeXDistributedRL:
    """Ape-Xåˆ†å¸ƒå¼RL"""
    
    def __init__(self, num_actors, num_learners, env_fn):
        self.num_actors = num_actors
        self.num_learners = num_learners
        
        # ä¼˜å…ˆçº§ç»éªŒå›æ”¾
        self.replay_buffer = PrioritizedReplayBuffer(capacity=1000000)
        
        # å…±äº«å‚æ•°æœåŠ¡å™¨
        self.param_server = ParameterServer()
        
        # å‚ä¸è€…
        self.actors = [
            Actor(i, env_fn, self.param_server, self.replay_buffer)
            for i in range(num_actors)
        ]
        
        # å­¦ä¹ è€…
        self.learners = [
            Learner(j, self.param_server, self.replay_buffer)
            for j in range(num_learners)
        ]
    
    def train(self, total_timesteps):
        # å¯åŠ¨æ‰€æœ‰ç»„ä»¶
        for actor in self.actors:
            actor.start()
        
        for learner in self.learners:
            learner.start()
        
        # ç­‰å¾…å®Œæˆ
        for actor in self.actors:
            actor.join()
        
        for learner in self.learners:
            learner.join()

class PrioritizedReplayBuffer:
    """ä¼˜å…ˆçº§ç»éªŒå›æ”¾"""
    
    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):
        self.capacity = capacity
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        self.frame = 1
        
        self.buffer = []
        self.priorities = np.zeros(capacity)
        self.max_priority = 1.0
        
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ"""
        max_priority = self.priorities[:self.position].max() if self.position > 0 else self.max_priority
        
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.position] = (state, action, reward, next_state, done)
        
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        """é‡‡æ ·"""
        # è®¡ç®—beta
        beta = min(1.0, self.beta_start + (1.0 - self.beta_start) * 
                   self.frame / self.beta_frames)
        self.frame += 1
        
        # æ¦‚ç‡é‡‡æ ·
        probabilities = self.priorities[:len(self.buffer)] ** self.alpha
        probabilities = probabilities / probabilities.sum()
        
        # é‡‡æ ·
        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities, replace=False)
        
        # é‡è¦æ€§é‡‡æ ·æƒé‡
        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)
        weights = weights / weights.max()
        
        # è·å–æ ·æœ¬
        samples = [self.buffer[idx] for idx in indices]
        
        return samples, indices, torch.tensor(weights)
    
    def update_priorities(self, indices, priorities):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority + 1e-6
```

### 103.2 ç¦»çº¿å¼ºåŒ–å­¦ä¹ 

```python
class ConservativeQLearning:
    """ä¿å®ˆQå­¦ä¹ ï¼ˆCQLï¼‰"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super().__init__()
        
        # Qç½‘ç»œ
        self.Q = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # ç›®æ ‡Qç½‘ç»œ
        self.Q_target = copy.deepcopy(self.Q)
        
        self.optimizer = optim.Adam(self.Q.parameters(), lr=1e-4)
        
        # CQLå‚æ•°
        self.alpha = 1.0  # ä¿å®ˆç³»æ•°
    
    def update(self, batch):
        states, actions, rewards, next_states, dones = batch
        
        # è®¡ç®—å½“å‰Qå€¼
        q_values = self.Q(torch.cat([states, actions], dim=1))
        
        # TDç›®æ ‡
        with torch.no_grad():
            next_q = self.Q_target(next_states).max(dim=1)[0]
            target = rewards + (1 - dones) * 0.99 * next_q
        
        # æ ‡å‡†MSEæŸå¤±
        td_loss = F.mse_loss(q_values.squeeze(), target)
        
        # CQLæŸå¤±ï¼šåœ¨é‡‡æ ·åŠ¨ä½œä¸Šçš„Qå€¼æœŸæœ›
        cql_q_values = self.Q(torch.cat([states, actions], dim=1))
        cql_loss = self.alpha * (cql_q_values.mean() - q_values.mean())
        
        # æ€»æŸå¤±
        loss = td_loss + cql_loss
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._soft_update()
        
        return loss.item()
    
    def _soft_update(self):
        for param, target_param in zip(self.Q.parameters(), self.Q_target.parameters()):
            target_param.data.copy_(
                0.005 * param.data + 0.995 * target_param.data
            )

class ImplicitQLearning:
    """éšå¼Qå­¦ä¹ ï¼ˆIQLï¼‰"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=256, tau=0.005):
        super().__init__()
        
        # ä»·å€¼ç½‘ç»œ
        self.V = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Qç½‘ç»œ
        self.Q = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # ç­–ç•¥ç½‘ç»œ
        self.policy = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        self.V_optimizer = optim.Adam(self.V.parameters(), lr=1e-4)
        self.Q_optimizer = optim.Adam(self.Q.parameters(), lr=1e-4)
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=1e-4)
        
        self.tau = tau
    
    def update(self, batch):
        states, actions, rewards, next_states, dones = batch
        
        # 1. æ›´æ–°Vç½‘ç»œ
        with torch.no_grad():
            q_values = self.Q(torch.cat([states, actions], dim=1))
        v_values = self.V(states)
        
        v_loss = F.mse_loss(v_values, q_values.detach())
        
        self.V_optimizer.zero_grad()
        v_loss.backward()
        self.V_optimizer.step()
        
        # 2. æ›´æ–°Qç½‘ç»œ
        q_values = self.Q(torch.cat([states, actions], dim=1))
        with torch.no_grad():
            next_v = self.V(next_states)
        q_targets = rewards + (1 - dones) * 0.99 * next_v
        q_loss = F.mse_loss(q_values, q_targets)
        
        self.Q_optimizer.zero_grad()
        q_loss.backward()
        self.Q_optimizer.step()
        
        # 3. æ›´æ–°ç­–ç•¥
        policy_actions = self.policy(states)
        q_values_policy = self.Q(torch.cat([states, policy_actions], dim=1))
        
        # æœŸæœ›ï¼šå–Qå€¼é«˜çš„åŠ¨ä½œ
        policy_loss = -q_values_policy.mean()
        
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()
```

### 103.3 å¼ºåŒ–å­¦ä¹ è®­ç»ƒæŠ€å·§

```python
class RLTrainer:
    """RLè®­ç»ƒå™¨"""
    
    def __init__(self, agent, env):
        self.agent = agent
        self.env = env
        
        # å½’ä¸€åŒ–
        self.state_normalizer = RunningMeanStd(shape=env.observation_space.shape)
        self.reward_scaler = RunningMeanStd()
    
    def train(self, num_episodes=1000, eval_interval=10):
        """è®­ç»ƒ"""
        for episode in range(num_episodes):
            state = self.env.reset()
            
            episode_reward = 0
            done = False
            
            while not done:
                # å½’ä¸€åŒ–çŠ¶æ€
                state_norm = self.state_normalizer.normalize(state)
                
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.select_action(state_norm)
                
                # æ‰§è¡Œ
                next_state, reward, done, _ = self.env.step(action)
                
                # å½’ä¸€åŒ–å¥–åŠ±
                reward = self.reward_scaler.normalize(reward)
                
                # å­˜å‚¨
                self.agent.replay_buffer.push(
                    state_norm, action, reward, 
                    self.state_normalizer.normalize(next_state), done
                )
                
                # æ›´æ–°
                if len(self.agent.replay_buffer) > self.agent.batch_size:
                    batch = self.agent.replay_buffer.sample(self.agent.batch_size)
                    self.agent.update(batch)
                
                state = next_state
                episode_reward += reward
            
            # è¯„ä¼°
            if episode % eval_interval == 0:
                eval_reward = self.evaluate()
                print(f"Episode {episode}: Train {episode_reward:.2f}, Eval {eval_reward:.2f}")
    
    def evaluate(self, num_episodes=10):
        """è¯„ä¼°"""
        total_reward = 0
        
        for _ in range(num_episodes):
            state = self.env.reset()
            done = False
            
            while not done:
                state_norm = self.state_normalizer.normalize(state)
                action = self.agent.select_action(state_norm, eval=True)
                
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                state = next_state
        
        return total_reward / num_episodes

class RunningMeanStd:
    """è¿è¡Œå‡å€¼æ ‡å‡†å·®"""
    
    def __init__(self, shape=()):
        self.mean = np.zeros(shape)
        self.var = np.ones(shape)
        self.count = 0
    
    def update(self, x):
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = len(x)
        
        self.mean, self.var, self.count = self._update(
            self.mean, self.var, self.count,
            batch_mean, batch_var, batch_count
        )
    
    def _update(self, mean, var, count, batch_mean, batch_var, batch_count):
        delta = batch_mean - mean
        new_count = count + batch_count
        new_mean = mean + delta * batch_count / new_count
        new_var = var + batch_var * batch_count + \
                  delta ** 2 * count * batch_count / new_count
        new_var /= new_count
        return new_mean, new_var, new_count
    
    def normalize(self, x):
        return (x - self.mean) / np.sqrt(self.var + 1e-8)
```

---

## 104. å¤šæ¨¡æ€å­¦ä¹ 

### 104.1 CLIPæ·±å…¥è§£æ

```python
class CLIPModel(nn.Module):
    """CLIPæ¨¡å‹"""
    
    def __init__(self, vision_model, text_model, projection_dim=512):
        super().__init__()
        
        self.vision_model = vision_model
        self.text_model = text_model
        
        # æŠ•å½±å±‚
        self.visual_projection = nn.Linear(vision_model.hidden_size, projection_dim)
        self.text_projection = nn.Linear(text_model.hidden_size, projection_dim)
        
        # æ¸©åº¦å‚æ•°
        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def forward(self, images, input_ids, attention_mask):
        # å›¾åƒç‰¹å¾
        image_features = self.vision_model(images)
        image_embeddings = self.visual_projection(image_features)
        
        # æ–‡æœ¬ç‰¹å¾
        text_features = self.text_model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        text_embeddings = self.text_projection(text_features)
        
        # å½’ä¸€åŒ–
        image_embeddings = F.normalize(image_embeddings, dim=-1)
        text_embeddings = F.normalize(text_embeddings, dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logits = torch.matmul(image_embeddings, text_embeddings.T) * self.temperature.exp()
        
        return logits
    
    def contrastive_loss(self, image_embeddings, text_embeddings):
        """å¯¹æ¯”æŸå¤±"""
        # å›¾åƒåˆ°æ–‡æœ¬
        logits = torch.matmul(image_embeddings, text_embeddings.T) / self.temperature
        labels = torch.arange(len(logits)).to(logits.device)
        
        loss_i2t = F.cross_entropy(logits, labels)
        loss_t2i = F.cross_entropy(logits.T, labels)
        
        return (loss_i2t + loss_t2i) / 2

class CLIPVisionEncoder(nn.Module):
    """CLIPè§†è§‰ç¼–ç å™¨"""
    
    def __init__(self, embed_dim=768, image_size=224, patch_size=16):
        super().__init__()
        
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        
        # å›¾åƒåˆ†å—åµŒå…¥
        self.patch_embedding = nn.Conv2d(
            in_channels=3,
            out_channels=embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
        
        # ä½ç½®ç¼–ç 
        self.position_embedding = nn.Embedding(self.num_patches + 1, embed_dim)
        self.register_buffer(
            "position_ids", 
            torch.arange(self.num_patches + 1).expand(1, -1)
        )
        
        # Class token
        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Transformer
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=12),
            num_layers=12
        )
        
        # å±‚å½’ä¸€åŒ–
        self.layernorm = nn.LayerNorm(embed_dim)
    
    def forward(self, images):
        # åˆ†å—
        patches = self.patch_embedding(images)
        patches = patches.flatten(2).transpose(1, 2)
        
        # æ·»åŠ class token
        class_token = self.class_token.expand(patches.size(0), -1, -1)
        patches = torch.cat([class_token, patches], dim=1)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        patches = patches + self.position_embedding(self.position_ids)
        
        # Transformer
        features = self.transformer(patches)
        
        # CLS token
        return self.layernorm(features[:, 0, :])
```

### 104.2 å¤šæ¨¡æ€å¤§æ¨¡å‹

```python
class LLaVAModel(nn.Module):
    """LLaVAå¤šæ¨¡æ€æ¨¡å‹"""
    
    def __init__(self, vision_tower, language_model, mm_projection_dim=512):
        super().__init__()
        
        self.vision_tower = vision_tower
        self.language_model = language_model
        
        # è§†è§‰æŠ•å½±
        self.mm_projector = nn.Sequential(
            nn.Linear(vision_tower.hidden_size, 4096),
            nn.GELU(),
            nn.Linear(4096, language_model.hidden_size)
        )
        
        # å›¾åƒæ ‡è®°
        self.image_newline = nn.Parameter(
            torch.zeros(1, 1, language_model.hidden_size)
        )
    
    def forward(self, images, input_ids, attention_mask):
        # å›¾åƒç‰¹å¾
        image_features = self.vision_tower(images)
        
        # æŠ•å½±åˆ°è¯­è¨€æ¨¡å‹ç©ºé—´
        image_features = self.mm_projector(image_features)
        
        # æ‰¾åˆ°è¾“å…¥ä¸­çš„å›¾åƒå ä½ç¬¦ä½ç½®
        image_token_mask = (input_ids == IMAGE_TOKEN_ID)
        
        # åœ¨å ä½ç¬¦ä½ç½®æ’å…¥å›¾åƒç‰¹å¾
        input_ids_new = []
        attention_mask_new = []
        image_features_new = []
        
        for b in range(input_ids.size(0)):
            ids = []
            masks = []
            feats = []
            
            for i, (id_, is_image) in enumerate(zip(input_ids[b], image_token_mask[b])):
                if is_image:
                    # æ’å…¥å›¾åƒç‰¹å¾
                    feats.append(image_features[b])
                else:
                    ids.append(id_)
                    masks.append(attention_mask[b