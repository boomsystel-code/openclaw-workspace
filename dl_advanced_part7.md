# ğŸš€ æ·±åº¦å­¦ä¹ é«˜çº§æŠ€æœ¯ Part 7

*æ›´å¤šå‰æ²¿æŠ€æœ¯ä¸å®è·µ*

---

## 105. æ¨¡å‹è’¸é¦ä¸å‹ç¼©

### 105.1 çŸ¥è¯†è’¸é¦

```python
class KnowledgeDistillation:
    """çŸ¥è¯†è’¸é¦"""
    
    def __init__(self, teacher_model, student_model, 
                 temperature=2.0, alpha=0.5):
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        self.alpha = alpha
    
    def train_epoch(self, train_loader):
        self.student.train()
        total_loss = 0
        
        for batch in train_loader:
            inputs, targets = batch
            
            # æ•™å¸ˆé¢„æµ‹ï¼ˆä¸æ›´æ–°ï¼‰
            with torch.no_grad():
                teacher_logits = self.teacher(inputs)
                teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)
            
            # å­¦ç”Ÿé¢„æµ‹
            student_logits = self.student(inputs)
            student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)
            
            # è’¸é¦æŸå¤±ï¼ˆKLæ•£åº¦ï¼‰
            distill_loss = F.kl_div(
                student_log_probs, 
                teacher_probs,
                reduction='batchmean'
            ) * (self.temperature ** 2)
            
            # æ ‡å‡†æŸå¤±
            ce_loss = F.cross_entropy(student_logits, targets)
            
            # æ€»æŸå¤±
            loss = self.alpha * distill_loss + (1 - self.alpha) * ce_loss
            
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            total_loss += loss.item()
        
        return total_loss / len(train_loader)

class SelfDistillation:
    """è‡ªè’¸é¦"""
    
    def __init__(self, model):
        self.model = model
        self.ema_model = copy.deepcopy(model)
        
        # EMAå‚æ•°
        self.ema_decay = 0.999
    
    def train_epoch(self, train_loader):
        self.model.train()
        
        for batch in train_loader:
            inputs, targets = batch
            
            # å½“å‰æ¨¡å‹é¢„æµ‹
            logits = self.model(inputs)
            
            # EMAæ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                ema_logits = self.ema_model(inputs)
            
            # è‡ªè’¸é¦æŸå¤±
            loss = self._self_distillation_loss(logits, targets, ema_logits)
            
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            # æ›´æ–°EMA
            self._update_ema()
        
        return loss.item()
    
    def _self_distillation_loss(self, logits, targets, ema_logits):
        # æ ‡å‡†CEæŸå¤±
        ce_loss = F.cross_entropy(logits, targets)
        
        # è’¸é¦æŸå¤±
        student_log_probs = F.log_softmax(logits, dim=1)
        ema_probs = F.softmax(ema_logits, dim=1)
        distill_loss = F.kl_div(
            student_log_probs, 
            ema_probs,
            reduction='batchmean'
        )
        
        return ce_loss + 0.1 * distill_loss
    
    def _update_ema(self):
        for param, ema_param in zip(
            self.model.parameters(), 
            self.ema_model.parameters()
        ):
            ema_param.data = self.ema_decay * ema_param.data + \
                            (1 - self.ema_decay) * param.data
```

### 105.2 é‡åŒ–

```python
class DynamicQuantization:
    """åŠ¨æ€é‡åŒ–"""
    
    def __init__(self, model):
        self.model = model
    
    def quantize(self):
        """é‡åŒ–æ¨¡å‹"""
        import torch.quantization
        
        # åŠ¨æ€é‡åŒ–
        quantized_model = torch.quantization.quantize_dynamic(
            self.model,
            {nn.Linear, nn.LSTM, nn.LSTMCell, nn.GRUCell, nn.GRUCell},
            dtype=torch.qint8
        )
        
        return quantized_model

class StaticQuantization:
    """é™æ€é‡åŒ–"""
    
    def __init__(self, model, dataloader):
        self.model = model
        self.dataloader = dataloader
    
    def quantize(self):
        """é™æ€é‡åŒ–"""
        import torch.quantization
        
        # å‡†å¤‡é‡åŒ–
        self.model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        torch.quantization.prepare(self.model, inplace=True)
        
        # æ ¡å‡†
        self.model.eval()
        for batch in self.dataloader:
            with torch.no_grad():
                self.model(batch)
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        quantized_model = torch.quantization.convert(
            self.model, inplace=False
        )
        
        return quantized_model

class GPTQQuantization:
    """GPTQé‡åŒ–"""
    
    def __init__(self, model, dataloader):
        self.model = model
        self.dataloader = dataloader
    
    def quantize(self, bits=4, perchannel=True):
        """GPTQé‡åŒ–"""
        import cudaq
        
        # åˆå§‹åŒ–GPTQ
        gptq = cudaq.GPTQ(self.model)
        gptq.quantize(self.dataloader, bits=bits, perchannel=perchannel)
        
        # ä¿å­˜é‡åŒ–æ¨¡å‹
        gptq.save('quantized_model')
        
        return gptq.model
```

### 105.3 å‰ªæ

```python
class MagnitudePruning:
    """å¹…åº¦å‰ªæ"""
    
    def __init__(self, model, pruning_ratio=0.3):
        self.model = model
        self.pruning_ratio = pruning_ratio
    
    def prune(self):
        """å‰ªæ"""
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # è®¡ç®—é˜ˆå€¼
                weights = module.weight.data.abs()
                threshold = np.percentile(
                    weights.cpu().numpy(), 
                    self.pruning_ratio * 100
                )
                
                # åˆ›å»ºæ©ç 
                mask = weights > threshold
                
                # åº”ç”¨å‰ªæ
                module.weight.data = module.weight.data * mask.float()
                module.weight.grad = None
    
    def iterative_pruning(self, epochs, prune_interval):
        """è¿­ä»£å‰ªæ"""
        for epoch in range(epochs):
            # è®­ç»ƒ
            self.train_epoch()
            
            # å‰ªæ
            if epoch % prune_interval == 0:
                self.prune()
                print(f"Epoch {epoch}: Pruned {self.pruning_ratio * 100}%")

class StructuredPruning:
    """ç»“æ„åŒ–å‰ªæ"""
    
    def __init__(self, model, pruning_ratio=0.3):
        self.model = model
        self.pruning_ratio = pruning_ratio
    
    def prune_conv_channels(self):
        """å‰ªæå·ç§¯é€šé“"""
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Conv2d):
                # è®¡ç®—æ¯ä¸ªé€šé“çš„L1èŒƒæ•°
                channel_norms = module.weight.data.abs().sum(dim=(1, 2, 3))
                
                # é€‰æ‹©è¦å‰ªæçš„é€šé“
                num_prune = int(len(channel_norms) * self.pruning_ratio)
                prune_indices = torch.argsort(channel_norms)[:num_prune]
                
                # æ›´æ–°è¾“å…¥é€šé“
                new_in_channels = module.in_channels - num_prune
                
                # åˆ›å»ºæ–°çš„å·ç§¯å±‚
                new_conv = nn.Conv2d(
                    new_in_channels,
                    module.out_channels,
                    kernel_size=module.kernel_size,
                    stride=module.stride,
                    padding=module.padding
                )
                
                # å¤åˆ¶ä¿ç•™çš„é€šé“
                keep_indices = [i for i in range(module.in_channels) 
                               if i not in prune_indices]
                new_conv.weight.data = module.weight.data[keep_indices]
                
                # æ›¿æ¢
                setattr(self.model, name, new_conv)
```

---

## 106. åˆ†å¸ƒå¼è®­ç»ƒæ·±å…¥

### 106.1 FSDP

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.wrap import wrap

class FSDPTrainer:
    """FSDPè®­ç»ƒå™¨"""
    
    def __init__(self, model, optimizer, lr=0.01):
        # åŒ…è£…æ¨¡å‹
        self.model = FSDP(model)
        
        self.optimizer = optimizer(self.model.parameters(), lr=lr)
    
    def train_epoch(self, dataloader):
        self.model.train()
        
        for batch in dataloader:
            # ç§»åŠ¨åˆ°GPU
            batch = batch.cuda()
            
            # å‰å‘
            loss = self.model(batch)
            
            # åå‘
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ­¥éª¤
            self.optimizer.step()
    
    def save_checkpoint(self, path):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        FSDP.save_state_dict(
            self.model.state_dict(),
            path,
            rank0_only=True
        )
    
    def load_checkpoint(self, path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        FSDP.load_state_dict(
            self.model.state_dict(),
            path
        )

class FSDPConfig:
    """FSDPé…ç½®"""
    
    def __init__(self):
        self.sharding_strategy = 'FULL_SHARD'  # 'FULL_SHARD', 'SHARD_GRAD_OP', 'NO_SHARD'
        self.backward_prefetch = 'PRE_FORWARD'  # 'PRE_FORWARD', 'POST_FORWARD'
        self.forward_prefetch = True
        self.activation_checkpointing = True
        self.cpu_offload = False
```

### 106.2 DeepSpeed

```python
import deepspeed

class DeepSpeedTrainer:
    """DeepSpeedè®­ç»ƒå™¨"""
    
    def __init__(self, model, args):
        self.model, self.optimizer, _, _ = deepspeed.initialize(
            model=model,
            model_parameters=model.parameters(),
            args=args
        )
    
    def train_epoch(self, dataloader):
        self.model.train()
        
        for batch in dataloader:
            # å‰å‘
            loss = self.model(batch)
            
            # åå‘
            self.model.backward(loss)
            self.model.step()
    
    def save_checkpoint(self, tag):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        self.model.save_checkpoint(tag)

# DeepSpeedé…ç½®æ–‡ä»¶
DEEPSPEED_CONFIG = {
    "train_batch_size": 32,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 0.001,
            "betas": [0.9, 0.999],
            "eps": 1e-8
        }
    },
    "fp16": {
        "enabled": True,
        "loss_scale": 0,
        "initial_scale_power": 16
    },
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu"
        },
        "allgather_partitions": True,
        "allgather_bucket_size": 5e8,
        "reduce_bucket_size": 5e8
    },
    "activation_checkpointing": {
        "partition_activations": True,
        "cpu_checkpointing": True
    }
}
```

### 106.3 Megatron-LM

```python
class MegatronTrainer:
    """Megatron-LMè®­ç»ƒå™¨"""
    
    def __init__(self, model, args):
        self.model = model
        self.args = args
    
    def train_epoch(self, train_dataloader):
        # è®¾ç½®
        self.model.set_train_batch_size(args.global_batch_size)
        
        # è¿­ä»£
        for iteration, batch in enumerate(train_dataloader):
            # ç­‰å¾…æ•°æ®
            batch = self._get_batch(batch)
            
            # å‰å‘
            loss = self.model(batch)
            
            # åå‘
            self.model.backward(loss)
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.model.step()
    
    def _get_batch(self, batch):
        """å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
        # å®ç°æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œçš„æ•°æ®åˆ†å‰²
        return batch

class PipelineParallelism:
    """æµæ°´çº¿å¹¶è¡Œ"""
    
    def __init__(self, model, devices):
        self.devices = devices
        self.model = model
        self.split_sizes = self._calculate_split_sizes()
    
    def _calculate_split_sizes(self):
        """è®¡ç®—åˆ†å‰²å¤§å°"""
        total_params = sum(p.numel() for p in self.model.parameters())
        per_device = total_params // len(self.devices)
        
        # åŸºäºå±‚çš„åˆ†å‰²
        layer_sizes = []
        for name, param in self.model.named_parameters():
            layer_sizes.append((name, param.numel()))
        
        return layer_sizes
```

---

## 107. è§†è§‰Transformeræ·±å…¥

### 107.1 ViTå˜ä½“

```python
class SwinTransformer(nn.Module):
    """Swin Transformer"""
    
    def __init__(self, img_size=224, patch_size=4, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24]):
        super().__init__()
        
        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)
        
        # å¤šä¸ªé˜¶æ®µ
        self.stages = nn.ModuleList()
        self.downsample_layers = nn.ModuleList()
        
        for i in range(4):
            stage = SwinTransformerBlock(
                dim=embed_dim * (2 ** i),
                num_heads=num_heads[i],
                window_size=7,
                depth=depths[i]
            )
            self.stages.append(stage)
            
            if i < 3:
                downsample = PatchMerging(
                    dim=embed_dim * (2 ** i),
                    out_dim=embed_dim * (2 ** (i + 1))
                )
                self.downsample_layers.append(downsample)
        
        # åˆ†ç±»å¤´
        self.norm = nn.LayerNorm(embed_dim * 16)
        self.head = nn.Linear(embed_dim * 16, num_classes)
    
    def forward(self, x):
        x = self.patch_embed(x)
        
        for i, stage in enumerate(self.stages):
            x = stage(x)
            
            if i < 3:
                x = self.downsample_layers[i](x)
        
        x = self.norm(x[:, 0])  # CLS token
        return self.head(x)

class SwinTransformerBlock(nn.Module):
    """Swin Transformerå—"""
    
    def __init__(self, dim, num_heads, window_size=7, depth=2):
        super().__init__()
        
        self.blocks = nn.ModuleList()
        for _ in range(depth):
            block = SwinAttentionBlock(
                dim=dim,
                num_heads=num_heads,
                window_size=window_size
            )
            self.blocks.append(block)
    
    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

class ShiftedWindowAttention(nn.Module):
    """ç§»ä½çª—å£æ³¨æ„åŠ›"""
    
    def __init__(self, dim, num_heads, window_size=7):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        
        self.qkv = nn.Linear(dim, dim * 3)
        self.attention = nn.MultiheadAttention(dim, num_heads)
    
    def forward(self, x):
        B, H, W, C = x.shape
        
        # ç§»ä½çª—å£
        x = self._shifted_window(x)
        
        # çª—å£åˆ†åŒº
        x = self._window_partition(x)
        
        # æ³¨æ„åŠ›
        x = self.attention(x, x, x)
        
        # çª—å£åˆå¹¶
        x = self._window_reverse(x)
        
        return x
    
    def _shifted_window(self, x):
        """ç§»ä½çª—å£"""
        shift_size = self.window_size // 2
        return torch.roll(x, shifts=(-shift_size, -shift_size), dims=(1, 2))
```

### 107.2 DeiT

```python
class DeiT(nn.Module):
    """Data-efficient Image Transformers"""
    
    def __init__(self, img_size=224, patch_size=16, num_classes=1000,
                 embed_dim=768, depth=12, num_heads=12):
        super().__init__()
        
        # Patch embedding
        self.patch_embed = nn.Conv2d(3, embed_dim, patch_size, patch_size)
        
        # Class token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Position embedding
        self.pos_embed = nn.Parameter(torch.zeros(1, 197, embed_dim))
        
        # Transformer
        self.blocks = nn.ModuleList([
            TransformerEncoderLayer(embed_dim, num_heads)
            for _ in range(depth)
        ])
        
        # è’¸é¦token
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # åˆ†ç±»å¤´
        self.head = nn.Linear(embed_dim, num_classes)
        self.head_dist = nn.Linear(embed_dim, num_classes)
    
    def forward(self, x):
        B = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x).flatten(2).transpose(1, 2)
        
        # æ·»åŠ token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        dist_tokens = self.dist_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # Transformer
        for block in self.blocks:
            x = block(x)
        
        # åˆ†ç±»
        return self.head(x[:, 0]), self.head_dist(x[:, 1])
```

### 107.3 è§†è§‰Prompt Tuning

```python
class VisualPromptTuning(nn.Module):
    """è§†è§‰Prompt Tuning"""
    
    def __init__(self, embed_dim=768, num_prompts=5, num_classes=1000):
        super().__init__()
        
        # å¯å­¦ä¹ çš„æç¤º
        self.prompts = nn.Parameter(
            torch.randn(num_prompts, embed_dim) * 0.02
        )
        
        # å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹
        self.backbone = load_pretrained_vit()
        for param in self.backbone.parameters():
            param.requires_grad = False
        
        # åˆ†ç±»å¤´
        self.head = nn.Linear(embed_dim, num_classes)
    
    def forward(self, images):
        # è·å–å›¾åƒç‰¹å¾
        features = self.backbone(images)
        
        # æ·»åŠ æç¤º
        batch_size = features.size(0)
        prompts = self.prompts.unsqueeze(0).expand(batch_size, -1, -1)
        
        # æ‹¼æ¥
        x = torch.cat([features[:, :1], prompts, features[:, 1:]], dim=1)
        
        # é€šè¿‡Transformer
        x = self.backbone.encoder(x)
        
        # CLS tokenç”¨äºåˆ†ç±»
        return self.head(x[:, 0])
```

---

## 108. æ‰©æ•£æ¨¡å‹é«˜çº§

### 108.1 Stable Diffusion XL

```python
class StableDiffusionXL(nn.Module):
    """Stable Diffusion XL"""
    
    def __init__(self, unet, vae, text_encoder, text_encoder_2):
        super().__init__()
        
        self.unet = unet
        self.vae = vae
        self.text_encoder = text_encoder
        self.text_encoder_2 = text_encoder_2
        
        # æ¯”ä¾‹å› å­
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
    
    def encode_prompt(self, prompt, prompt_2=None):
        """ç¼–ç æç¤º"""
        # CLIPæ–‡æœ¬ç¼–ç å™¨1
        prompt_embeds = self.text_encoder(
            prompt, output_hidden_states=True
        )
        pooled_prompt_embeds = prompt_embeds[0]
        prompt_embeds = prompt_embeds.hidden_states[-2]
        
        # CLIPæ–‡æœ¬ç¼–ç å™¨2
        prompt_embeds_2 = self.text_encoder_2(
            prompt_2 or prompt, output_hidden_states=True
        )
        prompt_embeds_2 = prompt_embeds_2.hidden_states[-2]
        
        # æ‹¼æ¥
        prompt_embeds = torch.cat([prompt_embeds, prompt_embeds_2], dim=-1)
        
        return prompt_embeds, pooled_prompt_embeds
    
    def decode_latents(self, latents):
        """è§£ç æ½œåœ¨å˜é‡"""
        latents = latents / self.vae.config.scaling_factor
        images = self.vae.decode(latents).sample
        return images
    
    def train_step(self, prompt, image):
        """è®­ç»ƒæ­¥éª¤"""
        # ç¼–ç å›¾åƒ
        latents = self.vae.encode(image).latent_dist.sample()
        latents = latents * self.vae.config.scaling_factor
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        timesteps = torch.randint(0, 1000, (latents.shape[0],))
        noisy_latents = self._add_noise(latents, noise, timesteps)
        
        # ç¼–ç æç¤º
        prompt_embeds, pooled_prompt_embeds = self.encode_prompt(prompt)
        
        # é¢„æµ‹å™ªå£°
        noise_pred = self.unet(
            noisy_latents, timesteps,
            encoder_hidden_states=prompt_embeds,
            added_cond_kwargs={'text_embeds': pooled_prompt_embeds}
        ).sample
        
        # æŸå¤±
        loss = F.mse_loss(noise_pred, noise)
        return loss
    
    def _add_noise(self, latents, noise, timesteps):
        """æ·»åŠ å™ªå£°"""
        return self.scheduler.add_noise(latents, noise, timesteps)
```

### 108.2 ControlNet

```python
class ControlNetConditioningEmbedding(nn.Module):
    """ControlNetæ¡ä»¶ç¼–ç """
    
    def __init__(self, conditioning_channels=3, image_size=512, 
                 block_out_channels=(16, 32, 96, 256)):
        super().__init__()
        
        self.conv_in = nn.Conv2d(conditioning_channels, 16, 3, padding=1)
        
        self.blocks = nn.ModuleList()
        for i in range(len(block_out_channels)):
            block = nn.Sequential(
                nn.Conv2d(16, 16, 3, padding=1),
                nn.SiLU(),
                nn.Conv2d(16, block_out_channels[i], 3, padding=1, stride=2)
            )
            self.blocks.append(block)
        
        self.zero_convs = nn.ModuleList([
            nn.Conv2d(block_out_channels[i], block_out_channels[i], 1)
            for i in range(len(block_out_channels))
        ])
    
    def forward(self, conditioning):
        # ç¼–ç 
        feature_maps = []
        x = self.conv_in(conditioning)
        
        for block, zero_conv in zip(self.blocks, self.zero_convs):
            x = block(x)
            feature_maps.append(zero_conv(x))
        
        return feature_maps

class ControlledUNet(nn.Module):
    """å¸¦ControlNetçš„U-Net"""
    
    def __init__(self, base_model, controlnet):
        super().__init__()
        
        self.base_model = base_model
        self.controlnet = controlnet
        
        # æ—¶é—´æ­¥æ¡ä»¶
        self.time_embed = base_model.time_embed
        self.add_time_proj = base_model.add_time_proj
        self.add_position_norm = base_model.add_position_norm
    
    def forward(self, sample, timestep, conditioning):
        # ControlNetæ¡ä»¶
        controlnet_residuals = self.controlnet(conditioning)
        
        # æ—¶é—´æ­¥
        timesteps_proj = self.add_time_proj(timestep)
        timesteps_proj = self.time_embed(timesteps_proj)
        
        # ä¸»UNet
        return self.base_model(
            sample, 
            timesteps_proj,
            controlnet_residuals=controlnet_residuals
        )
```

### 108.3 å›¾åƒç¼–è¾‘

```python
class ImageEditingPipeline:
    """å›¾åƒç¼–è¾‘æµæ°´çº¿"""
    
    def __init__(self, sd_model, edit_model):
        self.sd_model = sd_model
        self.edit_model = edit_model
    
    def edit(self, image, source_prompt, target_prompt, strength=0.5):
        """ç¼–è¾‘å›¾åƒ"""
        # ç¼–ç æºå›¾åƒ
        latents = self.sd_model.vae.encode(image).latent_dist.sample()
        latents = latents * self.sd_model.vae.config.scaling_factor
        
        # æ·»åŠ å™ªå£°ï¼ˆæ ¹æ®strengthï¼‰
        noise = torch.randn_like(latents)
        timesteps = int((1 - strength) * 1000)
        noisy_latents = self.sd_model.scheduler.add_noise(
            latents, noise, 
            torch.tensor([timesteps] * len(latents))
        )
        
        # ç¼–ç æç¤º
        target_embeds = self.sd_model.encode_prompt(target_prompt)
        
        # æ‰©æ•£
        edited_latents = self.sd_model.unet(
            noisy_latents,
            torch.tensor([timesteps]),
            encoder_hidden_states=target_embeds
        ).sample
        
        # è§£ç 
        edited_image = self.sd_model.decode_latents(edited_latents)
        
        return edited_image
```

---

## 109. è‡ªåŠ¨é©¾é©¶æ·±åº¦å­¦ä¹ 

### 109.1 BEVæ„ŸçŸ¥

```python
class BEVFormer(nn.Module):
    """BEVFormer"""
    
    def __init__(self, encoder, decoder, embed_dim=200):
        super().__init__()
        
        # å›¾åƒç¼–ç å™¨
        self.encoder = encoder
        
        # BEVæŸ¥è¯¢
        self.bev_embed = nn.Embedding(200 * 200, embed_dim)
        
        # ä¸´æ—¶æ³¨æ„åŠ›
        self.temporal_attention = TemporalSelfAttention(embed_dim)
        
        # BEVè§£ç å™¨
        self.decoder = decoder
        
        # æ£€æµ‹å¤´
        self.bbox_head = DetectionHead(embed_dim)
    
    def forward(self, images, timestamps):
        batch_size = images.size(0)
        
        # å¤šè§†è§’ç‰¹å¾æå–
        img_features = self.encoder(images)
        
        # ç”ŸæˆBEVæŸ¥è¯¢
        bev_queries = self.bev_embed.weight.unsqueeze(0).expand(
            batch_size, -1, -1
        )
        
        # æ—¶é—´èåˆ
        bev_features = self.temporal_attention(
            bev_queries, img_features, timestamps
        )
        
        # è§£ç 
        outputs = self.decoder(bev_features)
        
        # æ£€æµ‹
        predictions = self.bbox_head(outputs)
        
        return predictions

class TemporalSelfAttention(nn.Module):
    """æ—¶é—´è‡ªæ³¨æ„åŠ›"""
    
    def __init__(self, embed_dim, num_heads=8):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm = nn.LayerNorm(embed_dim)
    
    def forward(self, bev_queries, img_features, timestamps):
        # æ”¶é›†å†å²ç‰¹å¾
        history_features = self._collect_history(img_features, timestamps)
        
        # æ—¶é—´æ³¨æ„åŠ›
        fused = torch.cat([bev_queries] + history_features, dim=1)
        
        attended, _ = self.attention(bev_queries, fused, fused)
        
        return self.norm(bev_queries + attended)
```

### 109.2 ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶

```python
class EndToEndAutonomousDriving(nn.Module):
    """ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶"""
    
    def __init__(self, perception, planning, control):
        super().__init__()
        
        self.perception = perception
        self.planning = planning
        self.control = control
    
    def forward(self, sensors_data):
        """ç«¯åˆ°ç«¯æ¨ç†"""
        # æ„ŸçŸ¥
        scene_features = self.perception(sensors_data)
        
        # è§„åˆ’
        trajectory = self.planning(scene_features)
        
        # æ§åˆ¶
        control_signals = self.control(trajectory)
        
        return control_signals
    
    def train_step(self, sensors_data, expert_actions):
        """è®­ç»ƒæ­¥éª¤"""
        # æ„ŸçŸ¥æŸå¤±
        perception_loss = self._perception_loss(sensors_data)
        
        # è§„åˆ’æŸå¤±
        planning_loss = self._planning_loss(sensors_data, expert_actions)
        
        # æ§åˆ¶æŸå¤±
        control_loss = self._control_loss(sensors_data, expert_actions)
        
        # æ€»æŸå¤±
        total_loss = (
            perception_loss + 
            0.5 * planning_loss + 
            0.1 * control_loss
        )
        
        return total_loss
```

---

## 110. è›‹ç™½è´¨ä¸ç”Ÿç‰©AI

### 110.1 AlphaFold2å®ç°

```python
class AlphaFold2(nn.Module):
    """AlphaFold2"""
    
    def __init__(self, config):
        super().__init__()
        
        # MSAå †æ ˆ
        self.msa_stack = MSAStack(config)
        
        # é…å¯¹è¡¨ç¤º
        self.pair_stack = PairRepresentationStack(config)
        
        # ä¸‰è§’æ³¨æ„
        self.triangle_attention = TriangleAttention(config)
        
        # å¤´éƒ¨æ¨¡å—
        self.head = StructureModule(config)
    
    def forward(self, msa, pair, single):
        # MSAæ›´æ–°
        msa = self.msa_stack(msa, pair)
        
        # é…å¯¹è¡¨ç¤ºæ›´æ–°
        pair = self.pair_stack(pair)
        pair = self.triangle_attention(pair)
        
        # 3Dç»“æ„æ¨¡å—
        outputs = self.head(single, pair)
        
        return outputs

class MSAStack(nn.Module):
    """MSAå †æ ˆ"""
    
    def __init__(self, config):
        super().__init__()
        
        self.layers = nn.ModuleList([
            MSAColumnAttention(config)
            for _ in range(config.msa_depth)
        ])
    
    def forward(self, msa, pair):
        for layer in self.layers:
            msa = layer(msa, pair)
        return msa

class StructureModule(nn.Module):
    """ç»“æ„æ¨¡å—"""
    
    def __init__(self, config):
        super().__init__()
        
        self.single_layer_norm = nn.LayerNorm(config.hidden_size)
        self.pair_layer_norm = nn.LayerNorm(config.hidden_size)
        
        # æ³¨æ„åŠ›
        self.attention = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=config.hidden_size),
            num_layers=3
        )
        
        # IPAå¤´
        self.ipa = InvariantPointAttention(config)
        
        # éª¨æ¶å¤´
        self.backbone_head = BackboneHead(config)
    
    def forward(self, single, pair):
        # ç‰¹å¾èåˆ
        x = self.single_layer_norm(single)
        x = self.attention(x)
        
        # IPA
        x, angles = self.ipa(x, pair)
        
        # éª¨æ¶
        backbone = self.backbone_head(x)
        
        return {
            'frames': backbone['frames'],
            'angles': angles,
            'sidechains': backbone['sidechains']
        }
```

### 110.2 ESM-2

```python
class ESM2(nn.Module):
    """ESM-2è›‹ç™½è´¨è¯­è¨€æ¨¡å‹"""
    
    def __init__(self, num_layers=33, embed_dim=1280, 
                 attention_heads=20):
        super().__init__()
        
        # åµŒå…¥
        self.embed = nn.Embedding(33, embed_dim)
        
        # ä½ç½®ç¼–ç 
        self.pos_embed = RotaryEmbedding(embed_dim)
        
        # Transformer
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=attention_heads,
                dim_feedforward=embed_dim * 4
            )
            for _ in range(num_layers)
        ])
        
        # é¢„è®­ç»ƒå¤´
        self.lm_head = nn.Linear(embed_dim, 33)
        self.contact_head = nn.Linear(embed_dim, 1)
    
    def forward(self, tokens, return_repr=False):
        # åµŒå…¥
        x = self.embed(tokens)
        
        # ä½ç½®ç¼–ç 
        seq_len = tokens.size(1)
        cos, sin = self.pos_embed(seq_len, x.device)
        
        # Transformer
        for layer in self.layers:
            x = layer(x, cos=cos, sin=sin)
        
        # å¤´
        logits = self.lm_head(x)
        
        if return_repr:
            return x
        return logits
```

---

*æœ¬éƒ¨åˆ†è´¡çŒ®çº¦35KBé«˜çº§çŸ¥è¯†*

**æŒç»­å­¦ä¹ ï¼ç›®æ ‡10MBï¼** ğŸš€ğŸ’ª

