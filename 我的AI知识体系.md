# æˆ‘çš„AIçŸ¥è¯†ä½“ç³»

*ä¸ªäººAIå­¦ä¹ ç¬”è®°ä¸çŸ¥è¯†æ•´ç†*

---

## ğŸ“š ç›®å½•

1. [æ•°å­¦åŸºç¡€](#ä¸€æ•°å­¦åŸºç¡€)
2. [Pythonç¼–ç¨‹](#äºŒpythonç¼–ç¨‹)
3. [æœºå™¨å­¦ä¹ ](#ä¸‰æœºå™¨å­¦ä¹ )
4. [æ·±åº¦å­¦ä¹ ](#å››æ·±åº¦å­¦ä¹ )
5. [æ³¨æ„åŠ›æœºåˆ¶ä¸Transformer](#äº”æ³¨æ„åŠ›æœºåˆ¶ä¸transformer)
6. [é¢„è®­ç»ƒæ¨¡å‹](#å…­é¢„è®­ç»ƒæ¨¡å‹)
7. [ç”Ÿæˆæ¨¡å‹](#ä¸ƒç”Ÿæˆæ¨¡å‹)
8. [å¼ºåŒ–å­¦ä¹ ](#å…«å¼ºåŒ–å­¦ä¹ )
9. [å¤šæ¨¡æ€å­¦ä¹ ](#ä¹å¤šæ¨¡æ€å­¦ä¹ )
10. [æ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²](#åæ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²)
11. [MLOpsä¸AutoML](#åä¸€mlopsä¸automl)
12. [å¯è§£é‡ŠAIä¸ä¼¦ç†](#åäºŒå¯è§£é‡Šaiä¸ä¼¦ç†)
13. [åº”ç”¨é¢†åŸŸ](#åä¸‰åº”ç”¨é¢†åŸŸ)
14. [å·¥å…·ç”Ÿæ€](#åå››å·¥å…·ç”Ÿæ€)
15. [èŒä¸šå‘å±•](#åäº”èŒä¸šå‘å±•)
16. [å‰æ²¿æ–¹å‘](#åå…­å‰æ²¿æ–¹å‘)

---

## ä¸€ã€æ•°å­¦åŸºç¡€

### 1.1 çº¿æ€§ä»£æ•°

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- å‘é‡ä¸çŸ©é˜µè¿ç®—
- ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡
- å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰
- çŸ©é˜µåˆ†è§£ï¼ˆLUã€QRï¼‰

**åº”ç”¨**ï¼š
- ç¥ç»ç½‘ç»œçš„çŸ©é˜µè¿ç®—
- ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
- çº¿æ€§å˜æ¢ä¸å·ç§¯

### 1.2 æ¦‚ç‡è®ºä¸ç»Ÿè®¡

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- éšæœºå˜é‡ä¸æ¦‚ç‡åˆ†å¸ƒ
- æ¡ä»¶æ¦‚ç‡ä¸è´å¶æ–¯å®šç†
- æœŸæœ›ã€æ–¹å·®ã€åæ–¹å·®
- æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰
- è´å¶æ–¯ä¼°è®¡

**åº”ç”¨**ï¼š
- æ¦‚ç‡ç”Ÿæˆæ¨¡å‹
- è´å¶æ–¯ç¥ç»ç½‘ç»œ
- å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±è®¾è®¡

### 1.3 ä¼˜åŒ–ç†è®º

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- æ¢¯åº¦ä¸‹é™ä¸éšæœºæ¢¯åº¦ä¸‹é™
- åŠ¨é‡æ–¹æ³•ï¼ˆMomentumã€NAGï¼‰
- è‡ªé€‚åº”ä¼˜åŒ–ï¼ˆAdaGradã€RMSpropã€Adamï¼‰
- å‡¸ä¼˜åŒ–ä¸éå‡¸ä¼˜åŒ–
- æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ä¸KKTæ¡ä»¶

**åº”ç”¨**ï¼š
- ç¥ç»ç½‘ç»œè®­ç»ƒ
- æ­£åˆ™åŒ–çº¦æŸ
- å¯¹å¶é—®é¢˜

---

## äºŒã€Pythonç¼–ç¨‹

### 2.1 åŸºç¡€è¯­æ³•

```python
# æ•°æ®ç±»å‹
x = 10  # int
y = 3.14  # float
z = "hello"  # str
is_valid = True  # bool

# æ§åˆ¶æµ
if condition:
    print("True")
elif other_condition:
    print("Other")
else:
    print("False")

# å¾ªç¯
for i in range(10):
    print(i)

# å‡½æ•°
def my_function(arg1, arg2=default):
    return result
```

### 2.2 æ•°æ®ç»“æ„

- **åˆ—è¡¨ï¼ˆListï¼‰**ï¼šæœ‰åºå¯ä¿®æ”¹é›†åˆ
- **å­—å…¸ï¼ˆDictï¼‰**ï¼šé”®å€¼å¯¹æ˜ å°„
- **é›†åˆï¼ˆSetï¼‰**ï¼šæ— åºå”¯ä¸€å…ƒç´ 
- **å…ƒç»„ï¼ˆTupleï¼‰**ï¼šæœ‰åºä¸å¯ä¿®æ”¹

### 2.3 é¢å‘å¯¹è±¡ç¼–ç¨‹

```python
class MyClass:
    def __init__(self, param):
        self.param = param
    
    def method(self):
        return self.param
    
    @classmethod
    def class_method(cls):
        return "class method"
    
    @staticmethod
    def static_method():
        return "static method"
```

### 2.4 å‡½æ•°å¼ç¼–ç¨‹

```python
# Lambdaè¡¨è¾¾å¼
square = lambda x: x ** 2

# Map/Filter/Reduce
result = map(lambda x: x*2, [1, 2, 3])
result = filter(lambda x: x > 0, [-1, 0, 1])

# åˆ—è¡¨æ¨å¯¼å¼
squares = [x**2 for x in range(10)]

# ç”Ÿæˆå™¨
def my_generator():
    for i in range(100):
        yield i
```

### 2.5 NumPy

```python
import numpy as np

# åˆ›å»ºæ•°ç»„
arr = np.array([1, 2, 3])
zeros = np.zeros((3, 3))
ones = np.ones((2, 2))
arange = np.arange(0, 10, 2)
linspace = np.linspace(0, 1, 10)

# æ•°ç»„æ“ä½œ
arr.shape  # å½¢çŠ¶
arr.reshape((3, 3))  # é‡å¡‘
arr[0:2]  # åˆ‡ç‰‡
arr + arr  # å¹¿æ’­

# çŸ©é˜µè¿ç®—
dot = np.dot(A, B)  # çŸ©é˜µä¹˜æ³•
transpose = A.T  # è½¬ç½®
inverse = np.linalg.inv(A)  # é€†çŸ©é˜µ
eigenvalues, eigenvectors = np.linalg.eig(A)  # ç‰¹å¾å€¼åˆ†è§£
```

### 2.6 Pandas

```python
import pandas as pd

# åˆ›å»ºDataFrame
df = pd.DataFrame({
    'A': [1, 2, 3],
    'B': ['x', 'y', 'z'],
    'C': [True, False, True]
})

# æ•°æ®é€‰æ‹©
df['A']  # åˆ—é€‰æ‹©
df.loc[0]  # æ ‡ç­¾ç´¢å¼•
df.iloc[0]  # ä½ç½®ç´¢å¼•
df[df['A'] > 1]  # æ¡ä»¶ç­›é€‰

# æ•°æ®æ¸…æ´—
df.dropna()  # åˆ é™¤ç©ºå€¼
df.fillna(0)  # å¡«å……ç©ºå€¼
df.drop_duplicates()  # å»é‡

# èšåˆæ“ä½œ
df.groupby('B').mean()
df.agg({'A': ['mean', 'sum']})
```

---

## ä¸‰ã€æœºå™¨å­¦ä¹ 

### 3.1 ç›‘ç£å­¦ä¹ 

#### çº¿æ€§æ¨¡å‹

**çº¿æ€§å›å½’**ï¼š
$$y = wx + b$$

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

**é€»è¾‘å›å½’**ï¼ˆäºŒåˆ†ç±»ï¼‰ï¼š
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
proba = model.predict_proba(X_test)
```

#### æ ‘æ¨¡å‹

**å†³ç­–æ ‘**ï¼š
```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth=5)
model.fit(X_train, y_train)
```

**éšæœºæ£®æ—**ï¼š
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
```

**æ¢¯åº¦æå‡ï¼ˆXGBoost/LightGBMï¼‰**ï¼š
```python
import xgboost as xgb
model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1)
model.fit(X_train, y_train)
```

#### æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

```python
from sklearn.svm import SVC
model = SVC(kernel='rbf', C=1.0)
model.fit(X_train, y_train)
```

### 3.2 æ— ç›‘ç£å­¦ä¹ 

#### èšç±»

**K-means**ï¼š
```python
from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)
model.fit(X)
labels = model.labels_
```

**DBSCAN**ï¼ˆå¯†åº¦èšç±»ï¼‰ï¼š
```python
from sklearn.cluster import DBSCAN
model = DBSCAN(eps=0.5, min_samples=5)
labels = model.fit_predict(X)
```

#### é™ç»´

**PCA**ï¼š
```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
X_reduced = pca.fit_transform(X)
```

**t-SNE/UMAP**ï¼ˆéçº¿æ€§é™ç»´ï¼‰ï¼š
```python
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2)
X_2d = tsne.fit_transform(X)
```

### 3.3 é›†æˆå­¦ä¹ 

**Bagging**ï¼š
- å¤šä¸ªæ¨¡å‹ç‹¬ç«‹è®­ç»ƒï¼Œå¹³å‡ç»“æœ
- é™ä½æ–¹å·®ï¼Œæé«˜ç¨³å®šæ€§
- ä¾‹å­ï¼šéšæœºæ£®æ—

**Boosting**ï¼š
- ä¸²è¡Œè®­ç»ƒæ¨¡å‹ï¼Œæ¯æ¬¡å…³æ³¨ä¸Šæ¬¡é”™è¯¯
- é™ä½åå·®ï¼Œæé«˜ç²¾åº¦
- ä¾‹å­ï¼šAdaBoostã€GBDTã€XGBoost

**Stacking**ï¼š
- å¤šå±‚æ¨¡å‹å †å 
- ä½¿ç”¨å…ƒå­¦ä¹ å™¨ç»„åˆåŸºå­¦ä¹ å™¨

### 3.4 æ¨¡å‹è¯„ä¼°

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# äº¤å‰éªŒè¯
scores = cross_val_score(model, X, y, cv=5)

# è¯„ä¼°æŒ‡æ ‡
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

# åˆ†ç±»æŠ¥å‘Š
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred))
```

---

## å››ã€æ·±åº¦å­¦ä¹ 

### 4.1 ç¥ç»ç½‘ç»œåŸºç¡€

**ç¥ç»å…ƒæ¨¡å‹**ï¼š
$$y = f(Wx + b)$$

**å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰**ï¼š
```python
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super().__init__()
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.extend([nn.Linear(prev_dim, hidden_dim), nn.ReLU()])
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)
```

### 4.2 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

**æ ¸å¿ƒç»„ä»¶**ï¼š

```python
# å·ç§¯å±‚
nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)

# æ± åŒ–å±‚
nn.MaxPool2d(kernel_size=2, stride=2)
nn.AvgPool2d(kernel_size=2)

# æ‰¹å½’ä¸€åŒ–
nn.BatchNorm2d(num_features=64)

# ç»å…¸æ¶æ„
# LeNet â†’ AlexNet â†’ VGG â†’ GoogLeNet â†’ ResNet â†’ EfficientNet
```

**ResNetæ®‹å·®å—**ï¼š
```python
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
    
    def forward(self, x):
        residual = x
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        return F.relu(x + residual)
```

### 4.3 å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰

**RNNç»“æ„**ï¼š
```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
    
    def forward(self, x, h0=None):
        out, hn = self.rnn(x, h0)
        return out, hn
```

**LSTM**ï¼š
```python
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
    
    def forward(self, x, h0=None):
        out, (hn, cn) = self.lstm(x, h0)
        return out, (hn, cn)
```

### 4.4 è®­ç»ƒæŠ€å·§

**ä¼˜åŒ–å™¨**ï¼š
```python
# SGD
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# Adam
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# AdamW
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
```

**å­¦ä¹ ç‡è°ƒåº¦**ï¼š
```python
# é¢„çƒ­+ä½™å¼¦é€€ç«
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2
)

# é˜¶æ¢¯è¡°å‡
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# 1Cycleç­–ç•¥
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer, max_lr=0.1, epochs=10, steps_per_epoch=len(train_loader)
)
```

**æ­£åˆ™åŒ–**ï¼š
```python
# Dropout
nn.Dropout(p=0.5)

# æ‰¹å½’ä¸€åŒ–
nn.BatchNorm2d(num_features)

# æƒé‡è¡°å‡
optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5)

# æ ‡ç­¾å¹³æ»‘
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
```

---

## äº”ã€æ³¨æ„åŠ›æœºåˆ¶ä¸Transformer

### 5.1 æ³¨æ„åŠ›æœºåˆ¶

**è‡ªæ³¨æ„åŠ›**ï¼š
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

```python
class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads)
    
    def forward(self, x):
        attn_output, _ = self.attention(x, x, x)
        return attn_output
```

### 5.2 Transformeræ¶æ„

```python
class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
    
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®
        x = x + self.attention(self.norm1(x), self.norm1(x), self.norm1(x))[0]
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®
        x = x + self.ffn(self.norm2(x))
        return x
```

**ä½ç½®ç¼–ç **ï¼š
```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0)]
```

---

## å…­ã€é¢„è®­ç»ƒæ¨¡å‹

### 6.1 BERT

**æ¶æ„**ï¼šåŒå‘Transformerç¼–ç å™¨

**é¢„è®­ç»ƒä»»åŠ¡**ï¼š
- MLMï¼ˆMasked Language Modelingï¼‰
- NSPï¼ˆNext Sentence Predictionï¼‰

```python
from transformers import BertModel, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

inputs = tokenizer("Hello, world!", return_tensors='pt')
outputs = model(**inputs)
pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token
```

### 6.2 GPT

**æ¶æ„**ï¼šå•å‘Transformerè§£ç å™¨

**ç‰¹ç‚¹**ï¼š
- é€‚åˆæ–‡æœ¬ç”Ÿæˆ
- é›¶æ ·æœ¬/å°‘æ ·æœ¬èƒ½åŠ›å¼º

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

inputs = tokenizer("Once upon a time", return_tensors='pt')
outputs = model.generate(**inputs, max_length=100)
```

### 6.3 LLaMA

**ç‰¹ç‚¹**ï¼š
- å¼€æºå¤§è¯­è¨€æ¨¡å‹
- é«˜æ•ˆæ¶æ„è®¾è®¡
- å¤šç§å‚æ•°è§„æ¨¡ï¼ˆ7B, 13B, 70Bï¼‰

**å¾®è°ƒæ–¹æ³•**ï¼š
- å…¨å‚æ•°å¾®è°ƒ
- LoRAï¼ˆLow-Rank Adaptationï¼‰
- Prefix Tuning
- Prompt Tuning

---

## ä¸ƒã€ç”Ÿæˆæ¨¡å‹

### 7.1 ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰

```python
class Generator(nn.Module):
    def __init__(self, latent_dim, img_channels):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, img_channels * 28 * 28),
            nn.Tanh()
        )
    
    def forward(self, z):
        return self.model(z).view(-1, 1, 28, 28)
```

### 7.2 å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰

```python
class VAE(nn.Module):
    def __init__(self, img_channels, latent_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(img_channels * 28 * 28, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU()
        )
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_var = nn.Linear(256, latent_dim)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, img_channels * 28 * 28),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        log_var = self.fc_var(h)
        std = torch.exp(0.5 * log_var)
        z = mu + std * torch.randn_like(std)
        return self.decoder(z), mu, log_var
```

### 7.3 æ‰©æ•£æ¨¡å‹

**DDPMå‰å‘è¿‡ç¨‹**ï¼ˆé€æ¸åŠ å™ªï¼‰ï¼š
$$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$

**DDPMåå‘è¿‡ç¨‹**ï¼ˆå»å™ªï¼‰ï¼š
$$p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)$$

```python
class UNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # ç®€åŒ–çš„UNetç»“æ„
        self.down = nn.ModuleList([
            nn.Conv2d(in_channels, 64, 3, padding=1),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.Conv2d(128, 256, 3, padding=1),
        ])
        self.up = nn.ModuleList([
            nn.Conv2d(512, 128, 3, padding=1),
            nn.Conv2d(256, 64, 3, padding=1),
            nn.Conv2d(128, out_channels, 3, padding=1),
        ])
        self.time_mlp = nn.Linear(1, 512)
    
    def forward(self, x, t):
        # æ—¶é—´ç¼–ç 
        t_embed = self.time_mlp(t.float().unsqueeze(1))
        
        # ç¼–ç å™¨
        h = x
        outputs = []
        for layer in self.down:
            h = F.relu(layer(h))
            outputs.append(h)
        
        # ä¸­é—´å±‚
        h = h * t_embed.unsqueeze(-1).unsqueeze(-1)
        
        # è§£ç å™¨
        for i, layer in enumerate(self.up):
            h = F.relu(layer(h + outputs[2-i]))
        
        return h
```

---

## å…«ã€å¼ºåŒ–å­¦ä¹ 

### 8.1 åŸºç¡€æ¦‚å¿µ

**MDPäº”å…ƒç»„**ï¼š(S, A, P, R, Î³)

- Sï¼šçŠ¶æ€ç©ºé—´
- Aï¼šåŠ¨ä½œç©ºé—´
- Pï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡
- Rï¼šå¥–åŠ±å‡½æ•°
- Î³ï¼šæŠ˜æ‰£å› å­

**ä»·å€¼å‡½æ•°**ï¼š
$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t R(s_t, a_t)\right]$$

### 8.2 å€¼å­¦ä¹ æ–¹æ³•

**Q-Learning**ï¼š
$$Q(s, a) \leftarrow Q(s, a) + \alpha\left[r + \gamma\max_{a'}Q(s', a') - Q(s, a)\right]$$

**DQN**ï¼š
```python
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super().__init__()
        self.q_network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, state):
        return self.q_network(state)
```

### 8.3 ç­–ç•¥æ¢¯åº¦æ–¹æ³•

**PPOï¼ˆProximal Policy Optimizationï¼‰**ï¼š
```python
class PPO:
    def __init__(self, actor, critic, lr=3e-4, clip_epsilon=0.2):
        self.actor = actor
        self.critic = critic
        self.optimizer = torch.optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)
        self.clip_epsilon = clip_epsilon
    
    def update(self, states, actions, old_log_probs, advantages, returns):
        logits = self.actor(states)
        values = self.critic(states)
        
        # PPOæ›´æ–°
        new_probs = F.log_softmax(logits, dim=-1)
        new_log_probs = new_probs.gather(1, actions).squeeze()
        
        ratio = torch.exp(new_log_probs - old_log_probs)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()
        critic_loss = F.mse_loss(values.squeeze(), returns)
        
        loss = actor_loss + 0.5 * critic_loss
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

### 8.4 SACï¼ˆSoft Actor-Criticï¼‰

```python
class SAC:
    def __init__(self, state_dim, action_dim):
        self.actor = GaussianPolicy(state_dim, action_dim)
        self.critic = TwinQ(state_dim, action_dim)
        self.critic_target = TwinQ(state_dim, action_dim)
        self.log_alpha = torch.zeros(1, requires_grad=True)
    
    def update(self, batch):
        # è½¯Qå‡½æ•°æ›´æ–°
        # ç­–ç•¥æ›´æ–°
        # æ¸©åº¦å‚æ•°æ›´æ–°
        pass
```

---

## ä¹ã€å¤šæ¨¡æ€å­¦ä¹ 

### 9.1 CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰

```python
class CLIP(nn.Module):
    def __init__(self, vision_model, text_model, projection_dim=512):
        super().__init__()
        self.vision_model = vision_model
        self.text_model = text_model
        self.visual_projection = nn.Linear(vision_model.hidden_size, projection_dim)
        self.text_projection = nn.Linear(text_model.hidden_size, projection_dim)
        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def forward(self, images, input_ids, attention_mask):
        image_features = self.vision_model(images)
        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)
        
        image_embeddings = F.normalize(self.visual_projection(image_features), dim=-1)
        text_embeddings = F.normalize(self.text_projection(text_features), dim=-1)
        
        logits = torch.matmul(image_embeddings, text_embeddings.T) * self.temperature.exp()
        return logits
```

### 9.2 è§†è§‰è¯­è¨€æ¨¡å‹

**LLaVA**ï¼š
- è§†è§‰æŒ‡ä»¤å¾®è°ƒ
- æŠ•å½±å±‚è¿æ¥è§†è§‰å’Œè¯­è¨€æ¨¡å‹
- æ”¯æŒå¯¹è¯æ ¼å¼

---

## åã€æ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²

### 10.1 æ¨¡å‹å‹ç¼©

**é‡åŒ–**ï¼š
```python
import torch.quantization

# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear, nn.LSTM},
    dtype=torch.qint8
)

# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
torch.quantization.convert(model, inplace=True)
```

**å‰ªæ**ï¼š
```python
import torch.nn.utils.prune as prune

# ç»“æ„åŒ–å‰ªæ
prune.ln_structured(module, name='weight', amount=0.3, n=2, dim=0)

# éç»“æ„åŒ–å‰ªæ
prune.global_unstructured(
    [(module, 'weight') for module in model.modules()],
    pruning_method=prune.L1Unstructured,
    amount=0.3
)
```

**çŸ¥è¯†è’¸é¦**ï¼š
```python
class KnowledgeDistillation:
    def __init__(self, teacher, student, temperature=2.0, alpha=0.5):
        self.teacher = teacher
        self.student = student
        self.temperature = temperature
        self.alpha = alpha
    
    def train_step(self, x, target):
        with torch.no_grad():
            teacher_logits = self.teacher(x)
        
        student_logits = self.student(x)
        
        # è’¸é¦æŸå¤±
        distill_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=1),
            F.softmax(teacher_logits / self.temperature, dim=1),
            reduction='batchmean'
        ) * (self.temperature ** 2)
        
        # æ ‡å‡†æŸå¤±
        ce_loss = F.cross_entropy(student_logits, target)
        
        return self.alpha * ce_loss + (1 - self.alpha) * distill_loss
```

### 10.2 æ¨¡å‹å¯¼å‡º

```python
# TorchScript
scripted_model = torch.jit.script(model)
scripted_model.save('model_scripted.pt')

# ONNX
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(model, dummy_input, 'model.onnx',
                 input_names=['input'],
                 output_names=['output'],
                 dynamic_axes={'input': {0: 'batch_size'}})

# TensorRT
import torch_tensorrt
compiled_model = torch_tensorrt.compile(
    model,
    inputs=[torch_tensorrt.Input(shape=(1, 3, 224, 224))],
    enabled_precisions={torch.float, torch.half}
)
```

### 10.3 æ¨ç†ä¼˜åŒ–

```python
# æ‰¹å¤„ç†
batch_size = 32
for i in range(0, len(inputs), batch_size):
    batch = inputs[i:i+batch_size]
    outputs = model(batch)

# ç®—å­èåˆ
# ä½¿ç”¨TorchScriptæˆ–TensorRTè‡ªåŠ¨èåˆç®—å­

# å†…å­˜ä¼˜åŒ–
torch.cuda.empty_cache()
```

---

## åä¸€ã€MLOpsä¸AutoML

### 11.1 å®éªŒè·Ÿè¸ª

**MLflow**ï¼š
```python
import mlflow
import mlflow.pytorch

mlflow.start_run()
mlflow.log_param('learning_rate', 0.001)
mlflow.log_metric('accuracy', 0.95)
mlflow.pytorch.log_model(model, 'model')
mlflow.end_run()
```

**Weights & Biases**ï¼š
```python
import wandb

wandb.init(project='my-project')
wandb.config.update({'learning_rate': 0.001})
wandb.log({'loss': loss, 'accuracy': accuracy})
```

### 11.2 AutoML

**Optunaè¶…å‚ä¼˜åŒ–**ï¼š
```python
import optuna

def objective(trial):
    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])
    
    model = create_model(lr)
    train(model, batch_size)
    accuracy = evaluate(model)
    
    return accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
print(study.best_params)
```

---

## åäºŒã€å¯è§£é‡ŠAIä¸ä¼¦ç†

### 12.1 å¯è§£é‡Šæ€§æ–¹æ³•

**SHAP**ï¼š
```python
import shap

explainer = shap.DeepExplainer(model, background_data)
shap_values = explainer.shap_values(input_data)
shap.summary_plot(shap_values, input_data)
```

**Grad-CAM**ï¼š
```python
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
    
    def generate(self, input_image, target_class=None):
        output = self.model(input_image)
        if target_class is None:
            target_class = output.argmax()
        
        self.model.zero_grad()
        one_hot = torch.zeros_like(output)
        one_hot[0, target_class] = 1
        output.backward(gradient=one_hot)
        
        weights = self.gradients.mean(dim=(2, 3), keepdim=True)
        cam = (weights * self.activations).sum(dim=1)
        cam = F.relu(cam)
        
        return cam.squeeze().detach().numpy()
```

### 12.2 AIä¼¦ç†

**å…¬å¹³æ€§**ï¼š
- æ£€æµ‹åè§ï¼šç»Ÿè®¡å¥‡å¶æ€§å·®å¼‚
- å»åæŠ€æœ¯ï¼šé‡é‡‡æ ·ã€é‡åŠ æƒã€å¯¹æŠ—è®­ç»ƒ

**éšç§ä¿æŠ¤**ï¼š
- å·®åˆ†éšç§
- è”é‚¦å­¦ä¹ 
- åŒæ€åŠ å¯†

---

## åä¸‰ã€åº”ç”¨é¢†åŸŸ

### 13.1 è®¡ç®—æœºè§†è§‰

| ä»»åŠ¡ | æ¨¡å‹ | è¯„ä¼°æŒ‡æ ‡ |
|------|------|----------|
| å›¾åƒåˆ†ç±» | ResNet, EfficientNet | Top-1 Accuracy |
| ç›®æ ‡æ£€æµ‹ | YOLO, Faster R-CNN | mAP |
| è¯­ä¹‰åˆ†å‰² | U-Net, DeepLab | IoU |
| å®ä¾‹åˆ†å‰² | Mask R-CNN | mAP |

### 13.2 è‡ªç„¶è¯­è¨€å¤„ç†

| ä»»åŠ¡ | æ¨¡å‹ | è¯„ä¼°æŒ‡æ ‡ |
|------|------|----------|
| æ–‡æœ¬åˆ†ç±» | BERT | Accuracy, F1 |
| å‘½åå®ä½“è¯†åˆ« | BERT-CRF | F1 |
| æœºå™¨ç¿»è¯‘ | Transformer | BLEU |
| é—®ç­”ç³»ç»Ÿ | BERT, T5 | F1, EM |

### 13.3 æ¨èç³»ç»Ÿ

- ååŒè¿‡æ»¤
- æ·±åº¦æ¨èï¼ˆNCF, DINï¼‰
- å›¾ç¥ç»ç½‘ç»œæ¨è
- å¤šä»»åŠ¡æ¨è

---

## åå››ã€å·¥å…·ç”Ÿæ€

### 14.1 æ·±åº¦å­¦ä¹ æ¡†æ¶

| æ¡†æ¶ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| PyTorch | åŠ¨æ€å›¾ã€æ˜“è°ƒè¯• | ç ”ç©¶ã€å®éªŒ |
| TensorFlow | é™æ€å›¾ã€ç”Ÿäº§éƒ¨ç½² | ç”Ÿäº§ç¯å¢ƒ |
| JAX | å‡½æ•°å¼ã€é«˜æ€§èƒ½ | å¤§è§„æ¨¡è®­ç»ƒ |
| PaddlePaddle | å›½äº§ã€æ˜“ç”¨ | å·¥ä¸šåº”ç”¨ |

### 14.2 é¢„è®­ç»ƒæ¨¡å‹åº“

**Hugging Face**ï¼š
```python
from transformers import AutoModel, AutoTokenizer

model_name = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
```

### 14.3 å¼€å‘å·¥å…·

- **å®éªŒè·Ÿè¸ª**ï¼šMLflow, Weights & Biases, Neptune
- **æ•°æ®ç‰ˆæœ¬**ï¼šDVC, Delta Lake
- **æ¨¡å‹éƒ¨ç½²**ï¼šTorchServe, Triton, KServe
- **å®¹å™¨åŒ–**ï¼šDocker, Kubernetes

---

## åäº”ã€èŒä¸šå‘å±•

### 15.1 æŠ€èƒ½è¦æ±‚

**æŠ€æœ¯æŠ€èƒ½**ï¼š
- Pythonç¼–ç¨‹
- æ•°å­¦åŸºç¡€ï¼ˆçº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºã€ä¼˜åŒ–ï¼‰
- æ·±åº¦å­¦ä¹ ç†è®º
- æ¡†æ¶ä½¿ç”¨ï¼ˆPyTorch/TensorFlowï¼‰
- æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–

**è½¯æŠ€èƒ½**ï¼š
- æŠ€æœ¯æ²Ÿé€š
- é—®é¢˜åˆ†è§£
- é¡¹ç›®ç®¡ç†

### 15.2 å­¦ä¹ è·¯å¾„

```
å…¥é—¨é˜¶æ®µï¼ˆ3-6ä¸ªæœˆï¼‰ï¼š
â”œâ”€ Pythonç¼–ç¨‹
â”œâ”€ åŸºç¡€æ•°å­¦
â”œâ”€ æœºå™¨å­¦ä¹ åŸºç¡€
â””â”€ æ·±åº¦å­¦ä¹ å…¥é—¨

è¿›é˜¶é˜¶æ®µï¼ˆ6-12ä¸ªæœˆï¼‰ï¼š
â”œâ”€ æ·±å…¥ä¸€ä¸ªæ–¹å‘ï¼ˆCV/NLP/RLï¼‰
â”œâ”€ é˜…è¯»è®ºæ–‡
â”œâ”€ å®Œæˆé¡¹ç›®
â””â”€ å‚ä¸ç«èµ›

ç²¾é€šé˜¶æ®µï¼ˆ1-2å¹´ï¼‰ï¼š
â”œâ”€ é«˜çº§ä¸»é¢˜ï¼ˆå¤§è§„æ¨¡è®­ç»ƒã€éƒ¨ç½²ï¼‰
â”œâ”€ å¼€æºè´¡çŒ®
â”œâ”€ æŠ€æœ¯åˆ†äº«
â””â”€ æ¶æ„è®¾è®¡
```

### 15.3 é¢è¯•å‡†å¤‡

**ç®—æ³•é¢˜**ï¼š
- æœºå™¨å­¦ä¹ ç®—æ³•å®ç°
- ä¼˜åŒ–ç®—æ³•
- æ•°æ®ç»“æ„

**ç†è®ºé¢˜**ï¼š
- æ·±åº¦å­¦ä¹ ç†è®º
- æœºå™¨å­¦ä¹ ç†è®º
- æ¨¡å‹è¯„ä¼°

**é¡¹ç›®é¢˜**ï¼š
- é¡¹ç›®è®¾è®¡
- é—®é¢˜åˆ†æ
- è§£å†³æ–¹æ¡ˆ

---

## åå…­ã€å‰æ²¿æ–¹å‘

### 16.1 å¤§è¯­è¨€æ¨¡å‹

- å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆGPT-4V, Geminiï¼‰
- Agentç³»ç»Ÿï¼ˆAutoGPT, LangChainï¼‰
- RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰
- å·¥å…·ä½¿ç”¨ä¸å‡½æ•°è°ƒç”¨

### 16.2 å¤šæ¨¡æ€

- è§†è§‰è¯­è¨€æ¨¡å‹
- 3Dç†è§£
- å…·èº«æ™ºèƒ½

### 16.3 å…·èº«æ™ºèƒ½

- æœºå™¨äººå­¦ä¹ 
- è‡ªåŠ¨é©¾é©¶
- äººæœºäº¤äº’

### 16.4 ç§‘å­¦AI

- è¯ç‰©å‘ç°ï¼ˆAlphaFoldï¼‰
- ææ–™è®¾è®¡
- è›‹ç™½è´¨ç»“æ„é¢„æµ‹

---

## ğŸ“š å­¦ä¹ èµ„æºæ¨è

### è¯¾ç¨‹
- Stanford CS231nï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰
- Stanford CS224nï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰
- DeepLearning.AI
- Fast.ai

### ä¹¦ç±
- ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ï¼ˆææ²ï¼‰
- ã€Šæ·±åº¦å­¦ä¹ ã€‹ï¼ˆèŠ±ä¹¦ï¼‰
- ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰

### è®ºæ–‡
- NeurIPS, ICML, ICLR
- arXiv:cs.LG, cs.CL, cs.CV

### ç¤¾åŒº
- GitHub Trending
- Hacker News
- Reddit r/MachineLearning
- çŸ¥ä¹ä¸“æ 

---

## ğŸ¯ å­¦ä¹ å»ºè®®

1. **æ‰“å¥½åŸºç¡€**ï¼šæ•°å­¦ã€ç¼–ç¨‹ã€æœºå™¨å­¦ä¹ åŸºç¡€è¦æ‰å®
2. **åŠ¨æ‰‹å®è·µ**ï¼šä¸è¦åªçœ‹ç†è®ºï¼Œè¦å¤šå†™ä»£ç 
3. **é˜…è¯»è®ºæ–‡**ï¼šå…³æ³¨å‰æ²¿è¿›å±•ï¼ŒåŸ¹å…»ç ”ç©¶æ€ç»´
4. **å‚ä¸ç¤¾åŒº**ï¼šäº¤æµå­¦ä¹ ï¼Œåˆ†äº«ç»éªŒ
5. **æŒç»­å­¦ä¹ **ï¼šAIå‘å±•å¿«ï¼Œè¦ä¿æŒå­¦ä¹ 

---

*æœ¬çŸ¥è¯†ä½“ç³»ç”±ä¸ªäººæ•´ç†ï¼ŒæŒç»­æ›´æ–°ä¸­...*

**ğŸ“š å­¦ä¹ æ°¸æ— æ­¢å¢ƒï¼Œè¿›æ­¥æ°¸ä¸åœæ­‡ï¼** ğŸš€ğŸ’ª
